{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "CUDA available: True\n",
      "CUDA version in PyTorch: 11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# GPU 사용 설정\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "# CUDA 사용 가능 여부 확인\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")  # True여야 함\n",
    "\n",
    "# PyTorch에서 사용하는 CUDA 버전 확인\n",
    "print(f\"CUDA version in PyTorch: {torch.version.cuda}\")\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\isy\\test\\Lib\\site-packages\\albumentations\\__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.18 (you have 1.4.17). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 임포트\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 데이터 수: 2008\n",
      "Validation 데이터 수: 502\n",
      "Test 데이터 수: 628\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "import pandas as pd\n",
    "\n",
    "IMAGE_SIZE = 300\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "# 이미지 변환 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),  # 이미지를 Tensor로 변환 (0~255 범위를 0~1 범위로 정규화)\n",
    "])\n",
    "\n",
    "# 데이터셋 디렉토리 설정\n",
    "train_dir = '../PCB_imgs/all/resize/train'\n",
    "val_dir = '../PCB_imgs/all/resize/validation'\n",
    "test_dir = '../PCB_imgs/all/resize/test'\n",
    "\n",
    "# ImageFolder로 데이터셋 불러오기\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "# 파일 경로 및 타겟 추출\n",
    "train_file_paths = [img[0] for img in train_dataset.imgs]\n",
    "train_targets = train_dataset.targets\n",
    "\n",
    "val_file_paths = [img[0] for img in val_dataset.imgs]\n",
    "val_targets = val_dataset.targets\n",
    "\n",
    "test_file_paths = [img[0] for img in test_dataset.imgs]\n",
    "test_targets = test_dataset.targets\n",
    "\n",
    "# DataFrame 생성\n",
    "train_df = pd.DataFrame({'file_paths': train_file_paths, 'targets': train_targets})\n",
    "validation_df = pd.DataFrame({'file_paths': val_file_paths, 'targets': val_targets})\n",
    "test_df = pd.DataFrame({'file_paths': test_file_paths, 'targets': test_targets})\n",
    "\n",
    "# 확인을 위해 각 데이터셋의 크기 출력\n",
    "print(f\"Train 데이터 수: {len(train_df)}\")\n",
    "print(f\"Validation 데이터 수: {len(validation_df)}\")\n",
    "print(f\"Test 데이터 수: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 300, 300])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# resize된 데이터셋 로드\n",
    "input_dir = '../PCB_imgs/all/resize/'\n",
    "dataset = datasets.ImageFolder(root=input_dir, transform=transform)\n",
    "\n",
    "# 데이터 로더 정의\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 데이터 로더에서 데이터 가져오기\n",
    "for images, labels in dataloader:\n",
    "    print(images.shape)  # 배치의 이미지 텐서 크기 확인\n",
    "    break  # 첫 번째 배치만 확인\n",
    "    # 배치 크기, 채널 수(RGB), 이미지 크기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커스텀 데이터세트 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_paths, targets, aug=None, preprocess=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.targets = targets\n",
    "        self.aug = aug\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_path = self.file_paths[index]\n",
    "        target = self.targets[index]\n",
    "        \n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "\n",
    "        if self.aug is not None:\n",
    "            image = self.aug(image=image)['image']\n",
    "\n",
    "        if self.preprocess is not None:\n",
    "            image = self.preprocess(image)\n",
    "\n",
    "        image = np.transpose(image, (2, 0, 1))  # (H, W, C) -> (C, H, W)\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 함수 정의\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device).float()\n",
    "            outputs = model(images)\n",
    "            outputs = outputs.view(-1) # torch.Size([batch_size])로 변환\n",
    "            loss = criterion(outputs, labels)  # 손실 계산\n",
    "            running_test_loss += loss.item()\n",
    "            \n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()  # 0.5 기준으로 이진 분류\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "            \n",
    "    # 검증 손실과 정확도\n",
    "    test_loss = running_test_loss / len(test_loader)\n",
    "    test_accuracy = correct_test / total_test\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model_name='efficientnet'):\n",
    "        super(CustomModel, self).__init__()\n",
    "        if model_name == 'efficientnet':\n",
    "            self.base_model = torchvision.models.efficientnet_b3(weights='IMAGENET1K_V1')\n",
    "            self.base_model.classifier = nn.Identity()  # 마지막 분류기 제거\n",
    "        elif model_name == 'resnet50':\n",
    "            self.base_model = torchvision.models.resnet50(weights='IMAGENET1K_V1')\n",
    "            self.base_model = nn.Sequential(*list(self.base_model.children())[:-1])  # 마지막 레이어 제거\n",
    "        elif model_name == 'inception':\n",
    "            self.base_model = torchvision.models.inception_v3(weights='IMAGENET1K_V1')\n",
    "            self.base_model.classifier = nn.Identity()  # 마지막 분류기 제거\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.5)  # 첫 번째 드롭아웃\n",
    "        self.fc1 = nn.Linear(self._get_features_dim(model_name), 50)\n",
    "        self.bn1 = nn.BatchNorm1d(50)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.5)  # 두 번째 드롭아웃\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "\n",
    "    def _get_features_dim(self, model_name):\n",
    "        if model_name == 'efficientnet':\n",
    "            return 1536  # EfficientNet B3의 출력 차원\n",
    "        elif model_name == 'resnet50':\n",
    "            return 2048  # ResNet50의 출력 차원\n",
    "        elif model_name == 'inception':\n",
    "            return 2048  # Inception의 출력 차원\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        if isinstance(x, tuple):  \n",
    "            x = x[0]\n",
    "        x = x.view(x.size(0), -1)  \n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x) \n",
    "        x = self.bn1(x) \n",
    "        x = self.dropout2(x) \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 데이터 증강 정의\n",
    "aug = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),                # 좌우 반전\n",
    "    A.VerticalFlip(p=0.5),                  # 상하 반전\n",
    "    A.Rotate(limit=10, p=0.5),              # 작은 각도 회전 (10도 내외)\n",
    "    A.RandomBrightnessContrast(p=0.5),      # 밝기 및 대비 조절\n",
    "])\n",
    "\n",
    "# 데이터셋 인스턴스 생성\n",
    "train_dataset = CustomDataset(train_df['file_paths'].values, train_df['targets'].values, aug=aug)\n",
    "validation_dataset = CustomDataset(validation_df['file_paths'].values, validation_df['targets'].values)\n",
    "test_dataset = CustomDataset(test_df['file_paths'].values, test_df['targets'].values)\n",
    "\n",
    "# DataLoader 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b3_rwightman-b3899882.pth\" to C:\\Users\\enssel/.cache\\torch\\hub\\checkpoints\\efficientnet_b3_rwightman-b3899882.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: base_model.features.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.2.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.2.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.2.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.2.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.2.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.2.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.4.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.4.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.4.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.4.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.4.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.4.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.4.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.4.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.4.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.4.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.4.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.4.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.4.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.4.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.4.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.4.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.4.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.4.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.4.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.4.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.4.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.4.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.4.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.4.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.4.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.4.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.5.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.5.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.5.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.5.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.5.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.5.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.5.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.5.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.5.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.5.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.5.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.5.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.5.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.8.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.8.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.8.1.bias | Requires Grad: True\n",
      "Layer: fc1.weight | Requires Grad: True\n",
      "Layer: fc1.bias | Requires Grad: True\n",
      "Layer: bn1.weight | Requires Grad: True\n",
      "Layer: bn1.bias | Requires Grad: True\n",
      "Layer: fc2.weight | Requires Grad: True\n",
      "Layer: fc2.bias | Requires Grad: True\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "model = CustomModel(model_name='efficientnet').to(DEVICE)\n",
    "\n",
    "# 1. 사전 학습된 base_model의 파라미터를 동결 (fine-tuning 초기 단계)\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 마지막 몇 개의 레이어를 동결 해제 (features 블록 5 이후 층)\n",
    "for name, param in model.base_model.features[5:].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 동결 해제된 일부 층과 분류기 층 학습\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
    "\n",
    "# 학습할 손실 함수\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 각 층의 freeze/unfreeze 상태 확인\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Requires Grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved, saving model...\n",
      "Epoch [1/50], Train Loss: 0.6428, Train Accuracy: 0.6250, Val Loss: 0.5261, Val Accuracy: 0.7869, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [2/50], Train Loss: 0.5344, Train Accuracy: 0.7306, Val Loss: 0.4560, Val Accuracy: 0.8167, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [3/50], Train Loss: 0.4903, Train Accuracy: 0.7654, Val Loss: 0.4380, Val Accuracy: 0.8187, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [4/50], Train Loss: 0.4499, Train Accuracy: 0.7898, Val Loss: 0.4154, Val Accuracy: 0.8307, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [5/50], Train Loss: 0.4225, Train Accuracy: 0.8098, Val Loss: 0.3859, Val Accuracy: 0.8526, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [6/50], Train Loss: 0.3999, Train Accuracy: 0.8396, Val Loss: 0.3698, Val Accuracy: 0.8606, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [7/50], Train Loss: 0.3832, Train Accuracy: 0.8506, Val Loss: 0.3511, Val Accuracy: 0.8825, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [8/50], Train Loss: 0.3625, Train Accuracy: 0.8606, Val Loss: 0.3459, Val Accuracy: 0.8725, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [9/50], Train Loss: 0.3427, Train Accuracy: 0.8765, Val Loss: 0.3213, Val Accuracy: 0.8924, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [10/50], Train Loss: 0.3100, Train Accuracy: 0.8979, Val Loss: 0.2916, Val Accuracy: 0.9024, Learning Rate: 0.000050\n",
      "Epoch [11/50], Train Loss: 0.2915, Train Accuracy: 0.9168, Val Loss: 0.2962, Val Accuracy: 0.9004, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [12/50], Train Loss: 0.2760, Train Accuracy: 0.9173, Val Loss: 0.2900, Val Accuracy: 0.8964, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [13/50], Train Loss: 0.2638, Train Accuracy: 0.9288, Val Loss: 0.2585, Val Accuracy: 0.9124, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [14/50], Train Loss: 0.2442, Train Accuracy: 0.9328, Val Loss: 0.2495, Val Accuracy: 0.9203, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [15/50], Train Loss: 0.2361, Train Accuracy: 0.9402, Val Loss: 0.2431, Val Accuracy: 0.9183, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [16/50], Train Loss: 0.2202, Train Accuracy: 0.9457, Val Loss: 0.2365, Val Accuracy: 0.9263, Learning Rate: 0.000050\n",
      "Epoch [17/50], Train Loss: 0.2103, Train Accuracy: 0.9487, Val Loss: 0.2408, Val Accuracy: 0.9143, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [18/50], Train Loss: 0.1938, Train Accuracy: 0.9582, Val Loss: 0.2291, Val Accuracy: 0.9243, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [19/50], Train Loss: 0.1836, Train Accuracy: 0.9612, Val Loss: 0.2013, Val Accuracy: 0.9422, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [20/50], Train Loss: 0.1739, Train Accuracy: 0.9681, Val Loss: 0.1990, Val Accuracy: 0.9422, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [21/50], Train Loss: 0.1643, Train Accuracy: 0.9711, Val Loss: 0.1892, Val Accuracy: 0.9542, Learning Rate: 0.000050\n",
      "Epoch [22/50], Train Loss: 0.1589, Train Accuracy: 0.9661, Val Loss: 0.1902, Val Accuracy: 0.9422, Learning Rate: 0.000050\n",
      "Epoch [23/50], Train Loss: 0.1596, Train Accuracy: 0.9701, Val Loss: 0.1895, Val Accuracy: 0.9402, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [24/50], Train Loss: 0.1416, Train Accuracy: 0.9826, Val Loss: 0.1695, Val Accuracy: 0.9522, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [25/50], Train Loss: 0.1454, Train Accuracy: 0.9746, Val Loss: 0.1688, Val Accuracy: 0.9502, Learning Rate: 0.000050\n",
      "Epoch [26/50], Train Loss: 0.1318, Train Accuracy: 0.9796, Val Loss: 0.1712, Val Accuracy: 0.9502, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [27/50], Train Loss: 0.1255, Train Accuracy: 0.9831, Val Loss: 0.1671, Val Accuracy: 0.9462, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [28/50], Train Loss: 0.1180, Train Accuracy: 0.9861, Val Loss: 0.1569, Val Accuracy: 0.9542, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [29/50], Train Loss: 0.1154, Train Accuracy: 0.9796, Val Loss: 0.1422, Val Accuracy: 0.9582, Learning Rate: 0.000050\n",
      "Epoch [30/50], Train Loss: 0.1179, Train Accuracy: 0.9826, Val Loss: 0.1496, Val Accuracy: 0.9522, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [31/50], Train Loss: 0.1136, Train Accuracy: 0.9801, Val Loss: 0.1338, Val Accuracy: 0.9522, Learning Rate: 0.000050\n",
      "Epoch [32/50], Train Loss: 0.1070, Train Accuracy: 0.9846, Val Loss: 0.1474, Val Accuracy: 0.9522, Learning Rate: 0.000050\n",
      "Epoch [33/50], Train Loss: 0.1026, Train Accuracy: 0.9866, Val Loss: 0.1441, Val Accuracy: 0.9602, Learning Rate: 0.000050\n",
      "Epoch [34/50], Train Loss: 0.1002, Train Accuracy: 0.9851, Val Loss: 0.1462, Val Accuracy: 0.9562, Learning Rate: 0.000050\n",
      "Epoch [35/50], Train Loss: 0.1022, Train Accuracy: 0.9841, Val Loss: 0.1412, Val Accuracy: 0.9562, Learning Rate: 0.000025\n",
      "Validation loss improved, saving model...\n",
      "Epoch [36/50], Train Loss: 0.0948, Train Accuracy: 0.9880, Val Loss: 0.1297, Val Accuracy: 0.9582, Learning Rate: 0.000025\n",
      "Epoch [37/50], Train Loss: 0.0886, Train Accuracy: 0.9871, Val Loss: 0.1401, Val Accuracy: 0.9522, Learning Rate: 0.000025\n",
      "Validation loss improved, saving model...\n",
      "Epoch [38/50], Train Loss: 0.0923, Train Accuracy: 0.9885, Val Loss: 0.1296, Val Accuracy: 0.9641, Learning Rate: 0.000025\n",
      "Epoch [39/50], Train Loss: 0.0871, Train Accuracy: 0.9890, Val Loss: 0.1460, Val Accuracy: 0.9582, Learning Rate: 0.000025\n",
      "Epoch [40/50], Train Loss: 0.0837, Train Accuracy: 0.9915, Val Loss: 0.1440, Val Accuracy: 0.9582, Learning Rate: 0.000025\n",
      "Epoch [41/50], Train Loss: 0.0909, Train Accuracy: 0.9895, Val Loss: 0.1377, Val Accuracy: 0.9622, Learning Rate: 0.000025\n",
      "Validation loss improved, saving model...\n",
      "Epoch [42/50], Train Loss: 0.0780, Train Accuracy: 0.9910, Val Loss: 0.1256, Val Accuracy: 0.9641, Learning Rate: 0.000025\n",
      "Epoch [43/50], Train Loss: 0.0762, Train Accuracy: 0.9915, Val Loss: 0.1296, Val Accuracy: 0.9641, Learning Rate: 0.000025\n",
      "Epoch [44/50], Train Loss: 0.0879, Train Accuracy: 0.9875, Val Loss: 0.1356, Val Accuracy: 0.9602, Learning Rate: 0.000025\n",
      "Validation loss improved, saving model...\n",
      "Epoch [45/50], Train Loss: 0.0777, Train Accuracy: 0.9955, Val Loss: 0.1122, Val Accuracy: 0.9641, Learning Rate: 0.000025\n",
      "Epoch [46/50], Train Loss: 0.0779, Train Accuracy: 0.9895, Val Loss: 0.1197, Val Accuracy: 0.9641, Learning Rate: 0.000025\n",
      "Epoch [47/50], Train Loss: 0.0839, Train Accuracy: 0.9871, Val Loss: 0.1203, Val Accuracy: 0.9602, Learning Rate: 0.000025\n",
      "Epoch [48/50], Train Loss: 0.0749, Train Accuracy: 0.9900, Val Loss: 0.1171, Val Accuracy: 0.9661, Learning Rate: 0.000025\n",
      "Validation loss improved, saving model...\n",
      "Epoch [49/50], Train Loss: 0.0719, Train Accuracy: 0.9930, Val Loss: 0.1106, Val Accuracy: 0.9681, Learning Rate: 0.000025\n",
      "Epoch [50/50], Train Loss: 0.0715, Train Accuracy: 0.9925, Val Loss: 0.1127, Val Accuracy: 0.9681, Learning Rate: 0.000025\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "# 조기 종료 변수 초기화\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # 개선이 없을 때 기다릴 에포크 수\n",
    "patience_counter = 0\n",
    "\n",
    "# 학습률 스케줄러\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(DEVICE), targets.to(DEVICE).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = criterion(outputs.view(-1), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 정확도 계산\n",
    "        predicted = (torch.sigmoid(outputs.view(-1)) > 0.5).float()\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "        total_predictions += targets.size(0)\n",
    "\n",
    "    # 훈련 데이터 정확도\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # 검증\n",
    "    val_loss, val_accuracy = evaluate_model(model, validation_loader, criterion)\n",
    "\n",
    "    # 학습률 스케줄러 적용\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # 현재 학습률 출력\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    # 조기 종료 로직\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # 손실 개선 시 카운터 리셋\n",
    "        print(\"Validation loss improved, saving model...\")\n",
    "        # 모델 저장\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, '../model/efficientnet_b3_ft_best_model_01.pth')\n",
    "    else:\n",
    "        patience_counter += 1  # 손실이 개선되지 않으면 카운터 증가\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break  # 훈련 종료\n",
    "\n",
    "    # 에포크별 정보 출력 (학습률 포함)\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {running_loss / len(train_loader):.4f}, '\n",
    "          f'Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, '\n",
    "          f'Learning Rate: {current_lr:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\AppData\\Local\\Temp\\ipykernel_9220\\59477664.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('../model/efficientnet_b3_ft_best_model_01.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (base_model): EfficientNet(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.007692307692307693, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.015384615384615385, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.02307692307692308, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.03076923076923077, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=192, bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.038461538461538464, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
       "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.04615384615384616, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
       "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.05384615384615385, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)\n",
       "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06153846153846154, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06923076923076923, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.07692307692307693, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.08461538461538462, mode=row)\n",
       "        )\n",
       "        (4): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.09230769230769233, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(576, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1076923076923077, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.11538461538461539, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.12307692307692308, mode=row)\n",
       "        )\n",
       "        (4): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.13076923076923078, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=816, bias=False)\n",
       "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(816, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.13846153846153847, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.14615384615384616, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.15384615384615385, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.16153846153846155, mode=row)\n",
       "        )\n",
       "        (4): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.16923076923076924, mode=row)\n",
       "        )\n",
       "        (5): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.17692307692307693, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1392, 1392, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1392, bias=False)\n",
       "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1392, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.18461538461538465, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)\n",
       "              (1): BatchNorm2d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(2304, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(96, 2304, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.19230769230769232, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (8): Conv2dNormActivation(\n",
       "        (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Identity()\n",
       "  )\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=1536, out_features=50, bias=True)\n",
       "  (bn1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomModel('efficientnet').to(DEVICE)\n",
    "\n",
    "checkpoint = torch.load('../model/efficientnet_b3_ft_best_model_01.pth')\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 마지막 몇 개의 레이어를 동결 해제 (features 블록 5 이후 층)\n",
    "for name, param in model.base_model.features[5:].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 동결 해제된 일부 층과 분류기 층 학습\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.1025, Test accuracy: 0.9682\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추가 20epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved, saving model...\n",
      "Epoch [1/20], Train Loss: 0.0839, Train Accuracy: 0.9846, Val Loss: 0.1025, Val Accuracy: 0.9641, Learning Rate: 0.000050\n",
      "Epoch [2/20], Train Loss: 0.0697, Train Accuracy: 0.9920, Val Loss: 0.1158, Val Accuracy: 0.9582, Learning Rate: 0.000050\n",
      "Epoch [3/20], Train Loss: 0.0704, Train Accuracy: 0.9940, Val Loss: 0.1173, Val Accuracy: 0.9602, Learning Rate: 0.000050\n",
      "Epoch [4/20], Train Loss: 0.0731, Train Accuracy: 0.9895, Val Loss: 0.1078, Val Accuracy: 0.9641, Learning Rate: 0.000050\n",
      "Epoch [5/20], Train Loss: 0.0646, Train Accuracy: 0.9880, Val Loss: 0.1144, Val Accuracy: 0.9641, Learning Rate: 0.000025\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "# 조기 종료 변수 초기화\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # 개선이 없을 때 기다릴 에포크 수\n",
    "patience_counter = 0\n",
    "\n",
    "# 학습률 스케줄러\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(DEVICE), targets.to(DEVICE).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = criterion(outputs.view(-1), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 정확도 계산\n",
    "        predicted = (torch.sigmoid(outputs.view(-1)) > 0.5).float()\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "        total_predictions += targets.size(0)\n",
    "\n",
    "    # 훈련 데이터 정확도\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # 검증\n",
    "    val_loss, val_accuracy = evaluate_model(model, validation_loader, criterion)\n",
    "\n",
    "    # 학습률 스케줄러 적용\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # 현재 학습률 출력\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    # 조기 종료 로직\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # 손실 개선 시 카운터 리셋\n",
    "        print(\"Validation loss improved, saving model...\")\n",
    "        # 모델 저장\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, '../model/efficientnet_b3_ft_best_model_01.pth')\n",
    "    else:\n",
    "        patience_counter += 1  # 손실이 개선되지 않으면 카운터 증가\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break  # 훈련 종료\n",
    "\n",
    "    # 에포크별 정보 출력 (학습률 포함)\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {running_loss / len(train_loader):.4f}, '\n",
    "          f'Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, '\n",
    "          f'Learning Rate: {current_lr:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\AppData\\Local\\Temp\\ipykernel_9220\\59477664.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('../model/efficientnet_b3_ft_best_model_01.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (base_model): EfficientNet(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(40, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.007692307692307693, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.015384615384615385, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.02307692307692308, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.03076923076923077, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=192, bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.038461538461538464, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
       "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.04615384615384616, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
       "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.05384615384615385, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)\n",
       "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06153846153846154, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06923076923076923, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.07692307692307693, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.08461538461538462, mode=row)\n",
       "        )\n",
       "        (4): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.09230769230769233, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "              (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(576, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1076923076923077, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.11538461538461539, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.12307692307692308, mode=row)\n",
       "        )\n",
       "        (4): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.13076923076923078, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(816, 816, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=816, bias=False)\n",
       "              (1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(816, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.13846153846153847, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.14615384615384616, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.15384615384615385, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.16153846153846155, mode=row)\n",
       "        )\n",
       "        (4): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.16923076923076924, mode=row)\n",
       "        )\n",
       "        (5): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.17692307692307693, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1392, 1392, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1392, bias=False)\n",
       "              (1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1392, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.18461538461538465, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2304, bias=False)\n",
       "              (1): BatchNorm2d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(2304, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(96, 2304, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.19230769230769232, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (8): Conv2dNormActivation(\n",
       "        (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Identity()\n",
       "  )\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=1536, out_features=50, bias=True)\n",
       "  (bn1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomModel('efficientnet').to(DEVICE)\n",
    "\n",
    "checkpoint = torch.load('../model/efficientnet_b3_ft_best_model_01.pth')\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 마지막 몇 개의 레이어를 동결 해제 (features 블록 5 이후 층)\n",
    "for name, param in model.base_model.features[5:].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 동결 해제된 일부 층과 분류기 층 학습\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0962, Test accuracy: 0.9713\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
