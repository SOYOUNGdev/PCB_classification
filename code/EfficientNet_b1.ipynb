{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "CUDA available: True\n",
      "CUDA version in PyTorch: 11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# GPU 사용 설정\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "# CUDA 사용 가능 여부 확인\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")  # True여야 함\n",
    "\n",
    "# PyTorch에서 사용하는 CUDA 버전 확인\n",
    "print(f\"CUDA version in PyTorch: {torch.version.cuda}\")\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\isy\\test\\Lib\\site-packages\\albumentations\\__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.18 (you have 1.4.17). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 임포트\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NG 예측결과:\n",
      "Efficientnet: NG (결함 있음), 확률: 0.6173\n",
      "Efficientnet: OK (정상), 확률: 0.4670\n",
      "Efficientnet: NG (결함 있음), 확률: 0.6463\n",
      "Efficientnet: NG (결함 있음), 확률: 0.6715\n",
      "Efficientnet: NG (결함 있음), 확률: 0.5973\n",
      "Efficientnet: NG (결함 있음), 확률: 0.6222\n",
      "Efficientnet: NG (결함 있음), 확률: 0.6606\n",
      "Efficientnet: NG (결함 있음), 확률: 0.5893\n",
      "Efficientnet: NG (결함 있음), 확률: 0.5827\n",
      "Efficientnet: NG (결함 있음), 확률: 0.6079\n",
      "OK 예측결과:\n",
      "Efficientnet: NG (결함 있음), 확률: 0.5900\n",
      "Efficientnet: NG (결함 있음), 확률: 0.5702\n",
      "Efficientnet: OK (정상), 확률: 0.4841\n",
      "Efficientnet: NG (결함 있음), 확률: 0.5781\n",
      "Efficientnet: NG (결함 있음), 확률: 0.5663\n",
      "Efficientnet: NG (결함 있음), 확률: 0.6192\n",
      "Efficientnet: NG (결함 있음), 확률: 0.6229\n",
      "Efficientnet: NG (결함 있음), 확률: 0.5889\n",
      "Efficientnet: NG (결함 있음), 확률: 0.6276\n",
      "Efficientnet: NG (결함 있음), 확률: 0.5725\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# ResNet 및 Inception 모델 수정\n",
    "class CustomModel(torch.nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(CustomModel, self).__init__()\n",
    "        if model_name == 'efficientnet':\n",
    "            self.model = models.efficientnet_b1(weights='IMAGENET1K_V1') \n",
    "            self.model.classifier[1] = torch.nn.Linear(self.model.classifier[1].in_features, 1)  # 이진 분류\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# efficientnet 모델 로드\n",
    "efficientnet_model = CustomModel(model_name='efficientnet')\n",
    "efficientnet_model.eval()  # 평가 모드로 설정\n",
    "\n",
    "# 전처리 파이프라인 정의\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # OpenCV 이미지를 PIL 이미지로 변환\n",
    "    transforms.Resize((240, 240)),  # 모델 입력 크기에 맞게 크기 조정\n",
    "    transforms.ToTensor(),  # 텐서로 변환\n",
    "])\n",
    "\n",
    "# 이미지 전처리 함수\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # RGB로 변환\n",
    "    image = preprocess(image)  # 전처리 적용\n",
    "    image = image.unsqueeze(0)  # 배치 차원 추가\n",
    "    return image\n",
    "\n",
    "# 예측 함수 정의\n",
    "def predict_image(model, image_path, device):\n",
    "    model.to(DEVICE)  # 모델을 디바이스로 이동\n",
    "    image = preprocess_image(image_path).to(device)  # 전처리 후 디바이스로 이동\n",
    "    with torch.no_grad():  # 평가 모드에서는 gradient 계산 비활성화\n",
    "        output = model(image)\n",
    "        predicted_prob = torch.sigmoid(output).item()  # 확률로 변환\n",
    "\n",
    "    # 0.5를 기준으로 정상/비정상 예측\n",
    "    prediction = 'NG (결함 있음)' if predicted_prob > 0.5 else 'OK (정상)'\n",
    "    return prediction, predicted_prob\n",
    "\n",
    "# NG 폴더 경로\n",
    "ng_folder_path = '../PCB_imgs/all/resize/NG/' \n",
    "# OK 폴더 경로\n",
    "ok_folder_path = '../PCB_imgs/all/resize/OK/' \n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # 디바이스 설정\n",
    "\n",
    "# 각 폴더에서 무작위로 10개의 이미지를 가져오기\n",
    "def load_random_images(folder_path, num_images=10):\n",
    "    images = os.listdir(folder_path)\n",
    "    random_images = random.sample(images, min(num_images, len(images)))\n",
    "    return [os.path.join(folder_path, img) for img in random_images]\n",
    "\n",
    "# NG 및 OK 이미지 로드\n",
    "ng_images = load_random_images(ng_folder_path)\n",
    "ok_images = load_random_images(ok_folder_path)\n",
    "\n",
    "# NG 이미지 예측 결과와 확률\n",
    "print(f'NG 예측결과:')\n",
    "for image_path in ng_images:\n",
    "    prediction, prob = predict_image(efficientnet_model, image_path, DEVICE) \n",
    "    print(f\"Efficientnet: {prediction}, 확률: {prob:.4f}\")\n",
    "\n",
    "# OK 이미지 예측 결과와 확률\n",
    "print(f'OK 예측결과:')\n",
    "for image_path in ok_images:\n",
    "    prediction, prob = predict_image(efficientnet_model, image_path, DEVICE) \n",
    "    print(f\"Efficientnet: {prediction}, 확률: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 데이터 수: 2008\n",
      "Validation 데이터 수: 502\n",
      "Test 데이터 수: 628\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "import pandas as pd\n",
    "\n",
    "IMAGE_SIZE = 240\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "# 이미지 변환 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),  # 이미지를 Tensor로 변환 (0~255 범위를 0~1 범위로 정규화)\n",
    "])\n",
    "\n",
    "# 데이터셋 디렉토리 설정\n",
    "train_dir = '../PCB_imgs/all/resize/train'\n",
    "val_dir = '../PCB_imgs/all/resize/validation'\n",
    "test_dir = '../PCB_imgs/all/resize/test'\n",
    "\n",
    "# ImageFolder로 데이터셋 불러오기\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "# 파일 경로 및 타겟 추출\n",
    "train_file_paths = [img[0] for img in train_dataset.imgs]\n",
    "train_targets = train_dataset.targets\n",
    "\n",
    "val_file_paths = [img[0] for img in val_dataset.imgs]\n",
    "val_targets = val_dataset.targets\n",
    "\n",
    "test_file_paths = [img[0] for img in test_dataset.imgs]\n",
    "test_targets = test_dataset.targets\n",
    "\n",
    "# DataFrame 생성\n",
    "train_df = pd.DataFrame({'file_paths': train_file_paths, 'targets': train_targets})\n",
    "validation_df = pd.DataFrame({'file_paths': val_file_paths, 'targets': val_targets})\n",
    "test_df = pd.DataFrame({'file_paths': test_file_paths, 'targets': test_targets})\n",
    "\n",
    "# 확인을 위해 각 데이터셋의 크기 출력\n",
    "print(f\"Train 데이터 수: {len(train_df)}\")\n",
    "print(f\"Validation 데이터 수: {len(validation_df)}\")\n",
    "print(f\"Test 데이터 수: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 240, 240])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# resize된 데이터셋 로드\n",
    "input_dir = '../PCB_imgs/all/resize/'\n",
    "dataset = datasets.ImageFolder(root=input_dir, transform=transform)\n",
    "\n",
    "# 데이터 로더 정의\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 데이터 로더에서 데이터 가져오기\n",
    "for images, labels in dataloader:\n",
    "    print(images.shape)  # 배치의 이미지 텐서 크기 확인\n",
    "    break  # 첫 번째 배치만 확인\n",
    "    # 배치 크기, 채널 수(RGB), 이미지 크기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커스텀 데이터세트 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_paths, targets, aug=None, preprocess=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.targets = targets\n",
    "        self.aug = aug\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_path = self.file_paths[index]\n",
    "        target = self.targets[index]\n",
    "        \n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "\n",
    "        if self.aug is not None:\n",
    "            image = self.aug(image=image)['image']\n",
    "\n",
    "        if self.preprocess is not None:\n",
    "            image = self.preprocess(image)\n",
    "\n",
    "        image = np.transpose(image, (2, 0, 1))  # (H, W, C) -> (C, H, W)\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 데이터셋 인스턴스 생성\n",
    "train_dataset = CustomDataset(train_df['file_paths'].values, train_df['targets'].values)\n",
    "validation_dataset = CustomDataset(validation_df['file_paths'].values, validation_df['targets'].values)\n",
    "test_dataset = CustomDataset(test_df['file_paths'].values, test_df['targets'].values)\n",
    "\n",
    "# DataLoader 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 함수 정의\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device).float()\n",
    "            outputs = model(images)\n",
    "            outputs = outputs.view(-1) # torch.Size([batch_size])로 변환\n",
    "            loss = criterion(outputs, labels)  # 손실 계산\n",
    "            running_test_loss += loss.item()\n",
    "            \n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()  # 0.5 기준으로 이진 분류\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "            \n",
    "    # 검증 손실과 정확도\n",
    "    test_loss = running_test_loss / len(test_loader)\n",
    "    test_accuracy = correct_test / total_test\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "efficientnet b1 구조만 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "# 모델 정의\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model_name='efficientnet'):\n",
    "        super(CustomModel, self).__init__()\n",
    "        if model_name == 'efficientnet':\n",
    "            self.base_model = torchvision.models.efficientnet_b1()\n",
    "            self.base_model.classifier = nn.Identity()  # 마지막 분류기 제거\n",
    "        elif model_name == 'resnet50':\n",
    "            self.base_model = torchvision.models.resnet50()\n",
    "            self.base_model = nn.Sequential(*list(self.base_model.children())[:-1])  # 마지막 레이어 제거\n",
    "        elif model_name == 'inception':\n",
    "            self.base_model = torchvision.models.inception_v3()\n",
    "            self.base_model.classifier = nn.Identity()  # 마지막 분류기 제거\n",
    "\n",
    "        self.fc1 = nn.Linear(self._get_features_dim(model_name), 1)\n",
    "\n",
    "    def _get_features_dim(self, model_name):\n",
    "        if model_name == 'efficientnet':\n",
    "            return 1280  # EfficientNet의 출력 차원\n",
    "        elif model_name == 'resnet50':\n",
    "            return 2048  # ResNet50의 출력 차원\n",
    "        elif model_name == 'inception':\n",
    "            return 2048  # Inception의 출력 차원\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        \n",
    "        if isinstance(x, tuple): \n",
    "            x = x[0]  \n",
    "\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = self.fc1(x)  \n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 초기화\n",
    "model = CustomModel(model_name='efficientnet').to(DEVICE)\n",
    "# 손실 함수 및 최적화함수 정의\n",
    "criterion = nn.BCEWithLogitsLoss()  # 이진 교차 엔트로피 손실\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Adam 옵티마이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 0.6810, Train Accuracy: 0.6036, Val Loss: 0.6875, Val Accuracy: 0.5857\n",
      "Validation loss improved, saving model...\n",
      "Epoch [2/20], Train Loss: 0.5166, Train Accuracy: 0.7440, Val Loss: 0.3891, Val Accuracy: 0.8048\n",
      "Validation loss improved, saving model...\n",
      "Epoch [3/20], Train Loss: 0.4108, Train Accuracy: 0.8123, Val Loss: 0.3514, Val Accuracy: 0.8426\n",
      "Validation loss improved, saving model...\n",
      "Epoch [4/20], Train Loss: 0.3762, Train Accuracy: 0.8222, Val Loss: 0.3457, Val Accuracy: 0.8426\n",
      "Validation loss improved, saving model...\n",
      "Epoch [5/20], Train Loss: 0.3303, Train Accuracy: 0.8616, Val Loss: 0.3766, Val Accuracy: 0.8327\n",
      "Epoch [6/20], Train Loss: 0.2628, Train Accuracy: 0.8889, Val Loss: 0.4501, Val Accuracy: 0.8327\n",
      "Epoch [7/20], Train Loss: 0.2611, Train Accuracy: 0.8884, Val Loss: 0.3786, Val Accuracy: 0.8606\n",
      "Epoch [8/20], Train Loss: 0.2180, Train Accuracy: 0.9064, Val Loss: 0.3962, Val Accuracy: 0.8506\n",
      "Epoch [9/20], Train Loss: 0.1990, Train Accuracy: 0.9228, Val Loss: 0.4006, Val Accuracy: 0.8705\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "# 조기 종료 변수를 초기화합니다.\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # 개선이 없을 때 기다릴 에포크 수\n",
    "patience_counter = 0\n",
    "\n",
    "# 모델 훈련\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(DEVICE), targets.to(DEVICE).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = criterion(outputs.view(-1), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 정확도 계산\n",
    "        predicted = (torch.sigmoid(outputs.view(-1)) > 0.5).float()\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "        total_predictions += targets.size(0)\n",
    "\n",
    "    # 훈련 데이터 정확도\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # 검증\n",
    "    val_loss, val_accuracy = evaluate_model(model, validation_loader, criterion)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {running_loss / len(train_loader):.4f}, '\n",
    "          f'Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # 조기 종료 로직\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # 손실 개선 시 카운터 리셋\n",
    "        print(\"Validation loss improved, saving model...\")\n",
    "    else:\n",
    "        patience_counter += 1  # 손실이 개선되지 않으면 카운터 증가\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break  # 훈련 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.3406, Test accuracy: 0.8710\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Efficientnet + Imagenet 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "# 모델 정의\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model_name='efficientnet'):\n",
    "        super(CustomModel, self).__init__()\n",
    "        if model_name == 'efficientnet':\n",
    "            self.base_model = torchvision.models.efficientnet_b1(weights='IMAGENET1K_V1')\n",
    "            self.base_model.classifier = nn.Identity()  # 마지막 분류기 제거\n",
    "        elif model_name == 'resnet50':\n",
    "            self.base_model = torchvision.models.resnet50(weights='IMAGENET1K_V1')\n",
    "            self.base_model = nn.Sequential(*list(self.base_model.children())[:-1])  # 마지막 레이어 제거\n",
    "        elif model_name == 'inception':\n",
    "            self.base_model = torchvision.models.inception_v3(weights='IMAGENET1K_V1')\n",
    "            self.base_model.classifier = nn.Identity()  # 마지막 분류기 제거\n",
    "\n",
    "        self.fc1 = nn.Linear(self._get_features_dim(model_name), 1)\n",
    "\n",
    "    def _get_features_dim(self, model_name):\n",
    "        if model_name == 'efficientnet':\n",
    "            return 1280  # EfficientNet의 출력 차원\n",
    "        elif model_name == 'resnet50':\n",
    "            return 2048  # ResNet50의 출력 차원\n",
    "        elif model_name == 'inception':\n",
    "            return 2048  # Inception의 출력 차원\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        if isinstance(x, tuple): \n",
    "            x = x[0] \n",
    "        \n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = self.fc1(x)  \n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 초기화\n",
    "model = CustomModel(model_name='efficientnet').to(DEVICE)\n",
    "# 손실 함수 및 최적화함수 정의\n",
    "criterion = nn.BCEWithLogitsLoss()  # 이진 교차 엔트로피 손실\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Adam 옵티마이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 0.6522, Train Accuracy: 0.6419, Val Loss: 0.6797, Val Accuracy: 0.5857\n",
      "Validation loss improved, saving model...\n",
      "Epoch [2/20], Train Loss: 0.5045, Train Accuracy: 0.7605, Val Loss: 0.4254, Val Accuracy: 0.7988\n",
      "Validation loss improved, saving model...\n",
      "Epoch [3/20], Train Loss: 0.4316, Train Accuracy: 0.8038, Val Loss: 0.3448, Val Accuracy: 0.8566\n",
      "Validation loss improved, saving model...\n",
      "Epoch [4/20], Train Loss: 0.3708, Train Accuracy: 0.8327, Val Loss: 0.4352, Val Accuracy: 0.8347\n",
      "Epoch [5/20], Train Loss: 0.3190, Train Accuracy: 0.8601, Val Loss: 0.3762, Val Accuracy: 0.8486\n",
      "Epoch [6/20], Train Loss: 0.2832, Train Accuracy: 0.8820, Val Loss: 0.3614, Val Accuracy: 0.8426\n",
      "Epoch [7/20], Train Loss: 0.2170, Train Accuracy: 0.9143, Val Loss: 0.4275, Val Accuracy: 0.8506\n",
      "Epoch [8/20], Train Loss: 0.2001, Train Accuracy: 0.9208, Val Loss: 0.4819, Val Accuracy: 0.8546\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "# 조기 종료 변수를 초기화합니다.\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # 개선이 없을 때 기다릴 에포크 수\n",
    "patience_counter = 0\n",
    "\n",
    "# 모델 훈련\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(DEVICE), targets.to(DEVICE).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = criterion(outputs.view(-1), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 정확도 계산\n",
    "        predicted = (torch.sigmoid(outputs.view(-1)) > 0.5).float()\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "        total_predictions += targets.size(0)\n",
    "\n",
    "    # 훈련 데이터 정확도\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # 검증\n",
    "    val_loss, val_accuracy = evaluate_model(model, validation_loader, criterion)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {running_loss / len(train_loader):.4f}, '\n",
    "          f'Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # 조기 종료 로직\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # 손실 개선 시 카운터 리셋\n",
    "        print(\"Validation loss improved, saving model...\")\n",
    "    else:\n",
    "        patience_counter += 1  # 손실이 개선되지 않으면 카운터 증가\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break  # 훈련 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3495, Test Accuracy: 0.8599\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터세트로 평가\n",
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련데이터의 손실값은 계속 줄고, 정확도는 증가하는 반면, 검증 데이터의 손실값과 정확도는 줄지않음.  \n",
    "##### 데이터 증강, dropout 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 데이터 증강 정의\n",
    "aug = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),                # 좌우 반전\n",
    "    A.VerticalFlip(p=0.5),                  # 상하 반전\n",
    "    A.Rotate(limit=10, p=0.5),              # 작은 각도 회전 (10도 내외)\n",
    "    A.RandomBrightnessContrast(p=0.5),      # 밝기 및 대비 조절\n",
    "])\n",
    "\n",
    "# 데이터셋 인스턴스 생성\n",
    "train_dataset = CustomDataset(train_df['file_paths'].values, train_df['targets'].values, aug=aug)\n",
    "validation_dataset = CustomDataset(validation_df['file_paths'].values, validation_df['targets'].values)\n",
    "test_dataset = CustomDataset(test_df['file_paths'].values, test_df['targets'].values)\n",
    "\n",
    "# DataLoader 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model_name='efficientnet'):\n",
    "        super(CustomModel, self).__init__()\n",
    "        if model_name == 'efficientnet':\n",
    "            self.base_model = torchvision.models.efficientnet_b1(weights='IMAGENET1K_V1')\n",
    "            self.base_model.classifier = nn.Identity()  # 마지막 분류기 제거\n",
    "        elif model_name == 'resnet50':\n",
    "            self.base_model = torchvision.models.resnet50(weights='IMAGENET1K_V1')\n",
    "            self.base_model = nn.Sequential(*list(self.base_model.children())[:-1])  # 마지막 레이어 제거\n",
    "        elif model_name == 'inception':\n",
    "            self.base_model = torchvision.models.inception_v3(weights='IMAGENET1K_V1')\n",
    "            self.base_model.classifier = nn.Identity()  # 마지막 분류기 제거\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(self._get_features_dim(model_name), 1)\n",
    "\n",
    "    def _get_features_dim(self, model_name):\n",
    "        if model_name == 'efficientnet':\n",
    "            return 1280  # EfficientNet B0의 출력 차원\n",
    "        elif model_name == 'resnet50':\n",
    "            return 2048  # ResNet50의 출력 차원\n",
    "        elif model_name == 'inception':\n",
    "            return 2048  # Inception의 출력 차원\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        \n",
    "        # Inception 모델에 대한 수정\n",
    "        if isinstance(x, tuple):  # Inception 모델이 여러 출력을 반환하는 경우\n",
    "            x = x[0]  # 첫 번째 출력을 선택\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)  # Sigmoid 제거\n",
    "\n",
    "        return x  # 최종 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 초기화\n",
    "model = CustomModel(model_name='efficientnet').to(DEVICE)\n",
    "# 손실 함수 및 최적화함수 정의\n",
    "criterion = nn.BCEWithLogitsLoss()  # 이진 교차 엔트로피 손실\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Adam 옵티마이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 0.5063, Train Accuracy: 0.7560, Val Loss: 0.3639, Val Accuracy: 0.8486\n",
      "Validation loss improved, saving model...\n",
      "Epoch [2/20], Train Loss: 0.3296, Train Accuracy: 0.8471, Val Loss: 0.2887, Val Accuracy: 0.8924\n",
      "Validation loss improved, saving model...\n",
      "Epoch [3/20], Train Loss: 0.2515, Train Accuracy: 0.8949, Val Loss: 0.2420, Val Accuracy: 0.9084\n",
      "Validation loss improved, saving model...\n",
      "Epoch [4/20], Train Loss: 0.1999, Train Accuracy: 0.9228, Val Loss: 0.2100, Val Accuracy: 0.9323\n",
      "Validation loss improved, saving model...\n",
      "Epoch [5/20], Train Loss: 0.1448, Train Accuracy: 0.9422, Val Loss: 0.1781, Val Accuracy: 0.9402\n",
      "Validation loss improved, saving model...\n",
      "Epoch [6/20], Train Loss: 0.1107, Train Accuracy: 0.9626, Val Loss: 0.1527, Val Accuracy: 0.9422\n",
      "Validation loss improved, saving model...\n",
      "Epoch [7/20], Train Loss: 0.0964, Train Accuracy: 0.9681, Val Loss: 0.1551, Val Accuracy: 0.9402\n",
      "Epoch [8/20], Train Loss: 0.0832, Train Accuracy: 0.9731, Val Loss: 0.1306, Val Accuracy: 0.9602\n",
      "Validation loss improved, saving model...\n",
      "Epoch [9/20], Train Loss: 0.0754, Train Accuracy: 0.9746, Val Loss: 0.1387, Val Accuracy: 0.9502\n",
      "Epoch [10/20], Train Loss: 0.0494, Train Accuracy: 0.9851, Val Loss: 0.1243, Val Accuracy: 0.9602\n",
      "Validation loss improved, saving model...\n",
      "Epoch [11/20], Train Loss: 0.0506, Train Accuracy: 0.9831, Val Loss: 0.1211, Val Accuracy: 0.9661\n",
      "Validation loss improved, saving model...\n",
      "Epoch [12/20], Train Loss: 0.0414, Train Accuracy: 0.9861, Val Loss: 0.1323, Val Accuracy: 0.9582\n",
      "Epoch [13/20], Train Loss: 0.0349, Train Accuracy: 0.9875, Val Loss: 0.1179, Val Accuracy: 0.9622\n",
      "Validation loss improved, saving model...\n",
      "Epoch [14/20], Train Loss: 0.0219, Train Accuracy: 0.9955, Val Loss: 0.1263, Val Accuracy: 0.9562\n",
      "Epoch [15/20], Train Loss: 0.0328, Train Accuracy: 0.9895, Val Loss: 0.1148, Val Accuracy: 0.9661\n",
      "Validation loss improved, saving model...\n",
      "Epoch [16/20], Train Loss: 0.0302, Train Accuracy: 0.9895, Val Loss: 0.1084, Val Accuracy: 0.9681\n",
      "Validation loss improved, saving model...\n",
      "Epoch [17/20], Train Loss: 0.0213, Train Accuracy: 0.9935, Val Loss: 0.1212, Val Accuracy: 0.9741\n",
      "Epoch [18/20], Train Loss: 0.0208, Train Accuracy: 0.9935, Val Loss: 0.1266, Val Accuracy: 0.9721\n",
      "Epoch [19/20], Train Loss: 0.0159, Train Accuracy: 0.9955, Val Loss: 0.1212, Val Accuracy: 0.9681\n",
      "Epoch [20/20], Train Loss: 0.0252, Train Accuracy: 0.9925, Val Loss: 0.1294, Val Accuracy: 0.9602\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "# 조기 종료 변수를 초기화합니다.\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # 개선이 없을 때 기다릴 에포크 수\n",
    "patience_counter = 0\n",
    "\n",
    "# 모델 훈련\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(DEVICE), targets.to(DEVICE).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = criterion(outputs.view(-1), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 정확도 계산\n",
    "        predicted = (torch.sigmoid(outputs.view(-1)) > 0.5).float()\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "        total_predictions += targets.size(0)\n",
    "\n",
    "    # 훈련 데이터 정확도\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # 검증\n",
    "    val_loss, val_accuracy = evaluate_model(model, validation_loader, criterion)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {running_loss / len(train_loader):.4f}, '\n",
    "          f'Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # 조기 종료 로직\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # 손실 개선 시 카운터 리셋\n",
    "        print(\"Validation loss improved, saving model...\")\n",
    "        # 모델 저장\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, '../model/efficientnet_b1_best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1  # 손실이 개선되지 않으면 카운터 증가\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break  # 훈련 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\AppData\\Local\\Temp\\ipykernel_724\\3435278120.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('../model/efficientnet_b1_best_model.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (base_model): EfficientNet(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.10434782608695654, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.11304347826086956, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.12173913043478261, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1391304347826087, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.14782608695652175, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1565217391304348, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.16521739130434784, mode=row)\n",
       "        )\n",
       "        (4): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1920, 1920, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1920, bias=False)\n",
       "              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1920, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(80, 1920, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1920, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.19130434782608696, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (8): Conv2dNormActivation(\n",
       "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Identity()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=1280, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 클래스 정의 (CustomModel)\n",
    "model = CustomModel(model_name='efficientnet').to(DEVICE)\n",
    "\n",
    "# 저장된 체크포인트 불러오기\n",
    "checkpoint = torch.load('../model/efficientnet_b1_best_model.pth')\n",
    "\n",
    "# 모델에 체크포인트 적용\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Adam 옵티마이저\n",
    "\n",
    "# 모델을 평가 모드로 전환\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0996, Test Accuracy: 0.9650\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터세트로 평가\n",
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오버샘플링 + 증강 + bn층 추가 + 학습률 스케줄러 + l2 규제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NG 이미지 수: 1178\n",
      "OK 이미지 수: 830\n",
      "오버샘플링 후 NG 이미지 수: 1178\n",
      "오버샘플링 후 OK 이미지 수: 1178\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image \n",
    "\n",
    "class OverSamplingDataset(Dataset):\n",
    "    def __init__(self, ng_dir, ok_dir, transform=None, aug=None):\n",
    "        self.ng_images = [os.path.join(ng_dir, img) for img in os.listdir(ng_dir)]\n",
    "        self.ok_images = [os.path.join(ok_dir, img) for img in os.listdir(ok_dir)]\n",
    "        self.transform = transform\n",
    "        self.aug = aug\n",
    "\n",
    "        # NG 이미지 수와 OK 이미지 수 출력\n",
    "        print(f'NG 이미지 수: {len(self.ng_images)}')\n",
    "        print(f'OK 이미지 수: {len(self.ok_images)}')\n",
    "\n",
    "        # OK 이미지를 NG 이미지 수 만큼 반복하여 오버샘플링\n",
    "        self.ok_images = self.ok_images * (len(self.ng_images) // len(self.ok_images)) + self.ok_images[:len(self.ng_images) % len(self.ok_images)]\n",
    "\n",
    "        # 오버샘플링 후 OK 이미지 수가 830이 되도록 맞춤\n",
    "        self.ok_images = self.ok_images[:len(self.ng_images)]  # NG와 동일한 수로 제한\n",
    "\n",
    "        # 최종 데이터셋은 NG와 OK 이미지의 합\n",
    "        self.images = self.ng_images + self.ok_images\n",
    "        self.labels = [0] * len(self.ng_images) + [1] * len(self.ok_images)  # NG=0, OK=1\n",
    "\n",
    "        # 최종 이미지와 레이블 수 출력\n",
    "        print(f'오버샘플링 후 NG 이미지 수: {len(self.ng_images)}')\n",
    "        print(f'오버샘플링 후 OK 이미지 수: {len(self.ok_images)}')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # PIL 이미지로 열기\n",
    "\n",
    "        if self.aug is not None:\n",
    "            # PIL 이미지를 NumPy 배열로 변환\n",
    "            image = np.array(image)  # np.array로 변환\n",
    "            image = self.aug(image=image)['image']  # 데이터 증강 적용\n",
    "            image = Image.fromarray(image)  # 다시 PIL 이미지로 변환\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "# 데이터 변환 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 데이터 증강 정의\n",
    "aug = A.Compose([\n",
    "    A.HorizontalFlip(p=0.6),                # 좌우 반전\n",
    "    A.VerticalFlip(p=0.6),                  # 상하 반전\n",
    "    A.Rotate(limit=10, p=0.6),              # 작은 각도 회전 (10도 내외)\n",
    "    A.RandomBrightnessContrast(p=0.6),      # 밝기 및 대비 조절\n",
    "])\n",
    "\n",
    "# 데이터셋 인스턴스 생성\n",
    "train_dataset = OverSamplingDataset(ng_dir='../PCB_imgs/all/resize/train/NG/', ok_dir='../PCB_imgs/all/resize/train/OK/', transform=transform, aug=aug)\n",
    "validation_dataset = datasets.ImageFolder(root=val_dir, transform=transform)  # 검증, 테스트 데이터는 그대로 사용\n",
    "test_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
    "\n",
    "# DataLoader 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model_name='efficientnet'):\n",
    "        super(CustomModel, self).__init__()\n",
    "        if model_name == 'efficientnet':\n",
    "            self.base_model = torchvision.models.efficientnet_b1(weights='IMAGENET1K_V1')\n",
    "            self.base_model.classifier = nn.Identity()  # 마지막 분류기 제거\n",
    "        elif model_name == 'resnet50':\n",
    "            self.base_model = torchvision.models.resnet50(weights='IMAGENET1K_V1')\n",
    "            self.base_model = nn.Sequential(*list(self.base_model.children())[:-1])  # 마지막 레이어 제거\n",
    "        elif model_name == 'inception':\n",
    "            self.base_model = torchvision.models.inception_v3(weights='IMAGENET1K_V1')\n",
    "            self.base_model.classifier = nn.Identity()  # 마지막 분류기 제거\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(self._get_features_dim(model_name), 1)\n",
    "        self.bn1 = nn.BatchNorm1d(self._get_features_dim(model_name))\n",
    "\n",
    "    def _get_features_dim(self, model_name):\n",
    "        if model_name == 'efficientnet':\n",
    "            return 1280  # EfficientNet의 출력 차원\n",
    "        elif model_name == 'resnet50':\n",
    "            return 2048  # ResNet50의 출력 차원\n",
    "        elif model_name == 'inception':\n",
    "            return 2048  # Inception의 출력 차원\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        if isinstance(x, tuple):  \n",
    "            x = x[0] \n",
    "        \n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x) \n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 초기화\n",
    "model = CustomModel(model_name='efficientnet').to(DEVICE)\n",
    "# 손실 함수 및 최적화함수 정의\n",
    "criterion = nn.BCEWithLogitsLoss()  # 이진 교차 엔트로피 손실\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)  # Adam 옵티마이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 0.4784, Train Accuracy: 0.7691, Val Loss: 0.3524, Val Accuracy: 0.8347\n",
      "Validation loss improved, saving model...\n",
      "Epoch [2/20], Train Loss: 0.3261, Train Accuracy: 0.8608, Val Loss: 0.2569, Val Accuracy: 0.8944\n",
      "Validation loss improved, saving model...\n",
      "Epoch [3/20], Train Loss: 0.2384, Train Accuracy: 0.8973, Val Loss: 0.2130, Val Accuracy: 0.9183\n",
      "Validation loss improved, saving model...\n",
      "Epoch [4/20], Train Loss: 0.1790, Train Accuracy: 0.9278, Val Loss: 0.1746, Val Accuracy: 0.9323\n",
      "Validation loss improved, saving model...\n",
      "Epoch [5/20], Train Loss: 0.1517, Train Accuracy: 0.9368, Val Loss: 0.1804, Val Accuracy: 0.9303\n",
      "Epoch [6/20], Train Loss: 0.1345, Train Accuracy: 0.9491, Val Loss: 0.1464, Val Accuracy: 0.9442\n",
      "Validation loss improved, saving model...\n",
      "Epoch [7/20], Train Loss: 0.0999, Train Accuracy: 0.9622, Val Loss: 0.1531, Val Accuracy: 0.9323\n",
      "Epoch [8/20], Train Loss: 0.0958, Train Accuracy: 0.9639, Val Loss: 0.1374, Val Accuracy: 0.9502\n",
      "Validation loss improved, saving model...\n",
      "Epoch [9/20], Train Loss: 0.0799, Train Accuracy: 0.9733, Val Loss: 0.1425, Val Accuracy: 0.9542\n",
      "Epoch [10/20], Train Loss: 0.0561, Train Accuracy: 0.9805, Val Loss: 0.1335, Val Accuracy: 0.9622\n",
      "Validation loss improved, saving model...\n",
      "Epoch [11/20], Train Loss: 0.0566, Train Accuracy: 0.9796, Val Loss: 0.1065, Val Accuracy: 0.9681\n",
      "Validation loss improved, saving model...\n",
      "Epoch [12/20], Train Loss: 0.0536, Train Accuracy: 0.9796, Val Loss: 0.1249, Val Accuracy: 0.9622\n",
      "Epoch [13/20], Train Loss: 0.0396, Train Accuracy: 0.9856, Val Loss: 0.1130, Val Accuracy: 0.9661\n",
      "Epoch [14/20], Train Loss: 0.0322, Train Accuracy: 0.9885, Val Loss: 0.1189, Val Accuracy: 0.9701\n",
      "Epoch [15/20], Train Loss: 0.0287, Train Accuracy: 0.9885, Val Loss: 0.1068, Val Accuracy: 0.9761\n",
      "Epoch [16/20], Train Loss: 0.0280, Train Accuracy: 0.9919, Val Loss: 0.1115, Val Accuracy: 0.9761\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "# 조기 종료 변수를 초기화합니다.\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # 개선이 없을 때 기다릴 에포크 수\n",
    "patience_counter = 0\n",
    "\n",
    "# 학습률 스케줄러\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "# 모델 훈련\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(DEVICE), targets.to(DEVICE).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = criterion(outputs.view(-1), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 정확도 계산\n",
    "        predicted = (torch.sigmoid(outputs.view(-1)) > 0.5).float()\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "        total_predictions += targets.size(0)\n",
    "\n",
    "    # 훈련 데이터 정확도\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # 검증\n",
    "    val_loss, val_accuracy = evaluate_model(model, validation_loader, criterion)\n",
    "\n",
    "    # 학습률 스케줄러 적용\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {running_loss / len(train_loader):.4f}, '\n",
    "          f'Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # 조기 종료 로직\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # 손실 개선 시 카운터 리셋\n",
    "        print(\"Validation loss improved, saving model...\")\n",
    "        # 모델 저장\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, '../model/efficientnet_b1_best_model.2.pth')\n",
    "    else:\n",
    "        patience_counter += 1  # 손실이 개선되지 않으면 카운터 증가\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break  # 훈련 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\AppData\\Local\\Temp\\ipykernel_724\\2733988964.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('../model/efficientnet_b1_best_model.2.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (base_model): EfficientNet(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.10434782608695654, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.11304347826086956, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.12173913043478261, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1391304347826087, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.14782608695652175, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1565217391304348, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.16521739130434784, mode=row)\n",
       "        )\n",
       "        (4): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1920, 1920, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1920, bias=False)\n",
       "              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1920, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(80, 1920, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1920, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.19130434782608696, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (8): Conv2dNormActivation(\n",
       "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Identity()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=1280, out_features=1, bias=True)\n",
       "  (bn1): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 클래스 정의 (CustomModel)\n",
    "model = CustomModel(model_name='efficientnet').to(DEVICE)\n",
    "\n",
    "# 저장된 체크포인트 불러오기\n",
    "checkpoint = torch.load('../model/efficientnet_b1_best_model.2.pth')\n",
    "\n",
    "# 모델에 체크포인트 적용\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)  # Adam 옵티마이저\n",
    "\n",
    "# 모델을 평가 모드로 전환\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1065, Test Accuracy: 0.9681\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터세트로 평가\n",
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초기학습률 조정 (0.0001 -> 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 초기화\n",
    "model = CustomModel(model_name='efficientnet').to(DEVICE)\n",
    "# 손실 함수 및 최적화함수 정의\n",
    "criterion = nn.BCEWithLogitsLoss()  # 이진 교차 엔트로피 손실\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)  # Adam 옵티마이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 0.3985, Train Accuracy: 0.8183, Val Loss: 0.2461, Val Accuracy: 0.9104\n",
      "Validation loss improved, saving model...\n",
      "Epoch [2/20], Train Loss: 0.2229, Train Accuracy: 0.9070, Val Loss: 0.1847, Val Accuracy: 0.9343\n",
      "Validation loss improved, saving model...\n",
      "Epoch [3/20], Train Loss: 0.1197, Train Accuracy: 0.9576, Val Loss: 0.1361, Val Accuracy: 0.9522\n",
      "Validation loss improved, saving model...\n",
      "Epoch [4/20], Train Loss: 0.1007, Train Accuracy: 0.9622, Val Loss: 0.1137, Val Accuracy: 0.9681\n",
      "Validation loss improved, saving model...\n",
      "Epoch [5/20], Train Loss: 0.0927, Train Accuracy: 0.9660, Val Loss: 0.1188, Val Accuracy: 0.9622\n",
      "Epoch [6/20], Train Loss: 0.0559, Train Accuracy: 0.9796, Val Loss: 0.1127, Val Accuracy: 0.9701\n",
      "Validation loss improved, saving model...\n",
      "Epoch [7/20], Train Loss: 0.0435, Train Accuracy: 0.9868, Val Loss: 0.1299, Val Accuracy: 0.9602\n",
      "Epoch [8/20], Train Loss: 0.0385, Train Accuracy: 0.9856, Val Loss: 0.1250, Val Accuracy: 0.9701\n",
      "Epoch [9/20], Train Loss: 0.0252, Train Accuracy: 0.9936, Val Loss: 0.1230, Val Accuracy: 0.9741\n",
      "Epoch [10/20], Train Loss: 0.0635, Train Accuracy: 0.9792, Val Loss: 0.1135, Val Accuracy: 0.9641\n",
      "Epoch [11/20], Train Loss: 0.0311, Train Accuracy: 0.9885, Val Loss: 0.0846, Val Accuracy: 0.9761\n",
      "Validation loss improved, saving model...\n",
      "Epoch [12/20], Train Loss: 0.0217, Train Accuracy: 0.9941, Val Loss: 0.0789, Val Accuracy: 0.9801\n",
      "Validation loss improved, saving model...\n",
      "Epoch [13/20], Train Loss: 0.0256, Train Accuracy: 0.9907, Val Loss: 0.0863, Val Accuracy: 0.9821\n",
      "Epoch [14/20], Train Loss: 0.0142, Train Accuracy: 0.9966, Val Loss: 0.0760, Val Accuracy: 0.9801\n",
      "Validation loss improved, saving model...\n",
      "Epoch [15/20], Train Loss: 0.0152, Train Accuracy: 0.9953, Val Loss: 0.0757, Val Accuracy: 0.9801\n",
      "Validation loss improved, saving model...\n",
      "Epoch [16/20], Train Loss: 0.0116, Train Accuracy: 0.9962, Val Loss: 0.0870, Val Accuracy: 0.9821\n",
      "Epoch [17/20], Train Loss: 0.0071, Train Accuracy: 0.9979, Val Loss: 0.0919, Val Accuracy: 0.9781\n",
      "Epoch [18/20], Train Loss: 0.0114, Train Accuracy: 0.9970, Val Loss: 0.0995, Val Accuracy: 0.9821\n",
      "Epoch [19/20], Train Loss: 0.0114, Train Accuracy: 0.9970, Val Loss: 0.0862, Val Accuracy: 0.9841\n",
      "Epoch [20/20], Train Loss: 0.0114, Train Accuracy: 0.9970, Val Loss: 0.0801, Val Accuracy: 0.9841\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "# 조기 종료 변수를 초기화합니다.\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # 개선이 없을 때 기다릴 에포크 수\n",
    "patience_counter = 0\n",
    "\n",
    "# 학습률 스케줄러\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "# 모델 훈련\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(DEVICE), targets.to(DEVICE).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = criterion(outputs.view(-1), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 정확도 계산\n",
    "        predicted = (torch.sigmoid(outputs.view(-1)) > 0.5).float()\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "        total_predictions += targets.size(0)\n",
    "\n",
    "    # 훈련 데이터 정확도\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # 검증\n",
    "    val_loss, val_accuracy = evaluate_model(model, validation_loader, criterion)\n",
    "\n",
    "    # 학습률 스케줄러 적용\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {running_loss / len(train_loader):.4f}, '\n",
    "          f'Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # 조기 종료 로직\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # 손실 개선 시 카운터 리셋\n",
    "        print(\"Validation loss improved, saving model...\")\n",
    "        # 모델 저장\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, '../model/efficientnet_b1_best_model.3.pth')\n",
    "    else:\n",
    "        patience_counter += 1  # 손실이 개선되지 않으면 카운터 증가\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break  # 훈련 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\AppData\\Local\\Temp\\ipykernel_724\\2004558133.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('../model/efficientnet_b1_best_model.3.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (base_model): EfficientNet(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.10434782608695654, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.11304347826086956, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.12173913043478261, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1391304347826087, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.14782608695652175, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1565217391304348, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.16521739130434784, mode=row)\n",
       "        )\n",
       "        (4): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1920, 1920, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1920, bias=False)\n",
       "              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1920, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(80, 1920, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1920, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.19130434782608696, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (8): Conv2dNormActivation(\n",
       "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Identity()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=1280, out_features=1, bias=True)\n",
       "  (bn1): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 클래스 정의 (CustomModel)\n",
    "model = CustomModel(model_name='efficientnet').to(DEVICE)\n",
    "\n",
    "# 저장된 체크포인트 불러오기\n",
    "checkpoint = torch.load('../model/efficientnet_b1_best_model.3.pth')\n",
    "\n",
    "# 모델에 체크포인트 적용\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)  # Adam 옵티마이저\n",
    "\n",
    "# 모델을 평가 모드로 전환\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0757, Test Accuracy: 0.9801\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터세트로 평가\n",
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 데이터 수: 2008\n",
      "Validation 데이터 수: 502\n",
      "Test 데이터 수: 628\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "import pandas as pd\n",
    "\n",
    "IMAGE_SIZE = 240\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "# 이미지 변환 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),  # 이미지를 Tensor로 변환 (0~255 범위를 0~1 범위로 정규화)\n",
    "])\n",
    "\n",
    "# 데이터셋 디렉토리 설정\n",
    "train_dir = '../PCB_imgs/all/resize/train'\n",
    "val_dir = '../PCB_imgs/all/resize/validation'\n",
    "test_dir = '../PCB_imgs/all/resize/test'\n",
    "\n",
    "# ImageFolder로 데이터셋 불러오기\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "# 파일 경로 및 타겟 추출\n",
    "train_file_paths = [img[0] for img in train_dataset.imgs]\n",
    "train_targets = train_dataset.targets\n",
    "\n",
    "val_file_paths = [img[0] for img in val_dataset.imgs]\n",
    "val_targets = val_dataset.targets\n",
    "\n",
    "test_file_paths = [img[0] for img in test_dataset.imgs]\n",
    "test_targets = test_dataset.targets\n",
    "\n",
    "# DataFrame 생성\n",
    "train_df = pd.DataFrame({'file_paths': train_file_paths, 'targets': train_targets})\n",
    "validation_df = pd.DataFrame({'file_paths': val_file_paths, 'targets': val_targets})\n",
    "test_df = pd.DataFrame({'file_paths': test_file_paths, 'targets': test_targets})\n",
    "\n",
    "# 확인을 위해 각 데이터셋의 크기 출력\n",
    "print(f\"Train 데이터 수: {len(train_df)}\")\n",
    "print(f\"Validation 데이터 수: {len(validation_df)}\")\n",
    "print(f\"Test 데이터 수: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 240, 240])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# resize된 데이터셋 로드\n",
    "input_dir = '../PCB_imgs/all/resize/all/'\n",
    "dataset = datasets.ImageFolder(root=input_dir, transform=transform)\n",
    "\n",
    "# 데이터 로더 정의\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 데이터 로더에서 데이터 가져오기\n",
    "for images, labels in dataloader:\n",
    "    print(images.shape)  # 배치의 이미지 텐서 크기 확인\n",
    "    break  # 첫 번째 배치만 확인\n",
    "    # 배치 크기, 채널 수(RGB), 이미지 크기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 240\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "# 커스텀 데이터세트 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_paths, targets, aug=None, preprocess=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.targets = targets\n",
    "        self.aug = aug\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_path = self.file_paths[index]\n",
    "        target = self.targets[index]\n",
    "        \n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "\n",
    "        if self.aug is not None:\n",
    "            image = self.aug(image=image)['image']\n",
    "\n",
    "        if self.preprocess is not None:\n",
    "            image = self.preprocess(image)\n",
    "\n",
    "        image = np.transpose(image, (2, 0, 1))  # (H, W, C) -> (C, H, W)\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 데이터 증강 정의\n",
    "aug = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),                # 좌우 반전\n",
    "    A.VerticalFlip(p=0.5),                  # 상하 반전\n",
    "    A.Rotate(limit=10, p=0.5),              # 작은 각도 회전 (10도 내외)\n",
    "    A.RandomBrightnessContrast(p=0.5),      # 밝기 및 대비 조절\n",
    "])\n",
    "\n",
    "# 데이터셋 인스턴스 생성\n",
    "train_dataset = CustomDataset(train_df['file_paths'].values, train_df['targets'].values)\n",
    "validation_dataset = CustomDataset(validation_df['file_paths'].values, validation_df['targets'].values)\n",
    "test_dataset = CustomDataset(test_df['file_paths'].values, test_df['targets'].values)\n",
    "\n",
    "# DataLoader 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model_name='efficientnet'):\n",
    "        super(CustomModel, self).__init__()\n",
    "        if model_name == 'efficientnet':\n",
    "            self.base_model = torchvision.models.efficientnet_b1(weights='IMAGENET1K_V1')\n",
    "            self.base_model.classifier = nn.Identity()  # 마지막 분류기 제거\n",
    "        elif model_name == 'resnet50':\n",
    "            self.base_model = torchvision.models.resnet50(weights='IMAGENET1K_V1')\n",
    "            self.base_model = nn.Sequential(*list(self.base_model.children())[:-1])  # 마지막 레이어 제거\n",
    "        elif model_name == 'inception':\n",
    "            self.base_model = torchvision.models.inception_v3(weights='IMAGENET1K_V1')\n",
    "            self.base_model.classifier = nn.Identity()  # 마지막 분류기 제거\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(self._get_features_dim(model_name), 1)\n",
    "        self.bn1 = nn.BatchNorm1d(self._get_features_dim(model_name))\n",
    "\n",
    "    def _get_features_dim(self, model_name):\n",
    "        if model_name == 'efficientnet':\n",
    "            return 1280  # EfficientNet의 출력 차원\n",
    "        elif model_name == 'resnet50':\n",
    "            return 2048  # ResNet50의 출력 차원\n",
    "        elif model_name == 'inception':\n",
    "            return 2048  # Inception의 출력 차원\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        if isinstance(x, tuple):  \n",
    "            x = x[0] \n",
    "        \n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x) \n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: base_model.features.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.2.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.2.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.2.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.2.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.2.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.2.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.8.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.8.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.8.1.bias | Requires Grad: False\n",
      "Layer: fc1.weight | Requires Grad: True\n",
      "Layer: fc1.bias | Requires Grad: True\n",
      "Layer: bn1.weight | Requires Grad: True\n",
      "Layer: bn1.bias | Requires Grad: True\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# 모델 생성\n",
    "model = CustomModel(model_name='efficientnet').to(DEVICE)\n",
    "\n",
    "# 사전 학습된 base_model의 파라미터를 동결 (fine-tuning 초기 단계)\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 새로 추가된 분류기 레이어만 학습\n",
    "optimizer = optim.Adam(model.fc1.parameters(), lr=1e-5) \n",
    "\n",
    "# 학습할 손실 함수\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 각 층의 freeze/unfreeze 상태 확인\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Requires Grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved, saving model...\n",
      "Epoch [1/50], Train Loss: 0.7507, Train Accuracy: 0.5199, Val Loss: 0.6949, Val Accuracy: 0.5299\n",
      "Validation loss improved, saving model...\n",
      "Epoch [2/50], Train Loss: 0.7193, Train Accuracy: 0.5528, Val Loss: 0.6871, Val Accuracy: 0.5478\n",
      "Validation loss improved, saving model...\n",
      "Epoch [3/50], Train Loss: 0.7058, Train Accuracy: 0.5583, Val Loss: 0.6666, Val Accuracy: 0.5976\n",
      "Validation loss improved, saving model...\n",
      "Epoch [4/50], Train Loss: 0.6942, Train Accuracy: 0.5872, Val Loss: 0.6539, Val Accuracy: 0.6215\n",
      "Validation loss improved, saving model...\n",
      "Epoch [5/50], Train Loss: 0.6742, Train Accuracy: 0.6026, Val Loss: 0.6383, Val Accuracy: 0.6434\n",
      "Validation loss improved, saving model...\n",
      "Epoch [6/50], Train Loss: 0.6674, Train Accuracy: 0.6096, Val Loss: 0.6231, Val Accuracy: 0.6514\n",
      "Validation loss improved, saving model...\n",
      "Epoch [7/50], Train Loss: 0.6504, Train Accuracy: 0.6389, Val Loss: 0.6180, Val Accuracy: 0.6793\n",
      "Validation loss improved, saving model...\n",
      "Epoch [8/50], Train Loss: 0.6398, Train Accuracy: 0.6419, Val Loss: 0.6082, Val Accuracy: 0.6653\n",
      "Validation loss improved, saving model...\n",
      "Epoch [9/50], Train Loss: 0.6189, Train Accuracy: 0.6529, Val Loss: 0.5979, Val Accuracy: 0.6952\n",
      "Validation loss improved, saving model...\n",
      "Epoch [10/50], Train Loss: 0.6322, Train Accuracy: 0.6494, Val Loss: 0.5878, Val Accuracy: 0.7012\n",
      "Validation loss improved, saving model...\n",
      "Epoch [11/50], Train Loss: 0.6032, Train Accuracy: 0.6788, Val Loss: 0.5734, Val Accuracy: 0.7131\n",
      "Validation loss improved, saving model...\n",
      "Epoch [12/50], Train Loss: 0.5981, Train Accuracy: 0.6833, Val Loss: 0.5707, Val Accuracy: 0.7251\n",
      "Validation loss improved, saving model...\n",
      "Epoch [13/50], Train Loss: 0.5962, Train Accuracy: 0.6907, Val Loss: 0.5557, Val Accuracy: 0.7351\n",
      "Epoch [14/50], Train Loss: 0.5912, Train Accuracy: 0.6932, Val Loss: 0.5587, Val Accuracy: 0.7311\n",
      "Validation loss improved, saving model...\n",
      "Epoch [15/50], Train Loss: 0.5865, Train Accuracy: 0.6922, Val Loss: 0.5486, Val Accuracy: 0.7450\n",
      "Validation loss improved, saving model...\n",
      "Epoch [16/50], Train Loss: 0.5751, Train Accuracy: 0.6947, Val Loss: 0.5469, Val Accuracy: 0.7331\n",
      "Validation loss improved, saving model...\n",
      "Epoch [17/50], Train Loss: 0.5581, Train Accuracy: 0.7097, Val Loss: 0.5374, Val Accuracy: 0.7450\n",
      "Validation loss improved, saving model...\n",
      "Epoch [18/50], Train Loss: 0.5661, Train Accuracy: 0.7146, Val Loss: 0.5350, Val Accuracy: 0.7410\n",
      "Validation loss improved, saving model...\n",
      "Epoch [19/50], Train Loss: 0.5509, Train Accuracy: 0.7231, Val Loss: 0.5259, Val Accuracy: 0.7510\n",
      "Validation loss improved, saving model...\n",
      "Epoch [20/50], Train Loss: 0.5544, Train Accuracy: 0.7062, Val Loss: 0.5249, Val Accuracy: 0.7450\n",
      "Validation loss improved, saving model...\n",
      "Epoch [21/50], Train Loss: 0.5430, Train Accuracy: 0.7266, Val Loss: 0.5162, Val Accuracy: 0.7470\n",
      "Validation loss improved, saving model...\n",
      "Epoch [22/50], Train Loss: 0.5482, Train Accuracy: 0.7206, Val Loss: 0.5119, Val Accuracy: 0.7570\n",
      "Epoch [23/50], Train Loss: 0.5378, Train Accuracy: 0.7341, Val Loss: 0.5140, Val Accuracy: 0.7430\n",
      "Validation loss improved, saving model...\n",
      "Epoch [24/50], Train Loss: 0.5299, Train Accuracy: 0.7390, Val Loss: 0.5033, Val Accuracy: 0.7570\n",
      "Validation loss improved, saving model...\n",
      "Epoch [25/50], Train Loss: 0.5361, Train Accuracy: 0.7435, Val Loss: 0.4986, Val Accuracy: 0.7629\n",
      "Epoch [26/50], Train Loss: 0.5241, Train Accuracy: 0.7470, Val Loss: 0.4995, Val Accuracy: 0.7629\n",
      "Validation loss improved, saving model...\n",
      "Epoch [27/50], Train Loss: 0.5255, Train Accuracy: 0.7425, Val Loss: 0.4937, Val Accuracy: 0.7590\n",
      "Validation loss improved, saving model...\n",
      "Epoch [28/50], Train Loss: 0.5125, Train Accuracy: 0.7535, Val Loss: 0.4901, Val Accuracy: 0.7649\n",
      "Validation loss improved, saving model...\n",
      "Epoch [29/50], Train Loss: 0.5292, Train Accuracy: 0.7425, Val Loss: 0.4867, Val Accuracy: 0.7709\n",
      "Epoch [30/50], Train Loss: 0.5224, Train Accuracy: 0.7400, Val Loss: 0.4924, Val Accuracy: 0.7669\n",
      "Validation loss improved, saving model...\n",
      "Epoch [31/50], Train Loss: 0.5066, Train Accuracy: 0.7415, Val Loss: 0.4850, Val Accuracy: 0.7649\n",
      "Validation loss improved, saving model...\n",
      "Epoch [32/50], Train Loss: 0.5015, Train Accuracy: 0.7600, Val Loss: 0.4797, Val Accuracy: 0.7729\n",
      "Validation loss improved, saving model...\n",
      "Epoch [33/50], Train Loss: 0.5115, Train Accuracy: 0.7475, Val Loss: 0.4761, Val Accuracy: 0.7669\n",
      "Validation loss improved, saving model...\n",
      "Epoch [34/50], Train Loss: 0.5099, Train Accuracy: 0.7505, Val Loss: 0.4760, Val Accuracy: 0.7729\n",
      "Validation loss improved, saving model...\n",
      "Epoch [35/50], Train Loss: 0.4935, Train Accuracy: 0.7669, Val Loss: 0.4714, Val Accuracy: 0.7749\n",
      "Validation loss improved, saving model...\n",
      "Epoch [36/50], Train Loss: 0.5076, Train Accuracy: 0.7485, Val Loss: 0.4710, Val Accuracy: 0.7749\n",
      "Validation loss improved, saving model...\n",
      "Epoch [37/50], Train Loss: 0.4798, Train Accuracy: 0.7724, Val Loss: 0.4659, Val Accuracy: 0.7789\n",
      "Epoch [38/50], Train Loss: 0.4948, Train Accuracy: 0.7605, Val Loss: 0.4679, Val Accuracy: 0.7729\n",
      "Validation loss improved, saving model...\n",
      "Epoch [39/50], Train Loss: 0.5049, Train Accuracy: 0.7530, Val Loss: 0.4639, Val Accuracy: 0.7789\n",
      "Validation loss improved, saving model...\n",
      "Epoch [40/50], Train Loss: 0.5014, Train Accuracy: 0.7590, Val Loss: 0.4629, Val Accuracy: 0.7849\n",
      "Epoch [41/50], Train Loss: 0.4756, Train Accuracy: 0.7809, Val Loss: 0.4682, Val Accuracy: 0.7729\n",
      "Validation loss improved, saving model...\n",
      "Epoch [42/50], Train Loss: 0.4914, Train Accuracy: 0.7475, Val Loss: 0.4607, Val Accuracy: 0.7809\n",
      "Epoch [43/50], Train Loss: 0.4762, Train Accuracy: 0.7754, Val Loss: 0.4611, Val Accuracy: 0.7729\n",
      "Validation loss improved, saving model...\n",
      "Epoch [44/50], Train Loss: 0.4807, Train Accuracy: 0.7620, Val Loss: 0.4591, Val Accuracy: 0.7769\n",
      "Validation loss improved, saving model...\n",
      "Epoch [45/50], Train Loss: 0.4844, Train Accuracy: 0.7560, Val Loss: 0.4548, Val Accuracy: 0.7829\n",
      "Validation loss improved, saving model...\n",
      "Epoch [46/50], Train Loss: 0.4813, Train Accuracy: 0.7769, Val Loss: 0.4463, Val Accuracy: 0.7908\n",
      "Epoch [47/50], Train Loss: 0.4714, Train Accuracy: 0.7724, Val Loss: 0.4556, Val Accuracy: 0.7809\n",
      "Epoch [48/50], Train Loss: 0.4661, Train Accuracy: 0.7689, Val Loss: 0.4480, Val Accuracy: 0.7908\n",
      "Epoch [49/50], Train Loss: 0.4661, Train Accuracy: 0.7829, Val Loss: 0.4481, Val Accuracy: 0.7928\n",
      "Validation loss improved, saving model...\n",
      "Epoch [50/50], Train Loss: 0.4653, Train Accuracy: 0.7774, Val Loss: 0.4454, Val Accuracy: 0.7948\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "# 조기 종료 변수 초기화\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # 개선이 없을 때 기다릴 에포크 수\n",
    "patience_counter = 0\n",
    "\n",
    "# 학습률 스케줄러\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(DEVICE), targets.to(DEVICE).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = criterion(outputs.view(-1), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 정확도 계산\n",
    "        predicted = (torch.sigmoid(outputs.view(-1)) > 0.5).float()\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "        total_predictions += targets.size(0)\n",
    "\n",
    "    # 훈련 데이터 정확도\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # 검증\n",
    "    val_loss, val_accuracy = evaluate_model(model, validation_loader, criterion)\n",
    "\n",
    "    # 학습률 스케줄러 적용\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # 조기 종료 로직\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # 손실 개선 시 카운터 리셋\n",
    "        print(\"Validation loss improved, saving model...\")\n",
    "        # 모델 저장\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, '../model/efficientnet_b1_ft_best_model_01.pth')\n",
    "    else:\n",
    "        patience_counter += 1  # 손실이 개선되지 않으면 카운터 증가\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break  # 훈련 종료\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {running_loss / len(train_loader):.4f}, '\n",
    "          f'Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\AppData\\Local\\Temp\\ipykernel_724\\684982325.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('../model/efficientnet_b1_ft_best_model_01.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (base_model): EfficientNet(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.10434782608695654, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.11304347826086956, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.12173913043478261, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1391304347826087, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.14782608695652175, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1565217391304348, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.16521739130434784, mode=row)\n",
       "        )\n",
       "        (4): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1920, 1920, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1920, bias=False)\n",
       "              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1920, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(80, 1920, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1920, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.19130434782608696, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (8): Conv2dNormActivation(\n",
       "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Identity()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=1280, out_features=1, bias=True)\n",
       "  (bn1): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 클래스 정의 (CustomModel)\n",
    "model = CustomModel(model_name='efficientnet').to(DEVICE)\n",
    "\n",
    "# 저장된 체크포인트 불러오기\n",
    "checkpoint = torch.load('../model/efficientnet_b1_ft_best_model_01.pth')\n",
    "\n",
    "# 모델에 체크포인트 적용\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 새로 추가된 분류기 레이어만 학습\n",
    "optimizer = optim.Adam(model.fc1.parameters(), lr=1e-5) \n",
    "\n",
    "# 에포크 정보 가져오기 (옵션)\n",
    "start_epoch = checkpoint['epoch']\n",
    "\n",
    "# 모델을 평가 모드로 전환\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4527, Test Accuracy: 0.7755\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터세트로 평가\n",
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 마지막 layer unfreeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name: features, Layer Type: Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): SiLU(inplace=True)\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "    )\n",
      "    (1): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n",
      "    )\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    (0): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n",
      "    )\n",
      "    (1): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n",
      "    )\n",
      "    (2): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n",
      "    )\n",
      "  )\n",
      "  (3): Sequential(\n",
      "    (0): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n",
      "    )\n",
      "    (1): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n",
      "    )\n",
      "    (2): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n",
      "    )\n",
      "  )\n",
      "  (4): Sequential(\n",
      "    (0): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n",
      "    )\n",
      "    (1): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n",
      "    )\n",
      "    (2): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n",
      "    )\n",
      "    (3): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n",
      "    )\n",
      "  )\n",
      "  (5): Sequential(\n",
      "    (0): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
      "          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.10434782608695654, mode=row)\n",
      "    )\n",
      "    (1): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.11304347826086956, mode=row)\n",
      "    )\n",
      "    (2): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.12173913043478261, mode=row)\n",
      "    )\n",
      "    (3): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n",
      "    )\n",
      "  )\n",
      "  (6): Sequential(\n",
      "    (0): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.1391304347826087, mode=row)\n",
      "    )\n",
      "    (1): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.14782608695652175, mode=row)\n",
      "    )\n",
      "    (2): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.1565217391304348, mode=row)\n",
      "    )\n",
      "    (3): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.16521739130434784, mode=row)\n",
      "    )\n",
      "    (4): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n",
      "    )\n",
      "  )\n",
      "  (7): Sequential(\n",
      "    (0): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n",
      "    )\n",
      "    (1): MBConv(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(1920, 1920, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1920, bias=False)\n",
      "          (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): SiLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(1920, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(80, 1920, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): SiLU(inplace=True)\n",
      "          (scale_activation): Sigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(1920, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.19130434782608696, mode=row)\n",
      "    )\n",
      "  )\n",
      "  (8): Conv2dNormActivation(\n",
      "    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): SiLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "Layer Name: avgpool, Layer Type: AdaptiveAvgPool2d(output_size=1)\n",
      "Layer Name: classifier, Layer Type: Sequential(\n",
      "  (0): Dropout(p=0.2, inplace=True)\n",
      "  (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# 사전 학습된 모델 로드\n",
    "model = models.efficientnet_b1(weights='IMAGENET1K_V1')\n",
    "\n",
    "# 모델의 레이어 이름과 타입 확인\n",
    "for name, layer in model.named_children():\n",
    "    print(f\"Layer Name: {name}, Layer Type: {layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 데이터 증강 정의\n",
    "aug = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),                # 좌우 반전\n",
    "    A.VerticalFlip(p=0.5),                  # 상하 반전\n",
    "    A.Rotate(limit=10, p=0.5),              # 작은 각도 회전 (10도 내외)\n",
    "    A.RandomBrightnessContrast(p=0.5),      # 밝기 및 대비 조절\n",
    "])\n",
    "\n",
    "# 데이터셋 인스턴스 생성\n",
    "train_dataset = CustomDataset(train_df['file_paths'].values, train_df['targets'].values, aug=aug)\n",
    "validation_dataset = CustomDataset(validation_df['file_paths'].values, validation_df['targets'].values)\n",
    "test_dataset = CustomDataset(test_df['file_paths'].values, test_df['targets'].values)\n",
    "\n",
    "# DataLoader 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.0.weight\n",
      "features.0.1.weight\n",
      "features.0.1.bias\n",
      "features.1.0.block.0.0.weight\n",
      "features.1.0.block.0.1.weight\n",
      "features.1.0.block.0.1.bias\n",
      "features.1.0.block.1.fc1.weight\n",
      "features.1.0.block.1.fc1.bias\n",
      "features.1.0.block.1.fc2.weight\n",
      "features.1.0.block.1.fc2.bias\n",
      "features.1.0.block.2.0.weight\n",
      "features.1.0.block.2.1.weight\n",
      "features.1.0.block.2.1.bias\n",
      "features.1.1.block.0.0.weight\n",
      "features.1.1.block.0.1.weight\n",
      "features.1.1.block.0.1.bias\n",
      "features.1.1.block.1.fc1.weight\n",
      "features.1.1.block.1.fc1.bias\n",
      "features.1.1.block.1.fc2.weight\n",
      "features.1.1.block.1.fc2.bias\n",
      "features.1.1.block.2.0.weight\n",
      "features.1.1.block.2.1.weight\n",
      "features.1.1.block.2.1.bias\n",
      "features.2.0.block.0.0.weight\n",
      "features.2.0.block.0.1.weight\n",
      "features.2.0.block.0.1.bias\n",
      "features.2.0.block.1.0.weight\n",
      "features.2.0.block.1.1.weight\n",
      "features.2.0.block.1.1.bias\n",
      "features.2.0.block.2.fc1.weight\n",
      "features.2.0.block.2.fc1.bias\n",
      "features.2.0.block.2.fc2.weight\n",
      "features.2.0.block.2.fc2.bias\n",
      "features.2.0.block.3.0.weight\n",
      "features.2.0.block.3.1.weight\n",
      "features.2.0.block.3.1.bias\n",
      "features.2.1.block.0.0.weight\n",
      "features.2.1.block.0.1.weight\n",
      "features.2.1.block.0.1.bias\n",
      "features.2.1.block.1.0.weight\n",
      "features.2.1.block.1.1.weight\n",
      "features.2.1.block.1.1.bias\n",
      "features.2.1.block.2.fc1.weight\n",
      "features.2.1.block.2.fc1.bias\n",
      "features.2.1.block.2.fc2.weight\n",
      "features.2.1.block.2.fc2.bias\n",
      "features.2.1.block.3.0.weight\n",
      "features.2.1.block.3.1.weight\n",
      "features.2.1.block.3.1.bias\n",
      "features.2.2.block.0.0.weight\n",
      "features.2.2.block.0.1.weight\n",
      "features.2.2.block.0.1.bias\n",
      "features.2.2.block.1.0.weight\n",
      "features.2.2.block.1.1.weight\n",
      "features.2.2.block.1.1.bias\n",
      "features.2.2.block.2.fc1.weight\n",
      "features.2.2.block.2.fc1.bias\n",
      "features.2.2.block.2.fc2.weight\n",
      "features.2.2.block.2.fc2.bias\n",
      "features.2.2.block.3.0.weight\n",
      "features.2.2.block.3.1.weight\n",
      "features.2.2.block.3.1.bias\n",
      "features.3.0.block.0.0.weight\n",
      "features.3.0.block.0.1.weight\n",
      "features.3.0.block.0.1.bias\n",
      "features.3.0.block.1.0.weight\n",
      "features.3.0.block.1.1.weight\n",
      "features.3.0.block.1.1.bias\n",
      "features.3.0.block.2.fc1.weight\n",
      "features.3.0.block.2.fc1.bias\n",
      "features.3.0.block.2.fc2.weight\n",
      "features.3.0.block.2.fc2.bias\n",
      "features.3.0.block.3.0.weight\n",
      "features.3.0.block.3.1.weight\n",
      "features.3.0.block.3.1.bias\n",
      "features.3.1.block.0.0.weight\n",
      "features.3.1.block.0.1.weight\n",
      "features.3.1.block.0.1.bias\n",
      "features.3.1.block.1.0.weight\n",
      "features.3.1.block.1.1.weight\n",
      "features.3.1.block.1.1.bias\n",
      "features.3.1.block.2.fc1.weight\n",
      "features.3.1.block.2.fc1.bias\n",
      "features.3.1.block.2.fc2.weight\n",
      "features.3.1.block.2.fc2.bias\n",
      "features.3.1.block.3.0.weight\n",
      "features.3.1.block.3.1.weight\n",
      "features.3.1.block.3.1.bias\n",
      "features.3.2.block.0.0.weight\n",
      "features.3.2.block.0.1.weight\n",
      "features.3.2.block.0.1.bias\n",
      "features.3.2.block.1.0.weight\n",
      "features.3.2.block.1.1.weight\n",
      "features.3.2.block.1.1.bias\n",
      "features.3.2.block.2.fc1.weight\n",
      "features.3.2.block.2.fc1.bias\n",
      "features.3.2.block.2.fc2.weight\n",
      "features.3.2.block.2.fc2.bias\n",
      "features.3.2.block.3.0.weight\n",
      "features.3.2.block.3.1.weight\n",
      "features.3.2.block.3.1.bias\n",
      "features.4.0.block.0.0.weight\n",
      "features.4.0.block.0.1.weight\n",
      "features.4.0.block.0.1.bias\n",
      "features.4.0.block.1.0.weight\n",
      "features.4.0.block.1.1.weight\n",
      "features.4.0.block.1.1.bias\n",
      "features.4.0.block.2.fc1.weight\n",
      "features.4.0.block.2.fc1.bias\n",
      "features.4.0.block.2.fc2.weight\n",
      "features.4.0.block.2.fc2.bias\n",
      "features.4.0.block.3.0.weight\n",
      "features.4.0.block.3.1.weight\n",
      "features.4.0.block.3.1.bias\n",
      "features.4.1.block.0.0.weight\n",
      "features.4.1.block.0.1.weight\n",
      "features.4.1.block.0.1.bias\n",
      "features.4.1.block.1.0.weight\n",
      "features.4.1.block.1.1.weight\n",
      "features.4.1.block.1.1.bias\n",
      "features.4.1.block.2.fc1.weight\n",
      "features.4.1.block.2.fc1.bias\n",
      "features.4.1.block.2.fc2.weight\n",
      "features.4.1.block.2.fc2.bias\n",
      "features.4.1.block.3.0.weight\n",
      "features.4.1.block.3.1.weight\n",
      "features.4.1.block.3.1.bias\n",
      "features.4.2.block.0.0.weight\n",
      "features.4.2.block.0.1.weight\n",
      "features.4.2.block.0.1.bias\n",
      "features.4.2.block.1.0.weight\n",
      "features.4.2.block.1.1.weight\n",
      "features.4.2.block.1.1.bias\n",
      "features.4.2.block.2.fc1.weight\n",
      "features.4.2.block.2.fc1.bias\n",
      "features.4.2.block.2.fc2.weight\n",
      "features.4.2.block.2.fc2.bias\n",
      "features.4.2.block.3.0.weight\n",
      "features.4.2.block.3.1.weight\n",
      "features.4.2.block.3.1.bias\n",
      "features.4.3.block.0.0.weight\n",
      "features.4.3.block.0.1.weight\n",
      "features.4.3.block.0.1.bias\n",
      "features.4.3.block.1.0.weight\n",
      "features.4.3.block.1.1.weight\n",
      "features.4.3.block.1.1.bias\n",
      "features.4.3.block.2.fc1.weight\n",
      "features.4.3.block.2.fc1.bias\n",
      "features.4.3.block.2.fc2.weight\n",
      "features.4.3.block.2.fc2.bias\n",
      "features.4.3.block.3.0.weight\n",
      "features.4.3.block.3.1.weight\n",
      "features.4.3.block.3.1.bias\n",
      "features.5.0.block.0.0.weight\n",
      "features.5.0.block.0.1.weight\n",
      "features.5.0.block.0.1.bias\n",
      "features.5.0.block.1.0.weight\n",
      "features.5.0.block.1.1.weight\n",
      "features.5.0.block.1.1.bias\n",
      "features.5.0.block.2.fc1.weight\n",
      "features.5.0.block.2.fc1.bias\n",
      "features.5.0.block.2.fc2.weight\n",
      "features.5.0.block.2.fc2.bias\n",
      "features.5.0.block.3.0.weight\n",
      "features.5.0.block.3.1.weight\n",
      "features.5.0.block.3.1.bias\n",
      "features.5.1.block.0.0.weight\n",
      "features.5.1.block.0.1.weight\n",
      "features.5.1.block.0.1.bias\n",
      "features.5.1.block.1.0.weight\n",
      "features.5.1.block.1.1.weight\n",
      "features.5.1.block.1.1.bias\n",
      "features.5.1.block.2.fc1.weight\n",
      "features.5.1.block.2.fc1.bias\n",
      "features.5.1.block.2.fc2.weight\n",
      "features.5.1.block.2.fc2.bias\n",
      "features.5.1.block.3.0.weight\n",
      "features.5.1.block.3.1.weight\n",
      "features.5.1.block.3.1.bias\n",
      "features.5.2.block.0.0.weight\n",
      "features.5.2.block.0.1.weight\n",
      "features.5.2.block.0.1.bias\n",
      "features.5.2.block.1.0.weight\n",
      "features.5.2.block.1.1.weight\n",
      "features.5.2.block.1.1.bias\n",
      "features.5.2.block.2.fc1.weight\n",
      "features.5.2.block.2.fc1.bias\n",
      "features.5.2.block.2.fc2.weight\n",
      "features.5.2.block.2.fc2.bias\n",
      "features.5.2.block.3.0.weight\n",
      "features.5.2.block.3.1.weight\n",
      "features.5.2.block.3.1.bias\n",
      "features.5.3.block.0.0.weight\n",
      "features.5.3.block.0.1.weight\n",
      "features.5.3.block.0.1.bias\n",
      "features.5.3.block.1.0.weight\n",
      "features.5.3.block.1.1.weight\n",
      "features.5.3.block.1.1.bias\n",
      "features.5.3.block.2.fc1.weight\n",
      "features.5.3.block.2.fc1.bias\n",
      "features.5.3.block.2.fc2.weight\n",
      "features.5.3.block.2.fc2.bias\n",
      "features.5.3.block.3.0.weight\n",
      "features.5.3.block.3.1.weight\n",
      "features.5.3.block.3.1.bias\n",
      "features.6.0.block.0.0.weight\n",
      "features.6.0.block.0.1.weight\n",
      "features.6.0.block.0.1.bias\n",
      "features.6.0.block.1.0.weight\n",
      "features.6.0.block.1.1.weight\n",
      "features.6.0.block.1.1.bias\n",
      "features.6.0.block.2.fc1.weight\n",
      "features.6.0.block.2.fc1.bias\n",
      "features.6.0.block.2.fc2.weight\n",
      "features.6.0.block.2.fc2.bias\n",
      "features.6.0.block.3.0.weight\n",
      "features.6.0.block.3.1.weight\n",
      "features.6.0.block.3.1.bias\n",
      "features.6.1.block.0.0.weight\n",
      "features.6.1.block.0.1.weight\n",
      "features.6.1.block.0.1.bias\n",
      "features.6.1.block.1.0.weight\n",
      "features.6.1.block.1.1.weight\n",
      "features.6.1.block.1.1.bias\n",
      "features.6.1.block.2.fc1.weight\n",
      "features.6.1.block.2.fc1.bias\n",
      "features.6.1.block.2.fc2.weight\n",
      "features.6.1.block.2.fc2.bias\n",
      "features.6.1.block.3.0.weight\n",
      "features.6.1.block.3.1.weight\n",
      "features.6.1.block.3.1.bias\n",
      "features.6.2.block.0.0.weight\n",
      "features.6.2.block.0.1.weight\n",
      "features.6.2.block.0.1.bias\n",
      "features.6.2.block.1.0.weight\n",
      "features.6.2.block.1.1.weight\n",
      "features.6.2.block.1.1.bias\n",
      "features.6.2.block.2.fc1.weight\n",
      "features.6.2.block.2.fc1.bias\n",
      "features.6.2.block.2.fc2.weight\n",
      "features.6.2.block.2.fc2.bias\n",
      "features.6.2.block.3.0.weight\n",
      "features.6.2.block.3.1.weight\n",
      "features.6.2.block.3.1.bias\n",
      "features.6.3.block.0.0.weight\n",
      "features.6.3.block.0.1.weight\n",
      "features.6.3.block.0.1.bias\n",
      "features.6.3.block.1.0.weight\n",
      "features.6.3.block.1.1.weight\n",
      "features.6.3.block.1.1.bias\n",
      "features.6.3.block.2.fc1.weight\n",
      "features.6.3.block.2.fc1.bias\n",
      "features.6.3.block.2.fc2.weight\n",
      "features.6.3.block.2.fc2.bias\n",
      "features.6.3.block.3.0.weight\n",
      "features.6.3.block.3.1.weight\n",
      "features.6.3.block.3.1.bias\n",
      "features.6.4.block.0.0.weight\n",
      "features.6.4.block.0.1.weight\n",
      "features.6.4.block.0.1.bias\n",
      "features.6.4.block.1.0.weight\n",
      "features.6.4.block.1.1.weight\n",
      "features.6.4.block.1.1.bias\n",
      "features.6.4.block.2.fc1.weight\n",
      "features.6.4.block.2.fc1.bias\n",
      "features.6.4.block.2.fc2.weight\n",
      "features.6.4.block.2.fc2.bias\n",
      "features.6.4.block.3.0.weight\n",
      "features.6.4.block.3.1.weight\n",
      "features.6.4.block.3.1.bias\n",
      "features.7.0.block.0.0.weight\n",
      "features.7.0.block.0.1.weight\n",
      "features.7.0.block.0.1.bias\n",
      "features.7.0.block.1.0.weight\n",
      "features.7.0.block.1.1.weight\n",
      "features.7.0.block.1.1.bias\n",
      "features.7.0.block.2.fc1.weight\n",
      "features.7.0.block.2.fc1.bias\n",
      "features.7.0.block.2.fc2.weight\n",
      "features.7.0.block.2.fc2.bias\n",
      "features.7.0.block.3.0.weight\n",
      "features.7.0.block.3.1.weight\n",
      "features.7.0.block.3.1.bias\n",
      "features.7.1.block.0.0.weight\n",
      "features.7.1.block.0.1.weight\n",
      "features.7.1.block.0.1.bias\n",
      "features.7.1.block.1.0.weight\n",
      "features.7.1.block.1.1.weight\n",
      "features.7.1.block.1.1.bias\n",
      "features.7.1.block.2.fc1.weight\n",
      "features.7.1.block.2.fc1.bias\n",
      "features.7.1.block.2.fc2.weight\n",
      "features.7.1.block.2.fc2.bias\n",
      "features.7.1.block.3.0.weight\n",
      "features.7.1.block.3.1.weight\n",
      "features.7.1.block.3.1.bias\n",
      "features.8.0.weight\n",
      "features.8.1.weight\n",
      "features.8.1.bias\n",
      "classifier.1.weight\n",
      "classifier.1.bias\n"
     ]
    }
   ],
   "source": [
    "# 모든 레이어 이름 출력\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: base_model.features.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.2.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.2.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.2.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.2.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.2.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.2.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.8.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.8.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.8.1.bias | Requires Grad: True\n",
      "Layer: fc1.weight | Requires Grad: True\n",
      "Layer: fc1.bias | Requires Grad: True\n",
      "Layer: bn1.weight | Requires Grad: True\n",
      "Layer: bn1.bias | Requires Grad: True\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "model = CustomModel(model_name='efficientnet').to(DEVICE)\n",
    "\n",
    "# 1. 사전 학습된 base_model의 파라미터를 동결 (fine-tuning 초기 단계)\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 마지막 몇 개의 레이어를 동결 해제 (features 블록 8 이후 층)\n",
    "for name, param in model.base_model.features[8:].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 동결 해제된 일부 층과 분류기 층 학습\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
    "\n",
    "# 학습할 손실 함수\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 각 층의 freeze/unfreeze 상태 확인\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Requires Grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved, saving model...\n",
      "Epoch [1/50], Train Loss: 0.7608, Train Accuracy: 0.4970, Val Loss: 0.6991, Val Accuracy: 0.5159\n",
      "Validation loss improved, saving model...\n",
      "Epoch [2/50], Train Loss: 0.7167, Train Accuracy: 0.5513, Val Loss: 0.6582, Val Accuracy: 0.6036\n",
      "Validation loss improved, saving model...\n",
      "Epoch [3/50], Train Loss: 0.6661, Train Accuracy: 0.6071, Val Loss: 0.6198, Val Accuracy: 0.6793\n",
      "Validation loss improved, saving model...\n",
      "Epoch [4/50], Train Loss: 0.6416, Train Accuracy: 0.6355, Val Loss: 0.5931, Val Accuracy: 0.7112\n",
      "Validation loss improved, saving model...\n",
      "Epoch [5/50], Train Loss: 0.6201, Train Accuracy: 0.6509, Val Loss: 0.5707, Val Accuracy: 0.7131\n",
      "Validation loss improved, saving model...\n",
      "Epoch [6/50], Train Loss: 0.5817, Train Accuracy: 0.6833, Val Loss: 0.5576, Val Accuracy: 0.7291\n",
      "Validation loss improved, saving model...\n",
      "Epoch [7/50], Train Loss: 0.5756, Train Accuracy: 0.6858, Val Loss: 0.5339, Val Accuracy: 0.7450\n",
      "Validation loss improved, saving model...\n",
      "Epoch [8/50], Train Loss: 0.5509, Train Accuracy: 0.7281, Val Loss: 0.5218, Val Accuracy: 0.7450\n",
      "Validation loss improved, saving model...\n",
      "Epoch [9/50], Train Loss: 0.5391, Train Accuracy: 0.7266, Val Loss: 0.5086, Val Accuracy: 0.7530\n",
      "Validation loss improved, saving model...\n",
      "Epoch [10/50], Train Loss: 0.5270, Train Accuracy: 0.7336, Val Loss: 0.4974, Val Accuracy: 0.7629\n",
      "Validation loss improved, saving model...\n",
      "Epoch [11/50], Train Loss: 0.5159, Train Accuracy: 0.7450, Val Loss: 0.4837, Val Accuracy: 0.7749\n",
      "Validation loss improved, saving model...\n",
      "Epoch [12/50], Train Loss: 0.5056, Train Accuracy: 0.7505, Val Loss: 0.4754, Val Accuracy: 0.7908\n",
      "Epoch [13/50], Train Loss: 0.5045, Train Accuracy: 0.7570, Val Loss: 0.4764, Val Accuracy: 0.7789\n",
      "Validation loss improved, saving model...\n",
      "Epoch [14/50], Train Loss: 0.5007, Train Accuracy: 0.7639, Val Loss: 0.4621, Val Accuracy: 0.8068\n",
      "Validation loss improved, saving model...\n",
      "Epoch [15/50], Train Loss: 0.4937, Train Accuracy: 0.7639, Val Loss: 0.4590, Val Accuracy: 0.8008\n",
      "Validation loss improved, saving model...\n",
      "Epoch [16/50], Train Loss: 0.4726, Train Accuracy: 0.7779, Val Loss: 0.4499, Val Accuracy: 0.8008\n",
      "Validation loss improved, saving model...\n",
      "Epoch [17/50], Train Loss: 0.4867, Train Accuracy: 0.7744, Val Loss: 0.4463, Val Accuracy: 0.8008\n",
      "Validation loss improved, saving model...\n",
      "Epoch [18/50], Train Loss: 0.4739, Train Accuracy: 0.7510, Val Loss: 0.4374, Val Accuracy: 0.8028\n",
      "Validation loss improved, saving model...\n",
      "Epoch [19/50], Train Loss: 0.4736, Train Accuracy: 0.7629, Val Loss: 0.4369, Val Accuracy: 0.8127\n",
      "Validation loss improved, saving model...\n",
      "Epoch [20/50], Train Loss: 0.4744, Train Accuracy: 0.7759, Val Loss: 0.4279, Val Accuracy: 0.8088\n",
      "Epoch [21/50], Train Loss: 0.4549, Train Accuracy: 0.7878, Val Loss: 0.4285, Val Accuracy: 0.8167\n",
      "Validation loss improved, saving model...\n",
      "Epoch [22/50], Train Loss: 0.4724, Train Accuracy: 0.7669, Val Loss: 0.4257, Val Accuracy: 0.8187\n",
      "Validation loss improved, saving model...\n",
      "Epoch [23/50], Train Loss: 0.4375, Train Accuracy: 0.7844, Val Loss: 0.4157, Val Accuracy: 0.8108\n",
      "Validation loss improved, saving model...\n",
      "Epoch [24/50], Train Loss: 0.4499, Train Accuracy: 0.7844, Val Loss: 0.4136, Val Accuracy: 0.8187\n",
      "Validation loss improved, saving model...\n",
      "Epoch [25/50], Train Loss: 0.4434, Train Accuracy: 0.7809, Val Loss: 0.4073, Val Accuracy: 0.8127\n",
      "Epoch [26/50], Train Loss: 0.4317, Train Accuracy: 0.7878, Val Loss: 0.4124, Val Accuracy: 0.8187\n",
      "Validation loss improved, saving model...\n",
      "Epoch [27/50], Train Loss: 0.4407, Train Accuracy: 0.7869, Val Loss: 0.4053, Val Accuracy: 0.8147\n",
      "Validation loss improved, saving model...\n",
      "Epoch [28/50], Train Loss: 0.4377, Train Accuracy: 0.7943, Val Loss: 0.4039, Val Accuracy: 0.8167\n",
      "Validation loss improved, saving model...\n",
      "Epoch [29/50], Train Loss: 0.4307, Train Accuracy: 0.7918, Val Loss: 0.3962, Val Accuracy: 0.8167\n",
      "Epoch [30/50], Train Loss: 0.4315, Train Accuracy: 0.7908, Val Loss: 0.3984, Val Accuracy: 0.8147\n",
      "Validation loss improved, saving model...\n",
      "Epoch [31/50], Train Loss: 0.4178, Train Accuracy: 0.8028, Val Loss: 0.3927, Val Accuracy: 0.8167\n",
      "Epoch [32/50], Train Loss: 0.4194, Train Accuracy: 0.8083, Val Loss: 0.3944, Val Accuracy: 0.8167\n",
      "Validation loss improved, saving model...\n",
      "Epoch [33/50], Train Loss: 0.4044, Train Accuracy: 0.8063, Val Loss: 0.3862, Val Accuracy: 0.8287\n",
      "Epoch [34/50], Train Loss: 0.4068, Train Accuracy: 0.8127, Val Loss: 0.3888, Val Accuracy: 0.8207\n",
      "Validation loss improved, saving model...\n",
      "Epoch [35/50], Train Loss: 0.4121, Train Accuracy: 0.8053, Val Loss: 0.3789, Val Accuracy: 0.8207\n",
      "Epoch [36/50], Train Loss: 0.3974, Train Accuracy: 0.8103, Val Loss: 0.3850, Val Accuracy: 0.8287\n",
      "Validation loss improved, saving model...\n",
      "Epoch [37/50], Train Loss: 0.4024, Train Accuracy: 0.8127, Val Loss: 0.3782, Val Accuracy: 0.8287\n",
      "Validation loss improved, saving model...\n",
      "Epoch [38/50], Train Loss: 0.3984, Train Accuracy: 0.8142, Val Loss: 0.3767, Val Accuracy: 0.8247\n",
      "Validation loss improved, saving model...\n",
      "Epoch [39/50], Train Loss: 0.3944, Train Accuracy: 0.8127, Val Loss: 0.3753, Val Accuracy: 0.8347\n",
      "Validation loss improved, saving model...\n",
      "Epoch [40/50], Train Loss: 0.3929, Train Accuracy: 0.8192, Val Loss: 0.3708, Val Accuracy: 0.8267\n",
      "Epoch [41/50], Train Loss: 0.4077, Train Accuracy: 0.8132, Val Loss: 0.3725, Val Accuracy: 0.8367\n",
      "Validation loss improved, saving model...\n",
      "Epoch [42/50], Train Loss: 0.3972, Train Accuracy: 0.8172, Val Loss: 0.3661, Val Accuracy: 0.8267\n",
      "Validation loss improved, saving model...\n",
      "Epoch [43/50], Train Loss: 0.4071, Train Accuracy: 0.8127, Val Loss: 0.3630, Val Accuracy: 0.8187\n",
      "Epoch [44/50], Train Loss: 0.3957, Train Accuracy: 0.8142, Val Loss: 0.3693, Val Accuracy: 0.8386\n",
      "Validation loss improved, saving model...\n",
      "Epoch [45/50], Train Loss: 0.3930, Train Accuracy: 0.8172, Val Loss: 0.3628, Val Accuracy: 0.8446\n",
      "Epoch [46/50], Train Loss: 0.3963, Train Accuracy: 0.8137, Val Loss: 0.3638, Val Accuracy: 0.8426\n",
      "Validation loss improved, saving model...\n",
      "Epoch [47/50], Train Loss: 0.3929, Train Accuracy: 0.8162, Val Loss: 0.3550, Val Accuracy: 0.8486\n",
      "Epoch [48/50], Train Loss: 0.4021, Train Accuracy: 0.8127, Val Loss: 0.3626, Val Accuracy: 0.8486\n",
      "Epoch [49/50], Train Loss: 0.4019, Train Accuracy: 0.8048, Val Loss: 0.3565, Val Accuracy: 0.8446\n",
      "Validation loss improved, saving model...\n",
      "Epoch [50/50], Train Loss: 0.3775, Train Accuracy: 0.8272, Val Loss: 0.3544, Val Accuracy: 0.8506\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "# 조기 종료 변수 초기화\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # 개선이 없을 때 기다릴 에포크 수\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(DEVICE), targets.to(DEVICE).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = criterion(outputs.view(-1), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 정확도 계산\n",
    "        predicted = (torch.sigmoid(outputs.view(-1)) > 0.5).float()\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "        total_predictions += targets.size(0)\n",
    "\n",
    "    # 훈련 데이터 정확도\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # 검증\n",
    "    val_loss, val_accuracy = evaluate_model(model, validation_loader, criterion)\n",
    "\n",
    "    # 조기 종료 로직\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # 손실 개선 시 카운터 리셋\n",
    "        print(\"Validation loss improved, saving model...\")\n",
    "        # 모델 저장\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, '../model/efficientnet_b1_ft_best_model_02.pth')\n",
    "    else:\n",
    "        patience_counter += 1  # 손실이 개선되지 않으면 카운터 증가\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break  # 훈련 종료\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {running_loss / len(train_loader):.4f}, '\n",
    "          f'Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\AppData\\Local\\Temp\\ipykernel_724\\3408623219.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('../model/efficientnet_b1_ft_best_model_02.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (base_model): EfficientNet(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.10434782608695654, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.11304347826086956, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.12173913043478261, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1391304347826087, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.14782608695652175, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1565217391304348, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.16521739130434784, mode=row)\n",
       "        )\n",
       "        (4): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1920, 1920, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1920, bias=False)\n",
       "              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1920, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(80, 1920, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1920, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.19130434782608696, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (8): Conv2dNormActivation(\n",
       "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Identity()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=1280, out_features=1, bias=True)\n",
       "  (bn1): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomModel('efficientnet').to(DEVICE)\n",
    "\n",
    "# 저장된 체크포인트 불러오기\n",
    "checkpoint = torch.load('../model/efficientnet_b1_ft_best_model_02.pth')\n",
    "\n",
    "# 모델에 체크포인트 적용\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# 1. 사전 학습된 base_model의 파라미터를 동결 (fine-tuning 초기 단계)\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 마지막 몇 개의 레이어를 동결 해제 (features 블록 8 이후 층)\n",
    "for name, param in model.base_model.features[8:].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 동결 해제된 일부 층과 분류기 층 학습\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
    "\n",
    "# 에포크 정보 가져오기 (옵션)\n",
    "start_epoch = checkpoint['epoch']\n",
    "\n",
    "# 모델을 평가 모드로 전환\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved, saving model...\n",
      "Epoch [1/10], Train Loss: 0.1442, Train Accuracy: 0.9402, Val Loss: 0.1771, Val Accuracy: 0.9243\n",
      "Epoch [2/10], Train Loss: 0.1321, Train Accuracy: 0.9432, Val Loss: 0.1887, Val Accuracy: 0.9223\n",
      "Epoch [3/10], Train Loss: 0.1258, Train Accuracy: 0.9552, Val Loss: 0.1927, Val Accuracy: 0.9183\n",
      "Epoch [4/10], Train Loss: 0.1282, Train Accuracy: 0.9452, Val Loss: 0.1800, Val Accuracy: 0.9183\n",
      "Epoch [5/10], Train Loss: 0.1500, Train Accuracy: 0.9412, Val Loss: 0.1942, Val Accuracy: 0.9183\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "# 조기 종료 변수 초기화\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # 개선이 없을 때 기다릴 에포크 수\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(DEVICE), targets.to(DEVICE).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = criterion(outputs.view(-1), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 정확도 계산\n",
    "        predicted = (torch.sigmoid(outputs.view(-1)) > 0.5).float()\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "        total_predictions += targets.size(0)\n",
    "\n",
    "    # 훈련 데이터 정확도\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # 검증\n",
    "    val_loss, val_accuracy = evaluate_model(model, validation_loader, criterion)\n",
    "\n",
    "    # 조기 종료 로직\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # 손실 개선 시 카운터 리셋\n",
    "        print(\"Validation loss improved, saving model...\")\n",
    "        # 모델 저장\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, '../model/efficientnet_b1_ft_best_model_02.pth')\n",
    "    else:\n",
    "        patience_counter += 1  # 손실이 개선되지 않으면 카운터 증가\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break  # 훈련 종료\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {running_loss / len(train_loader):.4f}, '\n",
    "          f'Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\AppData\\Local\\Temp\\ipykernel_724\\3408623219.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('../model/efficientnet_b1_ft_best_model_02.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (base_model): EfficientNet(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.10434782608695654, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.11304347826086956, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.12173913043478261, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1391304347826087, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.14782608695652175, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1565217391304348, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.16521739130434784, mode=row)\n",
       "        )\n",
       "        (4): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1920, 1920, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1920, bias=False)\n",
       "              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1920, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(80, 1920, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1920, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.19130434782608696, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (8): Conv2dNormActivation(\n",
       "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Identity()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=1280, out_features=1, bias=True)\n",
       "  (bn1): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomModel('efficientnet').to(DEVICE)\n",
    "\n",
    "# 저장된 체크포인트 불러오기\n",
    "checkpoint = torch.load('../model/efficientnet_b1_ft_best_model_02.pth')\n",
    "\n",
    "# 모델에 체크포인트 적용\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# 1. 사전 학습된 base_model의 파라미터를 동결 (fine-tuning 초기 단계)\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 마지막 몇 개의 레이어를 동결 해제 (features 블록 8 이후 층)\n",
    "for name, param in model.base_model.features[8:].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 동결 해제된 일부 층과 분류기 층 학습\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
    "\n",
    "# 에포크 정보 가져오기 (옵션)\n",
    "start_epoch = checkpoint['epoch']\n",
    "\n",
    "# 모델을 평가 모드로 전환\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1618, Test Accuracy: 0.9411\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터세트로 평가\n",
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: base_model.features.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.2.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.2.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.2.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.2.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.2.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.2.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.8.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.8.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.8.1.bias | Requires Grad: True\n",
      "Layer: fc1.weight | Requires Grad: True\n",
      "Layer: fc1.bias | Requires Grad: True\n",
      "Layer: bn1.weight | Requires Grad: True\n",
      "Layer: bn1.bias | Requires Grad: True\n",
      "Layer: fc2.weight | Requires Grad: True\n",
      "Layer: fc2.bias | Requires Grad: True\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "model = CustomModel(model_name='efficientnet').to(DEVICE)\n",
    "\n",
    "# 1. 사전 학습된 base_model의 파라미터를 동결 (fine-tuning 초기 단계)\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 마지막 몇 개의 레이어를 동결 해제 (features 블록 7 이후 층)\n",
    "for name, param in model.base_model.features[7:].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 동결 해제된 일부 층과 분류기 층 학습\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
    "\n",
    "# 학습할 손실 함수\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 각 층의 freeze/unfreeze 상태 확인\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Requires Grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved, saving model...\n",
      "Epoch [1/50], Train Loss: 0.7050, Train Accuracy: 0.5588, Val Loss: 0.6123, Val Accuracy: 0.6733\n",
      "Validation loss improved, saving model...\n",
      "Epoch [2/50], Train Loss: 0.6125, Train Accuracy: 0.6633, Val Loss: 0.5556, Val Accuracy: 0.7271\n",
      "Validation loss improved, saving model...\n",
      "Epoch [3/50], Train Loss: 0.5746, Train Accuracy: 0.6972, Val Loss: 0.5138, Val Accuracy: 0.7570\n",
      "Validation loss improved, saving model...\n",
      "Epoch [4/50], Train Loss: 0.5351, Train Accuracy: 0.7206, Val Loss: 0.4788, Val Accuracy: 0.7928\n",
      "Validation loss improved, saving model...\n",
      "Epoch [5/50], Train Loss: 0.5041, Train Accuracy: 0.7450, Val Loss: 0.4630, Val Accuracy: 0.8008\n",
      "Validation loss improved, saving model...\n",
      "Epoch [6/50], Train Loss: 0.4837, Train Accuracy: 0.7679, Val Loss: 0.4353, Val Accuracy: 0.8147\n",
      "Validation loss improved, saving model...\n",
      "Epoch [7/50], Train Loss: 0.4614, Train Accuracy: 0.7829, Val Loss: 0.4251, Val Accuracy: 0.8147\n",
      "Validation loss improved, saving model...\n",
      "Epoch [8/50], Train Loss: 0.4458, Train Accuracy: 0.7874, Val Loss: 0.4071, Val Accuracy: 0.8207\n",
      "Validation loss improved, saving model...\n",
      "Epoch [9/50], Train Loss: 0.4365, Train Accuracy: 0.7983, Val Loss: 0.3923, Val Accuracy: 0.8347\n",
      "Validation loss improved, saving model...\n",
      "Epoch [10/50], Train Loss: 0.4196, Train Accuracy: 0.8023, Val Loss: 0.3914, Val Accuracy: 0.8227\n",
      "Validation loss improved, saving model...\n",
      "Epoch [11/50], Train Loss: 0.4173, Train Accuracy: 0.8038, Val Loss: 0.3789, Val Accuracy: 0.8327\n",
      "Validation loss improved, saving model...\n",
      "Epoch [12/50], Train Loss: 0.4158, Train Accuracy: 0.8048, Val Loss: 0.3680, Val Accuracy: 0.8347\n",
      "Validation loss improved, saving model...\n",
      "Epoch [13/50], Train Loss: 0.3996, Train Accuracy: 0.8162, Val Loss: 0.3630, Val Accuracy: 0.8426\n",
      "Validation loss improved, saving model...\n",
      "Epoch [14/50], Train Loss: 0.3890, Train Accuracy: 0.8162, Val Loss: 0.3565, Val Accuracy: 0.8466\n",
      "Validation loss improved, saving model...\n",
      "Epoch [15/50], Train Loss: 0.3930, Train Accuracy: 0.8172, Val Loss: 0.3512, Val Accuracy: 0.8426\n",
      "Validation loss improved, saving model...\n",
      "Epoch [16/50], Train Loss: 0.3739, Train Accuracy: 0.8177, Val Loss: 0.3453, Val Accuracy: 0.8486\n",
      "Validation loss improved, saving model...\n",
      "Epoch [17/50], Train Loss: 0.3857, Train Accuracy: 0.8182, Val Loss: 0.3411, Val Accuracy: 0.8486\n",
      "Validation loss improved, saving model...\n",
      "Epoch [18/50], Train Loss: 0.3727, Train Accuracy: 0.8247, Val Loss: 0.3383, Val Accuracy: 0.8486\n",
      "Validation loss improved, saving model...\n",
      "Epoch [19/50], Train Loss: 0.3646, Train Accuracy: 0.8252, Val Loss: 0.3318, Val Accuracy: 0.8546\n",
      "Validation loss improved, saving model...\n",
      "Epoch [20/50], Train Loss: 0.3487, Train Accuracy: 0.8381, Val Loss: 0.3255, Val Accuracy: 0.8526\n",
      "Validation loss improved, saving model...\n",
      "Epoch [21/50], Train Loss: 0.3480, Train Accuracy: 0.8441, Val Loss: 0.3194, Val Accuracy: 0.8606\n",
      "Epoch [22/50], Train Loss: 0.3460, Train Accuracy: 0.8367, Val Loss: 0.3195, Val Accuracy: 0.8566\n",
      "Validation loss improved, saving model...\n",
      "Epoch [23/50], Train Loss: 0.3413, Train Accuracy: 0.8456, Val Loss: 0.3139, Val Accuracy: 0.8566\n",
      "Validation loss improved, saving model...\n",
      "Epoch [24/50], Train Loss: 0.3388, Train Accuracy: 0.8506, Val Loss: 0.3131, Val Accuracy: 0.8606\n",
      "Validation loss improved, saving model...\n",
      "Epoch [25/50], Train Loss: 0.3320, Train Accuracy: 0.8496, Val Loss: 0.3043, Val Accuracy: 0.8665\n",
      "Validation loss improved, saving model...\n",
      "Epoch [26/50], Train Loss: 0.3213, Train Accuracy: 0.8526, Val Loss: 0.3021, Val Accuracy: 0.8645\n",
      "Validation loss improved, saving model...\n",
      "Epoch [27/50], Train Loss: 0.3328, Train Accuracy: 0.8461, Val Loss: 0.2983, Val Accuracy: 0.8645\n",
      "Validation loss improved, saving model...\n",
      "Epoch [28/50], Train Loss: 0.3310, Train Accuracy: 0.8476, Val Loss: 0.2935, Val Accuracy: 0.8725\n",
      "Validation loss improved, saving model...\n",
      "Epoch [29/50], Train Loss: 0.3212, Train Accuracy: 0.8541, Val Loss: 0.2923, Val Accuracy: 0.8725\n",
      "Epoch [30/50], Train Loss: 0.3183, Train Accuracy: 0.8576, Val Loss: 0.2943, Val Accuracy: 0.8725\n",
      "Validation loss improved, saving model...\n",
      "Epoch [31/50], Train Loss: 0.3119, Train Accuracy: 0.8616, Val Loss: 0.2918, Val Accuracy: 0.8725\n",
      "Validation loss improved, saving model...\n",
      "Epoch [32/50], Train Loss: 0.2915, Train Accuracy: 0.8745, Val Loss: 0.2882, Val Accuracy: 0.8685\n",
      "Validation loss improved, saving model...\n",
      "Epoch [33/50], Train Loss: 0.3003, Train Accuracy: 0.8640, Val Loss: 0.2747, Val Accuracy: 0.8745\n",
      "Validation loss improved, saving model...\n",
      "Epoch [34/50], Train Loss: 0.2922, Train Accuracy: 0.8710, Val Loss: 0.2725, Val Accuracy: 0.8745\n",
      "Epoch [35/50], Train Loss: 0.2939, Train Accuracy: 0.8755, Val Loss: 0.2833, Val Accuracy: 0.8805\n",
      "Validation loss improved, saving model...\n",
      "Epoch [36/50], Train Loss: 0.2911, Train Accuracy: 0.8710, Val Loss: 0.2683, Val Accuracy: 0.8765\n",
      "Epoch [37/50], Train Loss: 0.2967, Train Accuracy: 0.8710, Val Loss: 0.2805, Val Accuracy: 0.8785\n",
      "Validation loss improved, saving model...\n",
      "Epoch [38/50], Train Loss: 0.2824, Train Accuracy: 0.8760, Val Loss: 0.2646, Val Accuracy: 0.8884\n",
      "Validation loss improved, saving model...\n",
      "Epoch [39/50], Train Loss: 0.2805, Train Accuracy: 0.8720, Val Loss: 0.2614, Val Accuracy: 0.8865\n",
      "Validation loss improved, saving model...\n",
      "Epoch [40/50], Train Loss: 0.2715, Train Accuracy: 0.8820, Val Loss: 0.2601, Val Accuracy: 0.8944\n",
      "Validation loss improved, saving model...\n",
      "Epoch [41/50], Train Loss: 0.2777, Train Accuracy: 0.8825, Val Loss: 0.2590, Val Accuracy: 0.8865\n",
      "Epoch [42/50], Train Loss: 0.2645, Train Accuracy: 0.8914, Val Loss: 0.2591, Val Accuracy: 0.8924\n",
      "Validation loss improved, saving model...\n",
      "Epoch [43/50], Train Loss: 0.2808, Train Accuracy: 0.8770, Val Loss: 0.2441, Val Accuracy: 0.8984\n",
      "Epoch [44/50], Train Loss: 0.2565, Train Accuracy: 0.8924, Val Loss: 0.2564, Val Accuracy: 0.8924\n",
      "Epoch [45/50], Train Loss: 0.2655, Train Accuracy: 0.8845, Val Loss: 0.2456, Val Accuracy: 0.8984\n",
      "Epoch [46/50], Train Loss: 0.2541, Train Accuracy: 0.8919, Val Loss: 0.2569, Val Accuracy: 0.8825\n",
      "Epoch [47/50], Train Loss: 0.2536, Train Accuracy: 0.8889, Val Loss: 0.2465, Val Accuracy: 0.9004\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "# 조기 종료 변수 초기화\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # 개선이 없을 때 기다릴 에포크 수\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(DEVICE), targets.to(DEVICE).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = criterion(outputs.view(-1), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 정확도 계산\n",
    "        predicted = (torch.sigmoid(outputs.view(-1)) > 0.5).float()\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "        total_predictions += targets.size(0)\n",
    "\n",
    "    # 훈련 데이터 정확도\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # 검증\n",
    "    val_loss, val_accuracy = evaluate_model(model, validation_loader, criterion)\n",
    "\n",
    "    # 조기 종료 로직\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # 손실 개선 시 카운터 리셋\n",
    "        print(\"Validation loss improved, saving model...\")\n",
    "        # 모델 저장\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, '../model/efficientnet_b1_ft_best_model_03.pth')\n",
    "    else:\n",
    "        patience_counter += 1  # 손실이 개선되지 않으면 카운터 증가\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break  # 훈련 종료\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {running_loss / len(train_loader):.4f}, '\n",
    "          f'Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features 7 이후 블록을 unfreeze 한 후, 학습하였으나 이전보다 성능이 더 낮아짐.  ???????   \n",
    "features 8 이후 블록을 unfreeze 하되, 데이터 증강, ReLU함수 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model_name='efficientnet'):\n",
    "        super(CustomModel, self).__init__()\n",
    "        if model_name == 'efficientnet':\n",
    "            self.base_model = torchvision.models.efficientnet_b1(weights='IMAGENET1K_V1')\n",
    "            self.base_model.classifier = nn.Identity()  # 마지막 분류기 제거\n",
    "        elif model_name == 'resnet50':\n",
    "            self.base_model = torchvision.models.resnet50(weights='IMAGENET1K_V1')\n",
    "            self.base_model = nn.Sequential(*list(self.base_model.children())[:-1])  # 마지막 레이어 제거\n",
    "        elif model_name == 'inception':\n",
    "            self.base_model = torchvision.models.inception_v3(weights='IMAGENET1K_V1')\n",
    "            self.base_model.classifier = nn.Identity()  # 마지막 분류기 제거\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(self._get_features_dim(model_name), 50)\n",
    "        self.bn1 = nn.BatchNorm1d(50)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "\n",
    "    def _get_features_dim(self, model_name):\n",
    "        if model_name == 'efficientnet':\n",
    "            return 1280  # EfficientNet B0의 출력 차원\n",
    "        elif model_name == 'resnet50':\n",
    "            return 2048  # ResNet50의 출력 차원\n",
    "        elif model_name == 'inception':\n",
    "            return 2048  # Inception의 출력 차원\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        \n",
    "        # Inception 모델에 대한 수정\n",
    "        if isinstance(x, tuple):  # Inception 모델이 여러 출력을 반환하는 경우\n",
    "            x = x[0]  # 첫 번째 출력을 선택\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x  # 최종 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 데이터 증강 정의\n",
    "aug = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),                # 좌우 반전\n",
    "    A.VerticalFlip(p=0.5),                  # 상하 반전\n",
    "    A.Rotate(limit=10, p=0.5),              # 작은 각도 회전 (10도 내외)\n",
    "    A.RandomBrightnessContrast(p=0.5),      # 밝기 및 대비 조절\n",
    "])\n",
    "\n",
    "# 데이터셋 인스턴스 생성\n",
    "train_dataset = CustomDataset(train_df['file_paths'].values, train_df['targets'].values, aug=aug)\n",
    "validation_dataset = CustomDataset(validation_df['file_paths'].values, validation_df['targets'].values)\n",
    "test_dataset = CustomDataset(test_df['file_paths'].values, test_df['targets'].values)\n",
    "\n",
    "# DataLoader 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: base_model.features.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.2.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.2.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.2.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.2.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.2.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.2.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.8.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.8.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.8.1.bias | Requires Grad: True\n",
      "Layer: fc1.weight | Requires Grad: True\n",
      "Layer: fc1.bias | Requires Grad: True\n",
      "Layer: bn1.weight | Requires Grad: True\n",
      "Layer: bn1.bias | Requires Grad: True\n",
      "Layer: fc2.weight | Requires Grad: True\n",
      "Layer: fc2.bias | Requires Grad: True\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "model = CustomModel(model_name='efficientnet').to(DEVICE)\n",
    "\n",
    "# 1. 사전 학습된 base_model의 파라미터를 동결 (fine-tuning 초기 단계)\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 마지막 몇 개의 레이어를 동결 해제 (features 블록 8 이후 층)\n",
    "for name, param in model.base_model.features[8:].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 동결 해제된 일부 층과 분류기 층 학습\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
    "\n",
    "# 학습할 손실 함수\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 각 층의 freeze/unfreeze 상태 확인\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Requires Grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved, saving model...\n",
      "Epoch [1/50], Train Loss: 0.7569, Train Accuracy: 0.5085, Val Loss: 0.6839, Val Accuracy: 0.5319, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [2/50], Train Loss: 0.7171, Train Accuracy: 0.5518, Val Loss: 0.6505, Val Accuracy: 0.6335, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [3/50], Train Loss: 0.6722, Train Accuracy: 0.6061, Val Loss: 0.6169, Val Accuracy: 0.7131, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [4/50], Train Loss: 0.6529, Train Accuracy: 0.6150, Val Loss: 0.5940, Val Accuracy: 0.7390, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [5/50], Train Loss: 0.6302, Train Accuracy: 0.6340, Val Loss: 0.5703, Val Accuracy: 0.7550, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [6/50], Train Loss: 0.6132, Train Accuracy: 0.6584, Val Loss: 0.5538, Val Accuracy: 0.7649, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [7/50], Train Loss: 0.6026, Train Accuracy: 0.6728, Val Loss: 0.5389, Val Accuracy: 0.7709, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [8/50], Train Loss: 0.5955, Train Accuracy: 0.6718, Val Loss: 0.5253, Val Accuracy: 0.7709, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [9/50], Train Loss: 0.5749, Train Accuracy: 0.7042, Val Loss: 0.5196, Val Accuracy: 0.7709, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [10/50], Train Loss: 0.5581, Train Accuracy: 0.7077, Val Loss: 0.5098, Val Accuracy: 0.7789, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [11/50], Train Loss: 0.5599, Train Accuracy: 0.7122, Val Loss: 0.4957, Val Accuracy: 0.7928, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [12/50], Train Loss: 0.5487, Train Accuracy: 0.7226, Val Loss: 0.4906, Val Accuracy: 0.7849, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [13/50], Train Loss: 0.5416, Train Accuracy: 0.7291, Val Loss: 0.4854, Val Accuracy: 0.7888, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [14/50], Train Loss: 0.5243, Train Accuracy: 0.7341, Val Loss: 0.4806, Val Accuracy: 0.7968, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [15/50], Train Loss: 0.5279, Train Accuracy: 0.7351, Val Loss: 0.4729, Val Accuracy: 0.8008, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [16/50], Train Loss: 0.5222, Train Accuracy: 0.7495, Val Loss: 0.4639, Val Accuracy: 0.8028, Learning Rate: 0.000010\n",
      "Epoch [17/50], Train Loss: 0.5107, Train Accuracy: 0.7545, Val Loss: 0.4673, Val Accuracy: 0.8088, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [18/50], Train Loss: 0.5046, Train Accuracy: 0.7525, Val Loss: 0.4605, Val Accuracy: 0.7948, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [19/50], Train Loss: 0.4912, Train Accuracy: 0.7555, Val Loss: 0.4541, Val Accuracy: 0.8048, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [20/50], Train Loss: 0.5032, Train Accuracy: 0.7490, Val Loss: 0.4516, Val Accuracy: 0.8088, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [21/50], Train Loss: 0.4982, Train Accuracy: 0.7585, Val Loss: 0.4479, Val Accuracy: 0.8008, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [22/50], Train Loss: 0.4768, Train Accuracy: 0.7724, Val Loss: 0.4431, Val Accuracy: 0.8068, Learning Rate: 0.000010\n",
      "Epoch [23/50], Train Loss: 0.4851, Train Accuracy: 0.7600, Val Loss: 0.4479, Val Accuracy: 0.7948, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [24/50], Train Loss: 0.4819, Train Accuracy: 0.7644, Val Loss: 0.4312, Val Accuracy: 0.8008, Learning Rate: 0.000010\n",
      "Epoch [25/50], Train Loss: 0.4808, Train Accuracy: 0.7694, Val Loss: 0.4362, Val Accuracy: 0.8108, Learning Rate: 0.000010\n",
      "Epoch [26/50], Train Loss: 0.4776, Train Accuracy: 0.7714, Val Loss: 0.4324, Val Accuracy: 0.8088, Learning Rate: 0.000010\n",
      "Epoch [27/50], Train Loss: 0.4708, Train Accuracy: 0.7764, Val Loss: 0.4320, Val Accuracy: 0.8088, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [28/50], Train Loss: 0.4752, Train Accuracy: 0.7759, Val Loss: 0.4219, Val Accuracy: 0.8167, Learning Rate: 0.000010\n",
      "Epoch [29/50], Train Loss: 0.4811, Train Accuracy: 0.7585, Val Loss: 0.4264, Val Accuracy: 0.8147, Learning Rate: 0.000010\n",
      "Epoch [30/50], Train Loss: 0.4660, Train Accuracy: 0.7784, Val Loss: 0.4234, Val Accuracy: 0.8187, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [31/50], Train Loss: 0.4550, Train Accuracy: 0.7893, Val Loss: 0.4206, Val Accuracy: 0.8247, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [32/50], Train Loss: 0.4631, Train Accuracy: 0.7709, Val Loss: 0.4162, Val Accuracy: 0.8207, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [33/50], Train Loss: 0.4594, Train Accuracy: 0.7729, Val Loss: 0.4159, Val Accuracy: 0.8167, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [34/50], Train Loss: 0.4473, Train Accuracy: 0.7784, Val Loss: 0.4141, Val Accuracy: 0.8287, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [35/50], Train Loss: 0.4563, Train Accuracy: 0.7799, Val Loss: 0.4099, Val Accuracy: 0.8327, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [36/50], Train Loss: 0.4413, Train Accuracy: 0.7938, Val Loss: 0.4075, Val Accuracy: 0.8187, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [37/50], Train Loss: 0.4560, Train Accuracy: 0.7819, Val Loss: 0.4029, Val Accuracy: 0.8267, Learning Rate: 0.000010\n",
      "Epoch [38/50], Train Loss: 0.4428, Train Accuracy: 0.7968, Val Loss: 0.4076, Val Accuracy: 0.8227, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [39/50], Train Loss: 0.4465, Train Accuracy: 0.7888, Val Loss: 0.4026, Val Accuracy: 0.8327, Learning Rate: 0.000010\n",
      "Epoch [40/50], Train Loss: 0.4334, Train Accuracy: 0.7918, Val Loss: 0.4114, Val Accuracy: 0.8247, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [41/50], Train Loss: 0.4415, Train Accuracy: 0.7859, Val Loss: 0.3990, Val Accuracy: 0.8347, Learning Rate: 0.000010\n",
      "Epoch [42/50], Train Loss: 0.4457, Train Accuracy: 0.7839, Val Loss: 0.4040, Val Accuracy: 0.8127, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [43/50], Train Loss: 0.4351, Train Accuracy: 0.7923, Val Loss: 0.3950, Val Accuracy: 0.8327, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [44/50], Train Loss: 0.4240, Train Accuracy: 0.7943, Val Loss: 0.3896, Val Accuracy: 0.8347, Learning Rate: 0.000010\n",
      "Epoch [45/50], Train Loss: 0.4270, Train Accuracy: 0.7913, Val Loss: 0.3936, Val Accuracy: 0.8287, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [46/50], Train Loss: 0.4349, Train Accuracy: 0.7903, Val Loss: 0.3882, Val Accuracy: 0.8347, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [47/50], Train Loss: 0.4309, Train Accuracy: 0.7878, Val Loss: 0.3862, Val Accuracy: 0.8406, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [48/50], Train Loss: 0.4318, Train Accuracy: 0.8038, Val Loss: 0.3849, Val Accuracy: 0.8307, Learning Rate: 0.000010\n",
      "Epoch [49/50], Train Loss: 0.4274, Train Accuracy: 0.7963, Val Loss: 0.3878, Val Accuracy: 0.8386, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [50/50], Train Loss: 0.4174, Train Accuracy: 0.7968, Val Loss: 0.3841, Val Accuracy: 0.8367, Learning Rate: 0.000010\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "# 조기 종료 변수 초기화\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # 개선이 없을 때 기다릴 에포크 수\n",
    "patience_counter = 0\n",
    "\n",
    "# 학습률 스케줄러\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(DEVICE), targets.to(DEVICE).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = criterion(outputs.view(-1), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 정확도 계산\n",
    "        predicted = (torch.sigmoid(outputs.view(-1)) > 0.5).float()\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "        total_predictions += targets.size(0)\n",
    "\n",
    "    # 훈련 데이터 정확도\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # 검증\n",
    "    val_loss, val_accuracy = evaluate_model(model, validation_loader, criterion)\n",
    "\n",
    "    # 학습률 스케줄러 적용\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # 현재 학습률 출력\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    # 조기 종료 로직\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # 손실 개선 시 카운터 리셋\n",
    "        print(\"Validation loss improved, saving model...\")\n",
    "        # 모델 저장\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, '../model/efficientnet_b1_ft_best_model_04.pth')\n",
    "    else:\n",
    "        patience_counter += 1  # 손실이 개선되지 않으면 카운터 증가\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break  # 훈련 종료\n",
    "\n",
    "    # 에포크별 정보 출력 (학습률 포함)\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {running_loss / len(train_loader):.4f}, '\n",
    "          f'Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, '\n",
    "          f'Learning Rate: {current_lr:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\AppData\\Local\\Temp\\ipykernel_16704\\3511494907.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('../model/efficientnet_b1_ft_best_model_04.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (base_model): EfficientNet(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.10434782608695654, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.11304347826086956, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.12173913043478261, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1391304347826087, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.14782608695652175, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1565217391304348, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.16521739130434784, mode=row)\n",
       "        )\n",
       "        (4): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1920, 1920, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1920, bias=False)\n",
       "              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1920, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(80, 1920, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1920, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.19130434782608696, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (8): Conv2dNormActivation(\n",
       "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Identity()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=1280, out_features=50, bias=True)\n",
       "  (bn1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomModel('efficientnet').to(DEVICE)\n",
    "\n",
    "checkpoint = torch.load('../model/efficientnet_b1_ft_best_model_04.pth')\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 마지막 몇 개의 레이어를 동결 해제 (features 블록 8 이후 층)\n",
    "for name, param in model.base_model.features[8:].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 동결 해제된 일부 층과 분류기 층 학습\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.3839, Test accuracy: 0.8185\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추가 50 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved, saving model...\n",
      "Epoch [1/50], Train Loss: 0.4260, Train Accuracy: 0.7923, Val Loss: 0.3840, Val Accuracy: 0.8386, Learning Rate: 0.000010\n",
      "Epoch [2/50], Train Loss: 0.4291, Train Accuracy: 0.7978, Val Loss: 0.3878, Val Accuracy: 0.8406, Learning Rate: 0.000010\n",
      "Validation loss improved, saving model...\n",
      "Epoch [3/50], Train Loss: 0.4102, Train Accuracy: 0.8068, Val Loss: 0.3739, Val Accuracy: 0.8406, Learning Rate: 0.000010\n",
      "Epoch [4/50], Train Loss: 0.4195, Train Accuracy: 0.8063, Val Loss: 0.3785, Val Accuracy: 0.8446, Learning Rate: 0.000010\n",
      "Epoch [5/50], Train Loss: 0.4190, Train Accuracy: 0.7958, Val Loss: 0.3788, Val Accuracy: 0.8446, Learning Rate: 0.000010\n",
      "Epoch [6/50], Train Loss: 0.4080, Train Accuracy: 0.8038, Val Loss: 0.3780, Val Accuracy: 0.8406, Learning Rate: 0.000010\n",
      "Epoch [7/50], Train Loss: 0.4167, Train Accuracy: 0.8053, Val Loss: 0.3772, Val Accuracy: 0.8466, Learning Rate: 0.000005\n",
      "Validation loss improved, saving model...\n",
      "Epoch [8/50], Train Loss: 0.4049, Train Accuracy: 0.8192, Val Loss: 0.3725, Val Accuracy: 0.8466, Learning Rate: 0.000005\n",
      "Validation loss improved, saving model...\n",
      "Epoch [9/50], Train Loss: 0.4099, Train Accuracy: 0.8073, Val Loss: 0.3675, Val Accuracy: 0.8486, Learning Rate: 0.000005\n",
      "Epoch [10/50], Train Loss: 0.4063, Train Accuracy: 0.8123, Val Loss: 0.3739, Val Accuracy: 0.8486, Learning Rate: 0.000005\n",
      "Epoch [11/50], Train Loss: 0.4118, Train Accuracy: 0.7968, Val Loss: 0.3762, Val Accuracy: 0.8486, Learning Rate: 0.000005\n",
      "Epoch [12/50], Train Loss: 0.4112, Train Accuracy: 0.8068, Val Loss: 0.3731, Val Accuracy: 0.8466, Learning Rate: 0.000005\n",
      "Epoch [13/50], Train Loss: 0.4110, Train Accuracy: 0.8028, Val Loss: 0.3739, Val Accuracy: 0.8546, Learning Rate: 0.000003\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "# 조기 종료 변수 초기화\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # 개선이 없을 때 기다릴 에포크 수\n",
    "patience_counter = 0\n",
    "\n",
    "# 학습률 스케줄러\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(DEVICE), targets.to(DEVICE).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = criterion(outputs.view(-1), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 정확도 계산\n",
    "        predicted = (torch.sigmoid(outputs.view(-1)) > 0.5).float()\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "        total_predictions += targets.size(0)\n",
    "\n",
    "    # 훈련 데이터 정확도\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # 검증\n",
    "    val_loss, val_accuracy = evaluate_model(model, validation_loader, criterion)\n",
    "\n",
    "    # 학습률 스케줄러 적용\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # 현재 학습률 출력\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    # 조기 종료 로직\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # 손실 개선 시 카운터 리셋\n",
    "        print(\"Validation loss improved, saving model...\")\n",
    "        # 모델 저장\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, '../model/efficientnet_b1_ft_best_model_04.pth')\n",
    "    else:\n",
    "        patience_counter += 1  # 손실이 개선되지 않으면 카운터 증가\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break  # 훈련 종료\n",
    "\n",
    "    # 에포크별 정보 출력 (학습률 포함)\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {running_loss / len(train_loader):.4f}, '\n",
    "          f'Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, '\n",
    "          f'Learning Rate: {current_lr:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\AppData\\Local\\Temp\\ipykernel_16704\\3511494907.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('../model/efficientnet_b1_ft_best_model_04.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (base_model): EfficientNet(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.10434782608695654, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.11304347826086956, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.12173913043478261, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1391304347826087, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.14782608695652175, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1565217391304348, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.16521739130434784, mode=row)\n",
       "        )\n",
       "        (4): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1920, 1920, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1920, bias=False)\n",
       "              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1920, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(80, 1920, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1920, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.19130434782608696, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (8): Conv2dNormActivation(\n",
       "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Identity()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=1280, out_features=50, bias=True)\n",
       "  (bn1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomModel('efficientnet').to(DEVICE)\n",
    "\n",
    "checkpoint = torch.load('../model/efficientnet_b1_ft_best_model_04.pth')\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 마지막 몇 개의 레이어를 동결 해제 (features 블록 8 이후 층)\n",
    "for name, param in model.base_model.features[8:].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 동결 해제된 일부 층과 분류기 층 학습\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.3640, Test accuracy: 0.8392\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여전히 가벼운 모델..?  \n",
    "feature 5 ~ unfreeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model_name='efficientnet'):\n",
    "        super(CustomModel, self).__init__()\n",
    "        if model_name == 'efficientnet':\n",
    "            self.base_model = torchvision.models.efficientnet_b1(weights='IMAGENET1K_V1')\n",
    "            self.base_model.classifier = nn.Identity()  # 마지막 분류기 제거\n",
    "        elif model_name == 'resnet50':\n",
    "            self.base_model = torchvision.models.resnet50(weights='IMAGENET1K_V1')\n",
    "            self.base_model = nn.Sequential(*list(self.base_model.children())[:-1])  # 마지막 레이어 제거\n",
    "        elif model_name == 'inception':\n",
    "            self.base_model = torchvision.models.inception_v3(weights='IMAGENET1K_V1')\n",
    "            self.base_model.classifier = nn.Identity()  # 마지막 분류기 제거\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.5)  # 첫 번째 드롭아웃\n",
    "        self.fc1 = nn.Linear(self._get_features_dim(model_name), 50)\n",
    "        self.bn1 = nn.BatchNorm1d(50)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.5)  # 두 번째 드롭아웃\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "\n",
    "    def _get_features_dim(self, model_name):\n",
    "        if model_name == 'efficientnet':\n",
    "            return 1280  # EfficientNet B0의 출력 차원\n",
    "        elif model_name == 'resnet50':\n",
    "            return 2048  # ResNet50의 출력 차원\n",
    "        elif model_name == 'inception':\n",
    "            return 2048  # Inception의 출력 차원\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "\n",
    "        if isinstance(x, tuple):  \n",
    "            x = x[0]\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x) \n",
    "        x = self.bn1(x)  # 배치 정규화 적용\n",
    "        x = self.dropout2(x) \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: base_model.features.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.2.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.2.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.2.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.2.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.2.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.2.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.8.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.8.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.8.1.bias | Requires Grad: True\n",
      "Layer: fc1.weight | Requires Grad: True\n",
      "Layer: fc1.bias | Requires Grad: True\n",
      "Layer: bn1.weight | Requires Grad: True\n",
      "Layer: bn1.bias | Requires Grad: True\n",
      "Layer: fc2.weight | Requires Grad: True\n",
      "Layer: fc2.bias | Requires Grad: True\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "model = CustomModel(model_name='efficientnet').to(DEVICE)\n",
    "\n",
    "# 1. 사전 학습된 base_model의 파라미터를 동결 (fine-tuning 초기 단계)\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 마지막 몇 개의 레이어를 동결 해제 (features 블록 5 이후 층)\n",
    "for name, param in model.base_model.features[5:].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 동결 해제된 일부 층과 분류기 층 학습\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
    "\n",
    "# 학습할 손실 함수\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 각 층의 freeze/unfreeze 상태 확인\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Requires Grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved, saving model...\n",
      "Epoch [1/50], Train Loss: 0.6282, Train Accuracy: 0.6474, Val Loss: 0.4945, Val Accuracy: 0.8048, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [2/50], Train Loss: 0.5286, Train Accuracy: 0.7356, Val Loss: 0.4360, Val Accuracy: 0.8347, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [3/50], Train Loss: 0.4771, Train Accuracy: 0.7764, Val Loss: 0.3998, Val Accuracy: 0.8566, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [4/50], Train Loss: 0.4431, Train Accuracy: 0.7988, Val Loss: 0.3793, Val Accuracy: 0.8645, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [5/50], Train Loss: 0.4210, Train Accuracy: 0.8123, Val Loss: 0.3610, Val Accuracy: 0.8884, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [6/50], Train Loss: 0.4006, Train Accuracy: 0.8272, Val Loss: 0.3551, Val Accuracy: 0.8805, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [7/50], Train Loss: 0.3802, Train Accuracy: 0.8436, Val Loss: 0.3271, Val Accuracy: 0.9064, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [8/50], Train Loss: 0.3574, Train Accuracy: 0.8571, Val Loss: 0.3191, Val Accuracy: 0.9064, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [9/50], Train Loss: 0.3367, Train Accuracy: 0.8745, Val Loss: 0.3008, Val Accuracy: 0.9064, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [10/50], Train Loss: 0.3123, Train Accuracy: 0.8894, Val Loss: 0.2831, Val Accuracy: 0.9044, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [11/50], Train Loss: 0.2971, Train Accuracy: 0.9074, Val Loss: 0.2711, Val Accuracy: 0.9084, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [12/50], Train Loss: 0.2833, Train Accuracy: 0.9089, Val Loss: 0.2648, Val Accuracy: 0.9143, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [13/50], Train Loss: 0.2735, Train Accuracy: 0.9049, Val Loss: 0.2504, Val Accuracy: 0.9163, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [14/50], Train Loss: 0.2573, Train Accuracy: 0.9168, Val Loss: 0.2322, Val Accuracy: 0.9183, Learning Rate: 0.000050\n",
      "Epoch [15/50], Train Loss: 0.2473, Train Accuracy: 0.9228, Val Loss: 0.2335, Val Accuracy: 0.9243, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [16/50], Train Loss: 0.2242, Train Accuracy: 0.9333, Val Loss: 0.2265, Val Accuracy: 0.9283, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [17/50], Train Loss: 0.2220, Train Accuracy: 0.9308, Val Loss: 0.2240, Val Accuracy: 0.9303, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [18/50], Train Loss: 0.2163, Train Accuracy: 0.9363, Val Loss: 0.2164, Val Accuracy: 0.9283, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [19/50], Train Loss: 0.1946, Train Accuracy: 0.9537, Val Loss: 0.2069, Val Accuracy: 0.9442, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [20/50], Train Loss: 0.1883, Train Accuracy: 0.9512, Val Loss: 0.2008, Val Accuracy: 0.9382, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [21/50], Train Loss: 0.1925, Train Accuracy: 0.9537, Val Loss: 0.1905, Val Accuracy: 0.9422, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [22/50], Train Loss: 0.1662, Train Accuracy: 0.9622, Val Loss: 0.1869, Val Accuracy: 0.9482, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [23/50], Train Loss: 0.1696, Train Accuracy: 0.9622, Val Loss: 0.1806, Val Accuracy: 0.9502, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [24/50], Train Loss: 0.1526, Train Accuracy: 0.9686, Val Loss: 0.1721, Val Accuracy: 0.9502, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [25/50], Train Loss: 0.1622, Train Accuracy: 0.9547, Val Loss: 0.1682, Val Accuracy: 0.9502, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [26/50], Train Loss: 0.1472, Train Accuracy: 0.9686, Val Loss: 0.1571, Val Accuracy: 0.9562, Learning Rate: 0.000050\n",
      "Epoch [27/50], Train Loss: 0.1390, Train Accuracy: 0.9711, Val Loss: 0.1584, Val Accuracy: 0.9582, Learning Rate: 0.000050\n",
      "Epoch [28/50], Train Loss: 0.1317, Train Accuracy: 0.9691, Val Loss: 0.1603, Val Accuracy: 0.9522, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [29/50], Train Loss: 0.1315, Train Accuracy: 0.9781, Val Loss: 0.1381, Val Accuracy: 0.9522, Learning Rate: 0.000050\n",
      "Epoch [30/50], Train Loss: 0.1286, Train Accuracy: 0.9701, Val Loss: 0.1532, Val Accuracy: 0.9542, Learning Rate: 0.000050\n",
      "Epoch [31/50], Train Loss: 0.1264, Train Accuracy: 0.9696, Val Loss: 0.1565, Val Accuracy: 0.9462, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [32/50], Train Loss: 0.1193, Train Accuracy: 0.9746, Val Loss: 0.1361, Val Accuracy: 0.9602, Learning Rate: 0.000050\n",
      "Epoch [33/50], Train Loss: 0.1254, Train Accuracy: 0.9741, Val Loss: 0.1566, Val Accuracy: 0.9482, Learning Rate: 0.000050\n",
      "Epoch [34/50], Train Loss: 0.1092, Train Accuracy: 0.9781, Val Loss: 0.1455, Val Accuracy: 0.9542, Learning Rate: 0.000050\n",
      "Epoch [35/50], Train Loss: 0.1046, Train Accuracy: 0.9806, Val Loss: 0.1452, Val Accuracy: 0.9582, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [36/50], Train Loss: 0.0992, Train Accuracy: 0.9821, Val Loss: 0.1342, Val Accuracy: 0.9542, Learning Rate: 0.000050\n",
      "Epoch [37/50], Train Loss: 0.1028, Train Accuracy: 0.9756, Val Loss: 0.1433, Val Accuracy: 0.9502, Learning Rate: 0.000050\n",
      "Epoch [38/50], Train Loss: 0.0956, Train Accuracy: 0.9816, Val Loss: 0.1399, Val Accuracy: 0.9502, Learning Rate: 0.000050\n",
      "Epoch [39/50], Train Loss: 0.1120, Train Accuracy: 0.9726, Val Loss: 0.1395, Val Accuracy: 0.9582, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [40/50], Train Loss: 0.0849, Train Accuracy: 0.9895, Val Loss: 0.1283, Val Accuracy: 0.9622, Learning Rate: 0.000050\n",
      "Epoch [41/50], Train Loss: 0.0869, Train Accuracy: 0.9856, Val Loss: 0.1460, Val Accuracy: 0.9502, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [42/50], Train Loss: 0.0911, Train Accuracy: 0.9841, Val Loss: 0.1255, Val Accuracy: 0.9582, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [43/50], Train Loss: 0.0777, Train Accuracy: 0.9895, Val Loss: 0.1227, Val Accuracy: 0.9602, Learning Rate: 0.000050\n",
      "Epoch [44/50], Train Loss: 0.0784, Train Accuracy: 0.9856, Val Loss: 0.1586, Val Accuracy: 0.9422, Learning Rate: 0.000050\n",
      "Epoch [45/50], Train Loss: 0.0783, Train Accuracy: 0.9871, Val Loss: 0.1406, Val Accuracy: 0.9522, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [46/50], Train Loss: 0.0820, Train Accuracy: 0.9851, Val Loss: 0.1139, Val Accuracy: 0.9641, Learning Rate: 0.000050\n",
      "Epoch [47/50], Train Loss: 0.0801, Train Accuracy: 0.9875, Val Loss: 0.1159, Val Accuracy: 0.9602, Learning Rate: 0.000050\n",
      "Epoch [48/50], Train Loss: 0.0699, Train Accuracy: 0.9890, Val Loss: 0.1175, Val Accuracy: 0.9622, Learning Rate: 0.000050\n",
      "Epoch [49/50], Train Loss: 0.0785, Train Accuracy: 0.9831, Val Loss: 0.1295, Val Accuracy: 0.9542, Learning Rate: 0.000050\n",
      "Epoch [50/50], Train Loss: 0.0680, Train Accuracy: 0.9885, Val Loss: 0.1147, Val Accuracy: 0.9661, Learning Rate: 0.000025\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "# 조기 종료 변수 초기화\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # 개선이 없을 때 기다릴 에포크 수\n",
    "patience_counter = 0\n",
    "\n",
    "# 학습률 스케줄러\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(DEVICE), targets.to(DEVICE).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = criterion(outputs.view(-1), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 정확도 계산\n",
    "        predicted = (torch.sigmoid(outputs.view(-1)) > 0.5).float()\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "        total_predictions += targets.size(0)\n",
    "\n",
    "    # 훈련 데이터 정확도\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # 검증\n",
    "    val_loss, val_accuracy = evaluate_model(model, validation_loader, criterion)\n",
    "\n",
    "    # 학습률 스케줄러 적용\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # 현재 학습률 출력\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    # 조기 종료 로직\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # 손실 개선 시 카운터 리셋\n",
    "        print(\"Validation loss improved, saving model...\")\n",
    "        # 모델 저장\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, '../model/efficientnet_b1_ft_best_model_05.1.pth')\n",
    "    else:\n",
    "        patience_counter += 1  # 손실이 개선되지 않으면 카운터 증가\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break  # 훈련 종료\n",
    "\n",
    "    # 에포크별 정보 출력 (학습률 포함)\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {running_loss / len(train_loader):.4f}, '\n",
    "          f'Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, '\n",
    "          f'Learning Rate: {current_lr:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\AppData\\Local\\Temp\\ipykernel_16704\\2454304535.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('../model/efficientnet_b1_ft_best_model_05.1.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (base_model): EfficientNet(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.10434782608695654, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.11304347826086956, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.12173913043478261, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1391304347826087, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.14782608695652175, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1565217391304348, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.16521739130434784, mode=row)\n",
       "        )\n",
       "        (4): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1920, 1920, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1920, bias=False)\n",
       "              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1920, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(80, 1920, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1920, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.19130434782608696, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (8): Conv2dNormActivation(\n",
       "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Identity()\n",
       "  )\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=1280, out_features=50, bias=True)\n",
       "  (bn1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomModel('efficientnet').to(DEVICE)\n",
    "\n",
    "checkpoint = torch.load('../model/efficientnet_b1_ft_best_model_05.1.pth')\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 마지막 몇 개의 레이어를 동결 해제 (features 블록 5 이후 층)\n",
    "for name, param in model.base_model.features[5:].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 동결 해제된 일부 층과 분류기 층 학습\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.1147, Test accuracy: 0.9618\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "efficientNet b0랑 똑같이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model_name='efficientnet'):\n",
    "        super(CustomModel, self).__init__()\n",
    "        if model_name == 'efficientnet':\n",
    "            self.base_model = torchvision.models.efficientnet_b1(weights='IMAGENET1K_V1')\n",
    "            self.base_model.classifier = nn.Identity()  # 마지막 분류기 제거\n",
    "        elif model_name == 'resnet50':\n",
    "            self.base_model = torchvision.models.resnet50(weights='IMAGENET1K_V1')\n",
    "            self.base_model = nn.Sequential(*list(self.base_model.children())[:-1])  # 마지막 레이어 제거\n",
    "        elif model_name == 'inception':\n",
    "            self.base_model = torchvision.models.inception_v3(weights='IMAGENET1K_V1')\n",
    "            self.base_model.classifier = nn.Identity()  # 마지막 분류기 제거\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.5)  # 첫 번째 드롭아웃\n",
    "        self.fc1 = nn.Linear(self._get_features_dim(model_name), 50)\n",
    "        self.bn1 = nn.BatchNorm1d(50)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.5)  # 두 번째 드롭아웃\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "\n",
    "    def _get_features_dim(self, model_name):\n",
    "        if model_name == 'efficientnet':\n",
    "            return 1280  # EfficientNet B0의 출력 차원\n",
    "        elif model_name == 'resnet50':\n",
    "            return 2048  # ResNet50의 출력 차원\n",
    "        elif model_name == 'inception':\n",
    "            return 2048  # Inception의 출력 차원\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "\n",
    "        if isinstance(x, tuple):  \n",
    "            x = x[0]\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x) \n",
    "        x = self.bn1(x)  # 배치 정규화 적용\n",
    "        x = self.dropout2(x) \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 데이터 증강 정의\n",
    "aug = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),                # 좌우 반전\n",
    "    A.VerticalFlip(p=0.5),                  # 상하 반전\n",
    "    A.Rotate(limit=10, p=0.5),              # 작은 각도 회전 (10도 내외)\n",
    "    A.RandomBrightnessContrast(p=0.5),      # 밝기 및 대비 조절\n",
    "])\n",
    "\n",
    "# 데이터셋 인스턴스 생성\n",
    "train_dataset = CustomDataset(train_df['file_paths'].values, train_df['targets'].values, aug=aug)\n",
    "validation_dataset = CustomDataset(validation_df['file_paths'].values, validation_df['targets'].values)\n",
    "test_dataset = CustomDataset(test_df['file_paths'].values, test_df['targets'].values)\n",
    "\n",
    "# DataLoader 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: base_model.features.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.2.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.2.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.2.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.2.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.2.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.2.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.8.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.8.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.8.1.bias | Requires Grad: True\n",
      "Layer: fc1.weight | Requires Grad: True\n",
      "Layer: fc1.bias | Requires Grad: True\n",
      "Layer: bn1.weight | Requires Grad: True\n",
      "Layer: bn1.bias | Requires Grad: True\n",
      "Layer: fc2.weight | Requires Grad: True\n",
      "Layer: fc2.bias | Requires Grad: True\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "model = CustomModel(model_name='efficientnet').to(DEVICE)\n",
    "\n",
    "# 1. 사전 학습된 base_model의 파라미터를 동결 (fine-tuning 초기 단계)\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 마지막 몇 개의 레이어를 동결 해제 (features 블록 5 이후 층)\n",
    "for name, param in model.base_model.features[5:].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 동결 해제된 일부 층과 분류기 층 학습\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
    "\n",
    "# 학습할 손실 함수\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 각 층의 freeze/unfreeze 상태 확인\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Requires Grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved, saving model...\n",
      "Epoch [1/50], Train Loss: 0.6720, Train Accuracy: 0.5881, Val Loss: 0.5291, Val Accuracy: 0.7809, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [2/50], Train Loss: 0.5463, Train Accuracy: 0.7186, Val Loss: 0.4588, Val Accuracy: 0.8147, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [3/50], Train Loss: 0.4865, Train Accuracy: 0.7629, Val Loss: 0.4293, Val Accuracy: 0.8347, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [4/50], Train Loss: 0.4675, Train Accuracy: 0.7799, Val Loss: 0.4020, Val Accuracy: 0.8367, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [5/50], Train Loss: 0.4289, Train Accuracy: 0.8093, Val Loss: 0.3772, Val Accuracy: 0.8546, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [6/50], Train Loss: 0.4148, Train Accuracy: 0.8252, Val Loss: 0.3625, Val Accuracy: 0.8606, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [7/50], Train Loss: 0.4004, Train Accuracy: 0.8342, Val Loss: 0.3468, Val Accuracy: 0.8745, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [8/50], Train Loss: 0.3755, Train Accuracy: 0.8596, Val Loss: 0.3392, Val Accuracy: 0.8745, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [9/50], Train Loss: 0.3600, Train Accuracy: 0.8591, Val Loss: 0.3297, Val Accuracy: 0.8865, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [10/50], Train Loss: 0.3444, Train Accuracy: 0.8745, Val Loss: 0.3028, Val Accuracy: 0.9004, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [11/50], Train Loss: 0.3302, Train Accuracy: 0.8879, Val Loss: 0.2829, Val Accuracy: 0.9064, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [12/50], Train Loss: 0.3131, Train Accuracy: 0.8909, Val Loss: 0.2775, Val Accuracy: 0.9044, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [13/50], Train Loss: 0.2814, Train Accuracy: 0.9128, Val Loss: 0.2755, Val Accuracy: 0.9004, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [14/50], Train Loss: 0.2795, Train Accuracy: 0.9084, Val Loss: 0.2529, Val Accuracy: 0.9203, Learning Rate: 0.000050\n",
      "Epoch [15/50], Train Loss: 0.2592, Train Accuracy: 0.9243, Val Loss: 0.2613, Val Accuracy: 0.9223, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [16/50], Train Loss: 0.2388, Train Accuracy: 0.9353, Val Loss: 0.2489, Val Accuracy: 0.9183, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [17/50], Train Loss: 0.2341, Train Accuracy: 0.9308, Val Loss: 0.2215, Val Accuracy: 0.9343, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [18/50], Train Loss: 0.2150, Train Accuracy: 0.9522, Val Loss: 0.2147, Val Accuracy: 0.9343, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [19/50], Train Loss: 0.2097, Train Accuracy: 0.9452, Val Loss: 0.1987, Val Accuracy: 0.9462, Learning Rate: 0.000050\n",
      "Epoch [20/50], Train Loss: 0.2094, Train Accuracy: 0.9477, Val Loss: 0.2056, Val Accuracy: 0.9382, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [21/50], Train Loss: 0.1881, Train Accuracy: 0.9552, Val Loss: 0.1886, Val Accuracy: 0.9442, Learning Rate: 0.000050\n",
      "Epoch [22/50], Train Loss: 0.1919, Train Accuracy: 0.9507, Val Loss: 0.1941, Val Accuracy: 0.9382, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [23/50], Train Loss: 0.1815, Train Accuracy: 0.9562, Val Loss: 0.1834, Val Accuracy: 0.9422, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [24/50], Train Loss: 0.1696, Train Accuracy: 0.9617, Val Loss: 0.1706, Val Accuracy: 0.9562, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [25/50], Train Loss: 0.1664, Train Accuracy: 0.9602, Val Loss: 0.1630, Val Accuracy: 0.9641, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [26/50], Train Loss: 0.1692, Train Accuracy: 0.9622, Val Loss: 0.1621, Val Accuracy: 0.9562, Learning Rate: 0.000050\n",
      "Epoch [27/50], Train Loss: 0.1411, Train Accuracy: 0.9721, Val Loss: 0.1658, Val Accuracy: 0.9522, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [28/50], Train Loss: 0.1565, Train Accuracy: 0.9646, Val Loss: 0.1605, Val Accuracy: 0.9542, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [29/50], Train Loss: 0.1425, Train Accuracy: 0.9696, Val Loss: 0.1512, Val Accuracy: 0.9641, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [30/50], Train Loss: 0.1340, Train Accuracy: 0.9741, Val Loss: 0.1490, Val Accuracy: 0.9641, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [31/50], Train Loss: 0.1298, Train Accuracy: 0.9721, Val Loss: 0.1465, Val Accuracy: 0.9582, Learning Rate: 0.000050\n",
      "Epoch [32/50], Train Loss: 0.1230, Train Accuracy: 0.9761, Val Loss: 0.1512, Val Accuracy: 0.9562, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [33/50], Train Loss: 0.1215, Train Accuracy: 0.9756, Val Loss: 0.1450, Val Accuracy: 0.9582, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [34/50], Train Loss: 0.1111, Train Accuracy: 0.9821, Val Loss: 0.1399, Val Accuracy: 0.9622, Learning Rate: 0.000050\n",
      "Epoch [35/50], Train Loss: 0.1152, Train Accuracy: 0.9791, Val Loss: 0.1418, Val Accuracy: 0.9522, Learning Rate: 0.000050\n",
      "Epoch [36/50], Train Loss: 0.1006, Train Accuracy: 0.9821, Val Loss: 0.1403, Val Accuracy: 0.9582, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [37/50], Train Loss: 0.1070, Train Accuracy: 0.9796, Val Loss: 0.1361, Val Accuracy: 0.9562, Learning Rate: 0.000050\n",
      "Epoch [38/50], Train Loss: 0.1012, Train Accuracy: 0.9816, Val Loss: 0.1361, Val Accuracy: 0.9602, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [39/50], Train Loss: 0.0947, Train Accuracy: 0.9875, Val Loss: 0.1247, Val Accuracy: 0.9602, Learning Rate: 0.000050\n",
      "Epoch [40/50], Train Loss: 0.0918, Train Accuracy: 0.9871, Val Loss: 0.1249, Val Accuracy: 0.9641, Learning Rate: 0.000050\n",
      "Epoch [41/50], Train Loss: 0.0995, Train Accuracy: 0.9816, Val Loss: 0.1255, Val Accuracy: 0.9562, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [42/50], Train Loss: 0.0817, Train Accuracy: 0.9866, Val Loss: 0.1207, Val Accuracy: 0.9661, Learning Rate: 0.000050\n",
      "Epoch [43/50], Train Loss: 0.0865, Train Accuracy: 0.9846, Val Loss: 0.1317, Val Accuracy: 0.9661, Learning Rate: 0.000050\n",
      "Epoch [44/50], Train Loss: 0.0856, Train Accuracy: 0.9871, Val Loss: 0.1312, Val Accuracy: 0.9602, Learning Rate: 0.000050\n",
      "Epoch [45/50], Train Loss: 0.0767, Train Accuracy: 0.9890, Val Loss: 0.1207, Val Accuracy: 0.9622, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [46/50], Train Loss: 0.0870, Train Accuracy: 0.9811, Val Loss: 0.1192, Val Accuracy: 0.9622, Learning Rate: 0.000050\n",
      "Epoch [47/50], Train Loss: 0.0772, Train Accuracy: 0.9875, Val Loss: 0.1256, Val Accuracy: 0.9562, Learning Rate: 0.000050\n",
      "Epoch [48/50], Train Loss: 0.0819, Train Accuracy: 0.9831, Val Loss: 0.1216, Val Accuracy: 0.9602, Learning Rate: 0.000050\n",
      "Epoch [49/50], Train Loss: 0.0830, Train Accuracy: 0.9831, Val Loss: 0.1333, Val Accuracy: 0.9502, Learning Rate: 0.000050\n",
      "Epoch [50/50], Train Loss: 0.0700, Train Accuracy: 0.9895, Val Loss: 0.1277, Val Accuracy: 0.9562, Learning Rate: 0.000025\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "# 조기 종료 변수 초기화\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # 개선이 없을 때 기다릴 에포크 수\n",
    "patience_counter = 0\n",
    "\n",
    "# 학습률 스케줄러\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(DEVICE), targets.to(DEVICE).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = criterion(outputs.view(-1), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 정확도 계산\n",
    "        predicted = (torch.sigmoid(outputs.view(-1)) > 0.5).float()\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "        total_predictions += targets.size(0)\n",
    "\n",
    "    # 훈련 데이터 정확도\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # 검증\n",
    "    val_loss, val_accuracy = evaluate_model(model, validation_loader, criterion)\n",
    "\n",
    "    # 학습률 스케줄러 적용\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # 현재 학습률 출력\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    # 조기 종료 로직\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # 손실 개선 시 카운터 리셋\n",
    "        print(\"Validation loss improved, saving model...\")\n",
    "        # 모델 저장\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, '../model/efficientnet_b1_ft_best_model_06.pth')\n",
    "    else:\n",
    "        patience_counter += 1  # 손실이 개선되지 않으면 카운터 증가\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break  # 훈련 종료\n",
    "\n",
    "    # 에포크별 정보 출력 (학습률 포함)\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {running_loss / len(train_loader):.4f}, '\n",
    "          f'Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, '\n",
    "          f'Learning Rate: {current_lr:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\AppData\\Local\\Temp\\ipykernel_1892\\3854422640.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('../model/efficientnet_b1_ft_best_model_06.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (base_model): EfficientNet(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.10434782608695654, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.11304347826086956, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.12173913043478261, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1391304347826087, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.14782608695652175, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1565217391304348, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.16521739130434784, mode=row)\n",
       "        )\n",
       "        (4): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1920, 1920, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1920, bias=False)\n",
       "              (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1920, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(80, 1920, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1920, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.19130434782608696, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (8): Conv2dNormActivation(\n",
       "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Identity()\n",
       "  )\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=1280, out_features=50, bias=True)\n",
       "  (bn1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomModel('efficientnet').to(DEVICE)\n",
    "\n",
    "checkpoint = torch.load('../model/efficientnet_b1_ft_best_model_06.pth')\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 마지막 몇 개의 레이어를 동결 해제 (features 블록 5 이후 층)\n",
    "for name, param in model.base_model.features[5:].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 동결 해제된 일부 층과 분류기 층 학습\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.1270, Test accuracy: 0.9586\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
