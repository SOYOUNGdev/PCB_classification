{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "CUDA available: True\n",
      "CUDA version in PyTorch: 11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# GPU 사용 설정\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "# CUDA 사용 가능 여부 확인\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")  # True여야 함\n",
    "\n",
    "# PyTorch에서 사용하는 CUDA 버전 확인\n",
    "print(f\"CUDA version in PyTorch: {torch.version.cuda}\")\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\isy\\test\\Lib\\site-packages\\albumentations\\__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.18 (you have 1.4.17). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 임포트\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NG 예측결과:\n",
      "Efficientnet: OK (정상), 확률: 0.4919\n",
      "Efficientnet: NG (결함 있음), 확률: 0.5310\n",
      "Efficientnet: NG (결함 있음), 확률: 0.5072\n",
      "Efficientnet: OK (정상), 확률: 0.5000\n",
      "Efficientnet: NG (결함 있음), 확률: 0.5188\n",
      "Efficientnet: OK (정상), 확률: 0.4762\n",
      "Efficientnet: NG (결함 있음), 확률: 0.5247\n",
      "Efficientnet: NG (결함 있음), 확률: 0.5062\n",
      "Efficientnet: NG (결함 있음), 확률: 0.5311\n",
      "Efficientnet: OK (정상), 확률: 0.4719\n",
      "OK 예측결과:\n",
      "Efficientnet: NG (결함 있음), 확률: 0.6040\n",
      "Efficientnet: NG (결함 있음), 확률: 0.5677\n",
      "Efficientnet: NG (결함 있음), 확률: 0.5071\n",
      "Efficientnet: OK (정상), 확률: 0.4711\n",
      "Efficientnet: NG (결함 있음), 확률: 0.5034\n",
      "Efficientnet: OK (정상), 확률: 0.4977\n",
      "Efficientnet: OK (정상), 확률: 0.4961\n",
      "Efficientnet: NG (결함 있음), 확률: 0.5232\n",
      "Efficientnet: NG (결함 있음), 확률: 0.5397\n",
      "Efficientnet: NG (결함 있음), 확률: 0.5410\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# ResNet 및 Inception 모델 수정\n",
    "class CustomModel(torch.nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(CustomModel, self).__init__()\n",
    "        if model_name == 'efficientnet':\n",
    "            self.model = models.efficientnet_b2(weights='IMAGENET1K_V1') \n",
    "            self.model.classifier[1] = torch.nn.Linear(self.model.classifier[1].in_features, 1)  # 이진 분류\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# efficientnet 모델 로드\n",
    "efficientnet_model = CustomModel(model_name='efficientnet')\n",
    "efficientnet_model.eval()  # 평가 모드로 설정\n",
    "\n",
    "# 전처리 파이프라인 정의\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # OpenCV 이미지를 PIL 이미지로 변환\n",
    "    transforms.Resize((260, 260)),  # 모델 입력 크기에 맞게 크기 조정\n",
    "    transforms.ToTensor(),  # 텐서로 변환\n",
    "])\n",
    "\n",
    "# 이미지 전처리 함수\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # RGB로 변환\n",
    "    image = preprocess(image)  # 전처리 적용\n",
    "    image = image.unsqueeze(0)  # 배치 차원 추가\n",
    "    return image\n",
    "\n",
    "# 예측 함수 정의\n",
    "def predict_image(model, image_path, device):\n",
    "    model.to(DEVICE)  # 모델을 디바이스로 이동\n",
    "    image = preprocess_image(image_path).to(device)  # 전처리 후 디바이스로 이동\n",
    "    with torch.no_grad():  # 평가 모드에서는 gradient 계산 비활성화\n",
    "        output = model(image)\n",
    "        predicted_prob = torch.sigmoid(output).item()  # 확률로 변환\n",
    "\n",
    "    # 0.5를 기준으로 정상/비정상 예측\n",
    "    prediction = 'NG (결함 있음)' if predicted_prob > 0.5 else 'OK (정상)'\n",
    "    return prediction, predicted_prob\n",
    "\n",
    "# NG 폴더 경로\n",
    "ng_folder_path = '../PCB_imgs/all/resize/NG/' \n",
    "# OK 폴더 경로\n",
    "ok_folder_path = '../PCB_imgs/all/resize/OK/' \n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # 디바이스 설정\n",
    "\n",
    "# 각 폴더에서 무작위로 10개의 이미지를 가져오기\n",
    "def load_random_images(folder_path, num_images=10):\n",
    "    images = os.listdir(folder_path)\n",
    "    random_images = random.sample(images, min(num_images, len(images)))\n",
    "    return [os.path.join(folder_path, img) for img in random_images]\n",
    "\n",
    "# NG 및 OK 이미지 로드\n",
    "ng_images = load_random_images(ng_folder_path)\n",
    "ok_images = load_random_images(ok_folder_path)\n",
    "\n",
    "# NG 이미지 예측 결과와 확률\n",
    "print(f'NG 예측결과:')\n",
    "for image_path in ng_images:\n",
    "    prediction, prob = predict_image(efficientnet_model, image_path, DEVICE) \n",
    "    print(f\"Efficientnet: {prediction}, 확률: {prob:.4f}\")\n",
    "\n",
    "# OK 이미지 예측 결과와 확률\n",
    "print(f'OK 예측결과:')\n",
    "for image_path in ok_images:\n",
    "    prediction, prob = predict_image(efficientnet_model, image_path, DEVICE) \n",
    "    print(f\"Efficientnet: {prediction}, 확률: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 데이터 수: 2008\n",
      "Validation 데이터 수: 502\n",
      "Test 데이터 수: 628\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "import pandas as pd\n",
    "\n",
    "IMAGE_SIZE = 260\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "# 이미지 변환 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),  # 이미지를 Tensor로 변환 (0~255 범위를 0~1 범위로 정규화)\n",
    "])\n",
    "\n",
    "# 데이터셋 디렉토리 설정\n",
    "train_dir = '../PCB_imgs/all/resize/train'\n",
    "val_dir = '../PCB_imgs/all/resize/validation'\n",
    "test_dir = '../PCB_imgs/all/resize/test'\n",
    "\n",
    "# ImageFolder로 데이터셋 불러오기\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "# 파일 경로 및 타겟 추출\n",
    "train_file_paths = [img[0] for img in train_dataset.imgs]\n",
    "train_targets = train_dataset.targets\n",
    "\n",
    "val_file_paths = [img[0] for img in val_dataset.imgs]\n",
    "val_targets = val_dataset.targets\n",
    "\n",
    "test_file_paths = [img[0] for img in test_dataset.imgs]\n",
    "test_targets = test_dataset.targets\n",
    "\n",
    "# DataFrame 생성\n",
    "train_df = pd.DataFrame({'file_paths': train_file_paths, 'targets': train_targets})\n",
    "validation_df = pd.DataFrame({'file_paths': val_file_paths, 'targets': val_targets})\n",
    "test_df = pd.DataFrame({'file_paths': test_file_paths, 'targets': test_targets})\n",
    "\n",
    "# 확인을 위해 각 데이터셋의 크기 출력\n",
    "print(f\"Train 데이터 수: {len(train_df)}\")\n",
    "print(f\"Validation 데이터 수: {len(validation_df)}\")\n",
    "print(f\"Test 데이터 수: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 260, 260])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# resize된 데이터셋 로드\n",
    "input_dir = '../PCB_imgs/all/resize/'\n",
    "dataset = datasets.ImageFolder(root=input_dir, transform=transform)\n",
    "\n",
    "# 데이터 로더 정의\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 데이터 로더에서 데이터 가져오기\n",
    "for images, labels in dataloader:\n",
    "    print(images.shape)  # 배치의 이미지 텐서 크기 확인\n",
    "    break  # 첫 번째 배치만 확인\n",
    "    # 배치 크기, 채널 수(RGB), 이미지 크기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커스텀 데이터세트 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_paths, targets, aug=None, preprocess=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.targets = targets\n",
    "        self.aug = aug\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_path = self.file_paths[index]\n",
    "        target = self.targets[index]\n",
    "        \n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "\n",
    "        if self.aug is not None:\n",
    "            image = self.aug(image=image)['image']\n",
    "\n",
    "        if self.preprocess is not None:\n",
    "            image = self.preprocess(image)\n",
    "\n",
    "        image = np.transpose(image, (2, 0, 1))  # (H, W, C) -> (C, H, W)\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 데이터셋 인스턴스 생성\n",
    "train_dataset = CustomDataset(train_df['file_paths'].values, train_df['targets'].values)\n",
    "validation_dataset = CustomDataset(validation_df['file_paths'].values, validation_df['targets'].values)\n",
    "test_dataset = CustomDataset(test_df['file_paths'].values, test_df['targets'].values)\n",
    "\n",
    "# DataLoader 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 함수 정의\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device).float()\n",
    "            outputs = model(images)\n",
    "            outputs = outputs.view(-1) # torch.Size([batch_size])로 변환\n",
    "            loss = criterion(outputs, labels)  # 손실 계산\n",
    "            running_test_loss += loss.item()\n",
    "            \n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()  # 0.5 기준으로 이진 분류\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "            \n",
    "    # 검증 손실과 정확도\n",
    "    test_loss = running_test_loss / len(test_loader)\n",
    "    test_accuracy = correct_test / total_test\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "efficientnet_b2 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model_name='efficientnet'):\n",
    "        super(CustomModel, self).__init__()\n",
    "        if model_name == 'efficientnet':\n",
    "            self.base_model = torchvision.models.efficientnet_b2(weights='IMAGENET1K_V1')\n",
    "            self.base_model.classifier = nn.Identity()  # 마지막 분류기 제거\n",
    "        elif model_name == 'resnet50':\n",
    "            self.base_model = torchvision.models.resnet50(weights='IMAGENET1K_V1')\n",
    "            self.base_model = nn.Sequential(*list(self.base_model.children())[:-1])  # 마지막 레이어 제거\n",
    "        elif model_name == 'inception':\n",
    "            self.base_model = torchvision.models.inception_v3(weights='IMAGENET1K_V1')\n",
    "            self.base_model.classifier = nn.Identity()  # 마지막 분류기 제거\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.5)  # 첫 번째 드롭아웃\n",
    "        self.fc1 = nn.Linear(self._get_features_dim(model_name), 50)\n",
    "        self.bn1 = nn.BatchNorm1d(50)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.5)  # 두 번째 드롭아웃\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "\n",
    "    def _get_features_dim(self, model_name):\n",
    "        if model_name == 'efficientnet':\n",
    "            return 1408  # EfficientNet B2의 출력 차원\n",
    "        elif model_name == 'resnet50':\n",
    "            return 2048  # ResNet50의 출력 차원\n",
    "        elif model_name == 'inception':\n",
    "            return 2048  # Inception의 출력 차원\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        if isinstance(x, tuple):  \n",
    "            x = x[0]\n",
    "        x = x.view(x.size(0), -1)  \n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x) \n",
    "        x = self.bn1(x) \n",
    "        x = self.dropout2(x) \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 데이터 증강 정의\n",
    "aug = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),                # 좌우 반전\n",
    "    A.VerticalFlip(p=0.5),                  # 상하 반전\n",
    "    A.Rotate(limit=10, p=0.5),              # 작은 각도 회전 (10도 내외)\n",
    "    A.RandomBrightnessContrast(p=0.5),      # 밝기 및 대비 조절\n",
    "])\n",
    "\n",
    "# 데이터셋 인스턴스 생성\n",
    "train_dataset = CustomDataset(train_df['file_paths'].values, train_df['targets'].values, aug=aug)\n",
    "validation_dataset = CustomDataset(validation_df['file_paths'].values, validation_df['targets'].values)\n",
    "test_dataset = CustomDataset(test_df['file_paths'].values, test_df['targets'].values)\n",
    "\n",
    "# DataLoader 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: base_model.features.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.2.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.2.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.2.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.2.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.2.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.2.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.0.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.1.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.2.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.5.3.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.0.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.1.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.2.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.3.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.6.4.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.0.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.0.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.0.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.0.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.1.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.1.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.1.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.2.fc1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.2.fc1.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.2.fc2.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.2.fc2.bias | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.3.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.3.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.7.1.block.3.1.bias | Requires Grad: True\n",
      "Layer: base_model.features.8.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.8.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.8.1.bias | Requires Grad: True\n",
      "Layer: fc1.weight | Requires Grad: True\n",
      "Layer: fc1.bias | Requires Grad: True\n",
      "Layer: bn1.weight | Requires Grad: True\n",
      "Layer: bn1.bias | Requires Grad: True\n",
      "Layer: fc2.weight | Requires Grad: True\n",
      "Layer: fc2.bias | Requires Grad: True\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "model = CustomModel(model_name='efficientnet').to(DEVICE)\n",
    "\n",
    "# 1. 사전 학습된 base_model의 파라미터를 동결 (fine-tuning 초기 단계)\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 마지막 몇 개의 레이어를 동결 해제 (features 블록 5 이후 층)\n",
    "for name, param in model.base_model.features[5:].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 동결 해제된 일부 층과 분류기 층 학습\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
    "\n",
    "# 학습할 손실 함수\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 각 층의 freeze/unfreeze 상태 확인\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Requires Grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved, saving model...\n",
      "Epoch [1/50], Train Loss: 0.6642, Train Accuracy: 0.5901, Val Loss: 0.4841, Val Accuracy: 0.8108, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [2/50], Train Loss: 0.5080, Train Accuracy: 0.7560, Val Loss: 0.4435, Val Accuracy: 0.7988, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [3/50], Train Loss: 0.4769, Train Accuracy: 0.7744, Val Loss: 0.3968, Val Accuracy: 0.8406, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [4/50], Train Loss: 0.4317, Train Accuracy: 0.8142, Val Loss: 0.3786, Val Accuracy: 0.8466, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [5/50], Train Loss: 0.4008, Train Accuracy: 0.8302, Val Loss: 0.3494, Val Accuracy: 0.8805, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [6/50], Train Loss: 0.3789, Train Accuracy: 0.8461, Val Loss: 0.3272, Val Accuracy: 0.8865, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [7/50], Train Loss: 0.3590, Train Accuracy: 0.8581, Val Loss: 0.3067, Val Accuracy: 0.9064, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [8/50], Train Loss: 0.3388, Train Accuracy: 0.8760, Val Loss: 0.2929, Val Accuracy: 0.9143, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [9/50], Train Loss: 0.2983, Train Accuracy: 0.8994, Val Loss: 0.2773, Val Accuracy: 0.9064, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [10/50], Train Loss: 0.2786, Train Accuracy: 0.9079, Val Loss: 0.2723, Val Accuracy: 0.9024, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [11/50], Train Loss: 0.2638, Train Accuracy: 0.9288, Val Loss: 0.2443, Val Accuracy: 0.9303, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [12/50], Train Loss: 0.2587, Train Accuracy: 0.9288, Val Loss: 0.2372, Val Accuracy: 0.9343, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [13/50], Train Loss: 0.2333, Train Accuracy: 0.9377, Val Loss: 0.2117, Val Accuracy: 0.9382, Learning Rate: 0.000050\n",
      "Epoch [14/50], Train Loss: 0.2234, Train Accuracy: 0.9432, Val Loss: 0.2188, Val Accuracy: 0.9283, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [15/50], Train Loss: 0.2179, Train Accuracy: 0.9467, Val Loss: 0.1961, Val Accuracy: 0.9462, Learning Rate: 0.000050\n",
      "Epoch [16/50], Train Loss: 0.1891, Train Accuracy: 0.9577, Val Loss: 0.1989, Val Accuracy: 0.9402, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [17/50], Train Loss: 0.1816, Train Accuracy: 0.9631, Val Loss: 0.1954, Val Accuracy: 0.9422, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [18/50], Train Loss: 0.1742, Train Accuracy: 0.9676, Val Loss: 0.1847, Val Accuracy: 0.9462, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [19/50], Train Loss: 0.1807, Train Accuracy: 0.9542, Val Loss: 0.1756, Val Accuracy: 0.9542, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [20/50], Train Loss: 0.1606, Train Accuracy: 0.9676, Val Loss: 0.1520, Val Accuracy: 0.9542, Learning Rate: 0.000050\n",
      "Epoch [21/50], Train Loss: 0.1554, Train Accuracy: 0.9676, Val Loss: 0.1535, Val Accuracy: 0.9602, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [22/50], Train Loss: 0.1500, Train Accuracy: 0.9741, Val Loss: 0.1485, Val Accuracy: 0.9562, Learning Rate: 0.000050\n",
      "Epoch [23/50], Train Loss: 0.1429, Train Accuracy: 0.9711, Val Loss: 0.1562, Val Accuracy: 0.9562, Learning Rate: 0.000050\n",
      "Epoch [24/50], Train Loss: 0.1446, Train Accuracy: 0.9671, Val Loss: 0.1487, Val Accuracy: 0.9622, Learning Rate: 0.000050\n",
      "Epoch [25/50], Train Loss: 0.1249, Train Accuracy: 0.9806, Val Loss: 0.1719, Val Accuracy: 0.9442, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [26/50], Train Loss: 0.1235, Train Accuracy: 0.9791, Val Loss: 0.1410, Val Accuracy: 0.9622, Learning Rate: 0.000050\n",
      "Epoch [27/50], Train Loss: 0.1131, Train Accuracy: 0.9831, Val Loss: 0.1458, Val Accuracy: 0.9582, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [28/50], Train Loss: 0.1114, Train Accuracy: 0.9846, Val Loss: 0.1393, Val Accuracy: 0.9661, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [29/50], Train Loss: 0.1116, Train Accuracy: 0.9806, Val Loss: 0.1307, Val Accuracy: 0.9661, Learning Rate: 0.000050\n",
      "Epoch [30/50], Train Loss: 0.1192, Train Accuracy: 0.9776, Val Loss: 0.1467, Val Accuracy: 0.9562, Learning Rate: 0.000050\n",
      "Epoch [31/50], Train Loss: 0.1031, Train Accuracy: 0.9856, Val Loss: 0.1374, Val Accuracy: 0.9622, Learning Rate: 0.000050\n",
      "Epoch [32/50], Train Loss: 0.1001, Train Accuracy: 0.9856, Val Loss: 0.1443, Val Accuracy: 0.9562, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [33/50], Train Loss: 0.0976, Train Accuracy: 0.9856, Val Loss: 0.1232, Val Accuracy: 0.9562, Learning Rate: 0.000050\n",
      "Epoch [34/50], Train Loss: 0.0913, Train Accuracy: 0.9871, Val Loss: 0.1320, Val Accuracy: 0.9582, Learning Rate: 0.000050\n",
      "Epoch [35/50], Train Loss: 0.0810, Train Accuracy: 0.9905, Val Loss: 0.1375, Val Accuracy: 0.9562, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [36/50], Train Loss: 0.0878, Train Accuracy: 0.9885, Val Loss: 0.1200, Val Accuracy: 0.9701, Learning Rate: 0.000050\n",
      "Epoch [37/50], Train Loss: 0.1001, Train Accuracy: 0.9816, Val Loss: 0.1284, Val Accuracy: 0.9582, Learning Rate: 0.000050\n",
      "Epoch [38/50], Train Loss: 0.0874, Train Accuracy: 0.9866, Val Loss: 0.1226, Val Accuracy: 0.9681, Learning Rate: 0.000050\n",
      "Epoch [39/50], Train Loss: 0.0855, Train Accuracy: 0.9866, Val Loss: 0.1297, Val Accuracy: 0.9602, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [40/50], Train Loss: 0.0879, Train Accuracy: 0.9816, Val Loss: 0.1178, Val Accuracy: 0.9681, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [41/50], Train Loss: 0.0738, Train Accuracy: 0.9905, Val Loss: 0.1147, Val Accuracy: 0.9641, Learning Rate: 0.000050\n",
      "Epoch [42/50], Train Loss: 0.0763, Train Accuracy: 0.9895, Val Loss: 0.1244, Val Accuracy: 0.9641, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [43/50], Train Loss: 0.0720, Train Accuracy: 0.9890, Val Loss: 0.1095, Val Accuracy: 0.9721, Learning Rate: 0.000050\n",
      "Epoch [44/50], Train Loss: 0.0686, Train Accuracy: 0.9910, Val Loss: 0.1174, Val Accuracy: 0.9641, Learning Rate: 0.000050\n",
      "Epoch [45/50], Train Loss: 0.0703, Train Accuracy: 0.9890, Val Loss: 0.1189, Val Accuracy: 0.9661, Learning Rate: 0.000050\n",
      "Epoch [46/50], Train Loss: 0.0654, Train Accuracy: 0.9915, Val Loss: 0.1115, Val Accuracy: 0.9701, Learning Rate: 0.000050\n",
      "Epoch [47/50], Train Loss: 0.0721, Train Accuracy: 0.9866, Val Loss: 0.1211, Val Accuracy: 0.9661, Learning Rate: 0.000025\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "# 조기 종료 변수 초기화\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # 개선이 없을 때 기다릴 에포크 수\n",
    "patience_counter = 0\n",
    "\n",
    "# 학습률 스케줄러\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(DEVICE), targets.to(DEVICE).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = criterion(outputs.view(-1), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 정확도 계산\n",
    "        predicted = (torch.sigmoid(outputs.view(-1)) > 0.5).float()\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "        total_predictions += targets.size(0)\n",
    "\n",
    "    # 훈련 데이터 정확도\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # 검증\n",
    "    val_loss, val_accuracy = evaluate_model(model, validation_loader, criterion)\n",
    "\n",
    "    # 학습률 스케줄러 적용\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # 현재 학습률 출력\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    # 조기 종료 로직\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # 손실 개선 시 카운터 리셋\n",
    "        print(\"Validation loss improved, saving model...\")\n",
    "        # 모델 저장\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, '../model/efficientnet_b2_ft_best_model_01.pth')\n",
    "    else:\n",
    "        patience_counter += 1  # 손실이 개선되지 않으면 카운터 증가\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break  # 훈련 종료\n",
    "\n",
    "    # 에포크별 정보 출력 (학습률 포함)\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {running_loss / len(train_loader):.4f}, '\n",
    "          f'Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, '\n",
    "          f'Learning Rate: {current_lr:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\AppData\\Local\\Temp\\ipykernel_4440\\457214973.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('../model/efficientnet_b2_ft_best_model_01.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (base_model): EfficientNet(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
       "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
       "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)\n",
       "              (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)\n",
       "              (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)\n",
       "              (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)\n",
       "              (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(528, 528, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=528, bias=False)\n",
       "              (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(528, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.10434782608695654, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(720, 720, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=720, bias=False)\n",
       "              (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.11304347826086956, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(720, 720, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=720, bias=False)\n",
       "              (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.12173913043478261, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(720, 720, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=720, bias=False)\n",
       "              (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(720, 720, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=720, bias=False)\n",
       "              (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(720, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1391304347826087, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n",
       "              (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.14782608695652175, mode=row)\n",
       "        )\n",
       "        (2): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n",
       "              (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1565217391304348, mode=row)\n",
       "        )\n",
       "        (3): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n",
       "              (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.16521739130434784, mode=row)\n",
       "        )\n",
       "        (4): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n",
       "              (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1248, bias=False)\n",
       "              (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(1248, 352, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(352, 2112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(2112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(2112, 2112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2112, bias=False)\n",
       "              (1): BatchNorm2d(2112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(2112, 88, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(88, 2112, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): SiLU(inplace=True)\n",
       "              (scale_activation): Sigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(2112, 352, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.19130434782608696, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (8): Conv2dNormActivation(\n",
       "        (0): Conv2d(352, 1408, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Identity()\n",
       "  )\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=1408, out_features=50, bias=True)\n",
       "  (bn1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomModel('efficientnet').to(DEVICE)\n",
    "\n",
    "checkpoint = torch.load('../model/efficientnet_b2_ft_best_model_01.pth')\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 마지막 몇 개의 레이어를 동결 해제 (features 블록 5 이후 층)\n",
    "for name, param in model.base_model.features[5:].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 동결 해제된 일부 층과 분류기 층 학습\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.1010, Test accuracy: 0.9697\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "적은 연산량으로 좋은 성능을 낼수있는 모델임.  \n",
    "feature 8 ~ unfreeze "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model_name='efficientnet'):\n",
    "        super(CustomModel, self).__init__()\n",
    "        if model_name == 'efficientnet':\n",
    "            self.base_model = torchvision.models.efficientnet_b2(weights='IMAGENET1K_V1')\n",
    "            self.base_model.classifier = nn.Identity()  # 마지막 분류기 제거\n",
    "        elif model_name == 'resnet50':\n",
    "            self.base_model = torchvision.models.resnet50(weights='IMAGENET1K_V1')\n",
    "            self.base_model = nn.Sequential(*list(self.base_model.children())[:-1])  # 마지막 레이어 제거\n",
    "        elif model_name == 'inception':\n",
    "            self.base_model = torchvision.models.inception_v3(weights='IMAGENET1K_V1')\n",
    "            self.base_model.classifier = nn.Identity()  # 마지막 분류기 제거\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.5)  # 첫 번째 드롭아웃\n",
    "        self.fc1 = nn.Linear(self._get_features_dim(model_name), 50)\n",
    "        self.bn1 = nn.BatchNorm1d(50)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.5)  # 두 번째 드롭아웃\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "\n",
    "    def _get_features_dim(self, model_name):\n",
    "        if model_name == 'efficientnet':\n",
    "            return 1408  # EfficientNet B2의 출력 차원\n",
    "        elif model_name == 'resnet50':\n",
    "            return 2048  # ResNet50의 출력 차원\n",
    "        elif model_name == 'inception':\n",
    "            return 2048  # Inception의 출력 차원\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        if isinstance(x, tuple):  \n",
    "            x = x[0]\n",
    "        x = x.view(x.size(0), -1)  \n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x) \n",
    "        x = self.bn1(x) \n",
    "        x = self.dropout2(x) \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: base_model.features.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.1.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.2.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.2.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.0.block.2.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.1.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.2.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.2.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.1.1.block.2.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.2.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.3.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.4.3.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.5.3.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.2.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.3.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.6.4.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.0.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.0.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.0.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.0.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.1.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.1.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.1.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.2.fc1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.2.fc1.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.2.fc2.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.2.fc2.bias | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.3.0.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.3.1.weight | Requires Grad: False\n",
      "Layer: base_model.features.7.1.block.3.1.bias | Requires Grad: False\n",
      "Layer: base_model.features.8.0.weight | Requires Grad: True\n",
      "Layer: base_model.features.8.1.weight | Requires Grad: True\n",
      "Layer: base_model.features.8.1.bias | Requires Grad: True\n",
      "Layer: fc1.weight | Requires Grad: True\n",
      "Layer: fc1.bias | Requires Grad: True\n",
      "Layer: bn1.weight | Requires Grad: True\n",
      "Layer: bn1.bias | Requires Grad: True\n",
      "Layer: fc2.weight | Requires Grad: True\n",
      "Layer: fc2.bias | Requires Grad: True\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "model = CustomModel(model_name='efficientnet').to(DEVICE)\n",
    "\n",
    "# 1. 사전 학습된 base_model의 파라미터를 동결 (fine-tuning 초기 단계)\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 마지막 몇 개의 레이어를 동결 해제 (features 블록 8 이후 층)\n",
    "for name, param in model.base_model.features[8:].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 동결 해제된 일부 층과 분류기 층 학습\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
    "\n",
    "# 학습할 손실 함수\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 각 층의 freeze/unfreeze 상태 확인\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Requires Grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss improved, saving model...\n",
      "Epoch [1/50], Train Loss: 0.6941, Train Accuracy: 0.5702, Val Loss: 0.5888, Val Accuracy: 0.6793, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [2/50], Train Loss: 0.5934, Train Accuracy: 0.6788, Val Loss: 0.5236, Val Accuracy: 0.7709, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [3/50], Train Loss: 0.5673, Train Accuracy: 0.6987, Val Loss: 0.4936, Val Accuracy: 0.7809, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [4/50], Train Loss: 0.5349, Train Accuracy: 0.7186, Val Loss: 0.4731, Val Accuracy: 0.7948, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [5/50], Train Loss: 0.5246, Train Accuracy: 0.7390, Val Loss: 0.4497, Val Accuracy: 0.8127, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [6/50], Train Loss: 0.4882, Train Accuracy: 0.7694, Val Loss: 0.4372, Val Accuracy: 0.8207, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [7/50], Train Loss: 0.4812, Train Accuracy: 0.7699, Val Loss: 0.4286, Val Accuracy: 0.8227, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [8/50], Train Loss: 0.4859, Train Accuracy: 0.7714, Val Loss: 0.4199, Val Accuracy: 0.8347, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [9/50], Train Loss: 0.4681, Train Accuracy: 0.7719, Val Loss: 0.4111, Val Accuracy: 0.8307, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [10/50], Train Loss: 0.4562, Train Accuracy: 0.7829, Val Loss: 0.4004, Val Accuracy: 0.8307, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [11/50], Train Loss: 0.4509, Train Accuracy: 0.7844, Val Loss: 0.3933, Val Accuracy: 0.8367, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [12/50], Train Loss: 0.4395, Train Accuracy: 0.7958, Val Loss: 0.3892, Val Accuracy: 0.8367, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [13/50], Train Loss: 0.4289, Train Accuracy: 0.8038, Val Loss: 0.3850, Val Accuracy: 0.8466, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [14/50], Train Loss: 0.4304, Train Accuracy: 0.8003, Val Loss: 0.3796, Val Accuracy: 0.8426, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [15/50], Train Loss: 0.4207, Train Accuracy: 0.8068, Val Loss: 0.3793, Val Accuracy: 0.8466, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [16/50], Train Loss: 0.4083, Train Accuracy: 0.8152, Val Loss: 0.3636, Val Accuracy: 0.8546, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [17/50], Train Loss: 0.4033, Train Accuracy: 0.8247, Val Loss: 0.3551, Val Accuracy: 0.8526, Learning Rate: 0.000050\n",
      "Epoch [18/50], Train Loss: 0.4080, Train Accuracy: 0.8197, Val Loss: 0.3581, Val Accuracy: 0.8526, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [19/50], Train Loss: 0.3963, Train Accuracy: 0.8262, Val Loss: 0.3538, Val Accuracy: 0.8566, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [20/50], Train Loss: 0.4003, Train Accuracy: 0.8312, Val Loss: 0.3499, Val Accuracy: 0.8566, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [21/50], Train Loss: 0.3913, Train Accuracy: 0.8337, Val Loss: 0.3499, Val Accuracy: 0.8546, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [22/50], Train Loss: 0.3947, Train Accuracy: 0.8277, Val Loss: 0.3417, Val Accuracy: 0.8546, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [23/50], Train Loss: 0.3912, Train Accuracy: 0.8287, Val Loss: 0.3385, Val Accuracy: 0.8625, Learning Rate: 0.000050\n",
      "Epoch [24/50], Train Loss: 0.3746, Train Accuracy: 0.8367, Val Loss: 0.3392, Val Accuracy: 0.8566, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [25/50], Train Loss: 0.3685, Train Accuracy: 0.8406, Val Loss: 0.3286, Val Accuracy: 0.8665, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [26/50], Train Loss: 0.3656, Train Accuracy: 0.8431, Val Loss: 0.3241, Val Accuracy: 0.8685, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [27/50], Train Loss: 0.3637, Train Accuracy: 0.8461, Val Loss: 0.3208, Val Accuracy: 0.8705, Learning Rate: 0.000050\n",
      "Epoch [28/50], Train Loss: 0.3554, Train Accuracy: 0.8426, Val Loss: 0.3233, Val Accuracy: 0.8685, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [29/50], Train Loss: 0.3509, Train Accuracy: 0.8471, Val Loss: 0.3122, Val Accuracy: 0.8705, Learning Rate: 0.000050\n",
      "Epoch [30/50], Train Loss: 0.3475, Train Accuracy: 0.8531, Val Loss: 0.3145, Val Accuracy: 0.8625, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [31/50], Train Loss: 0.3442, Train Accuracy: 0.8531, Val Loss: 0.3070, Val Accuracy: 0.8705, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [32/50], Train Loss: 0.3378, Train Accuracy: 0.8521, Val Loss: 0.3023, Val Accuracy: 0.8685, Learning Rate: 0.000050\n",
      "Epoch [33/50], Train Loss: 0.3235, Train Accuracy: 0.8705, Val Loss: 0.3087, Val Accuracy: 0.8685, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [34/50], Train Loss: 0.3383, Train Accuracy: 0.8546, Val Loss: 0.3006, Val Accuracy: 0.8705, Learning Rate: 0.000050\n",
      "Epoch [35/50], Train Loss: 0.3477, Train Accuracy: 0.8511, Val Loss: 0.3054, Val Accuracy: 0.8665, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [36/50], Train Loss: 0.3260, Train Accuracy: 0.8660, Val Loss: 0.2924, Val Accuracy: 0.8745, Learning Rate: 0.000050\n",
      "Epoch [37/50], Train Loss: 0.3252, Train Accuracy: 0.8591, Val Loss: 0.2998, Val Accuracy: 0.8685, Learning Rate: 0.000050\n",
      "Epoch [38/50], Train Loss: 0.3317, Train Accuracy: 0.8680, Val Loss: 0.2932, Val Accuracy: 0.8745, Learning Rate: 0.000050\n",
      "Epoch [39/50], Train Loss: 0.3179, Train Accuracy: 0.8670, Val Loss: 0.2938, Val Accuracy: 0.8725, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [40/50], Train Loss: 0.3197, Train Accuracy: 0.8685, Val Loss: 0.2863, Val Accuracy: 0.8845, Learning Rate: 0.000050\n",
      "Epoch [41/50], Train Loss: 0.3111, Train Accuracy: 0.8765, Val Loss: 0.2869, Val Accuracy: 0.8805, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [42/50], Train Loss: 0.3173, Train Accuracy: 0.8700, Val Loss: 0.2832, Val Accuracy: 0.8845, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [43/50], Train Loss: 0.3223, Train Accuracy: 0.8660, Val Loss: 0.2766, Val Accuracy: 0.8904, Learning Rate: 0.000050\n",
      "Epoch [44/50], Train Loss: 0.3073, Train Accuracy: 0.8660, Val Loss: 0.2818, Val Accuracy: 0.8805, Learning Rate: 0.000050\n",
      "Epoch [45/50], Train Loss: 0.3150, Train Accuracy: 0.8650, Val Loss: 0.2801, Val Accuracy: 0.8884, Learning Rate: 0.000050\n",
      "Validation loss improved, saving model...\n",
      "Epoch [46/50], Train Loss: 0.3067, Train Accuracy: 0.8815, Val Loss: 0.2764, Val Accuracy: 0.8845, Learning Rate: 0.000050\n",
      "Epoch [47/50], Train Loss: 0.2966, Train Accuracy: 0.8810, Val Loss: 0.2780, Val Accuracy: 0.8845, Learning Rate: 0.000050\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "# 조기 종료 변수 초기화\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # 개선이 없을 때 기다릴 에포크 수\n",
    "patience_counter = 0\n",
    "\n",
    "# 학습률 스케줄러\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(DEVICE), targets.to(DEVICE).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = criterion(outputs.view(-1), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 정확도 계산\n",
    "        predicted = (torch.sigmoid(outputs.view(-1)) > 0.5).float()\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "        total_predictions += targets.size(0)\n",
    "\n",
    "    # 훈련 데이터 정확도\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # 검증\n",
    "    val_loss, val_accuracy = evaluate_model(model, validation_loader, criterion)\n",
    "\n",
    "    # 학습률 스케줄러 적용\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # 현재 학습률 출력\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    # 조기 종료 로직\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # 손실 개선 시 카운터 리셋\n",
    "        print(\"Validation loss improved, saving model...\")\n",
    "        # 모델 저장\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, '../model/efficientnet_b2_ft_best_model_02.pth')\n",
    "    else:\n",
    "        patience_counter += 1  # 손실이 개선되지 않으면 카운터 증가\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break  # 훈련 종료\n",
    "\n",
    "    # 에포크별 정보 출력 (학습률 포함)\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {running_loss / len(train_loader):.4f}, '\n",
    "          f'Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, '\n",
    "          f'Learning Rate: {current_lr:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomModel('efficientnet').to(DEVICE)\n",
    "\n",
    "checkpoint = torch.load('../model/efficientnet_b2_ft_best_model_02.pth')\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 마지막 몇 개의 레이어를 동결 해제 (features 블록 8 이후 층)\n",
    "for name, param in model.base_model.features[8:].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 동결 해제된 일부 층과 분류기 층 학습\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률 조정 5e-5 -> 1e-5, features.5~unfreeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = CustomModel(model_name='efficientnet').to(DEVICE)\n",
    "\n",
    "# 1. 사전 학습된 base_model의 파라미터를 동결 (fine-tuning 초기 단계)\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 마지막 몇 개의 레이어를 동결 해제 (features 블록 5 이후 층)\n",
    "for name, param in model.base_model.features[5:].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 동결 해제된 일부 층과 분류기 층 학습\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
    "\n",
    "# 학습할 손실 함수\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 각 층의 freeze/unfreeze 상태 확인\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Requires Grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "# 조기 종료 변수 초기화\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # 개선이 없을 때 기다릴 에포크 수\n",
    "patience_counter = 0\n",
    "\n",
    "# 학습률 스케줄러\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(DEVICE), targets.to(DEVICE).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = criterion(outputs.view(-1), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 정확도 계산\n",
    "        predicted = (torch.sigmoid(outputs.view(-1)) > 0.5).float()\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "        total_predictions += targets.size(0)\n",
    "\n",
    "    # 훈련 데이터 정확도\n",
    "    train_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # 검증\n",
    "    val_loss, val_accuracy = evaluate_model(model, validation_loader, criterion)\n",
    "\n",
    "    # 학습률 스케줄러 적용\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # 현재 학습률 출력\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    # 조기 종료 로직\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0  # 손실 개선 시 카운터 리셋\n",
    "        print(\"Validation loss improved, saving model...\")\n",
    "        # 모델 저장\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, '../model/efficientnet_b2_ft_best_model_03.pth')\n",
    "    else:\n",
    "        patience_counter += 1  # 손실이 개선되지 않으면 카운터 증가\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break  # 훈련 종료\n",
    "\n",
    "    # 에포크별 정보 출력 (학습률 포함)\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Train Loss: {running_loss / len(train_loader):.4f}, '\n",
    "          f'Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, '\n",
    "          f'Learning Rate: {current_lr:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomModel('efficientnet').to(DEVICE)\n",
    "\n",
    "checkpoint = torch.load('../model/efficientnet_b2_ft_best_model_03.pth')\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 마지막 몇 개의 레이어를 동결 해제 (features 블록 5 이후 층)\n",
    "for name, param in model.base_model.features[5:].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 동결 해제된 일부 층과 분류기 층 학습\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "print(f'Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
